{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import requests\n",
    "from thefuzz import fuzz\n",
    "import re\n",
    "import time\n",
    "import itertools\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract SCSE Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_scse_profiles():    \n",
    "    # rpp query parameter specifies number of rows to display\n",
    "    # start query parameter specifies which row to start displaying from.\n",
    "    start = 0\n",
    "    rpp = 50\n",
    "    dr_ntu = {\"email\":[],'name':[], \"dr_ntu_url\":[]}\n",
    "    while True:\n",
    "        SCSE_list_url = f\"https://dr.ntu.edu.sg/simple-search?query=&location=researcherprofiles&filter_field_1=school&filter_type_1=authority&filter_value_1=ou00030&crisID=&relationName=&sort_by=bi_sort_4_sort&order=asc&rpp={rpp}&etal=0&start={start}\"\n",
    "\n",
    "        soup_source = requests.get(SCSE_list_url).text\n",
    "        soup = BeautifulSoup(soup_source,'lxml')\n",
    "\n",
    "        table = soup.find('table')\n",
    "        if table==None:\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            table_rows = table.find_all('tr')\n",
    "            # Skip Header Column\n",
    "            for row in table_rows[1:]:\n",
    "                name = row.find(name='td', headers='t1').text\n",
    "                dr_ntu_url = row.find(name='td', headers='t1').find(name='a')['href']\n",
    "                email = row.find(name='td', headers='t3').text\n",
    "\n",
    "                dr_ntu['name'].append(name)\n",
    "                dr_ntu['dr_ntu_url'].append(\"https://dr.ntu.edu.sg\"+dr_ntu_url)\n",
    "                dr_ntu['email'].append(email)\n",
    "                \n",
    "        start+=rpp\n",
    "    return pd.DataFrame(dr_ntu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scse_profiles = extract_scse_profiles()\n",
    "\n",
    "# Save to csv file in prof_raw_data dir\n",
    "raw_dir = './prof_raw_data/'\n",
    "filename = 'scse_profiles'\n",
    "filepath = f\"{raw_dir}{filename}.csv\" \n",
    "\n",
    "scse_profiles.to_csv(filepath)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Individual's Information from DR-NTU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dr_ntu(row):\n",
    "\n",
    "    name = row['name']\n",
    "    dr_ntu_url = row['dr_ntu_url']\n",
    "    ntu_email = row['email']\n",
    "\n",
    "    profile = {}\n",
    "    profile['full_name'] = name\n",
    "    profile['email'] = ntu_email\n",
    "\n",
    "\n",
    "    response = requests.get(dr_ntu_url).text\n",
    "    html = BeautifulSoup(response,'lxml')\n",
    "    \n",
    "    \n",
    "    # scrape the text on the name card class\n",
    "    span = html.find(name='span',attrs={'class':'namecard-fullname'})\n",
    "    if span is None:\n",
    "        profile['name_card'] = None\n",
    "    else:\n",
    "        profile['name_card'] = span.text.strip()\n",
    "    \n",
    "    # scraping designations can have multiple designation\n",
    "    div = span.parent\n",
    "    div = div.find_next_sibling('div')\n",
    "    designations = []\n",
    "    while div.get('id') is None:\n",
    "        designations.append(div.text.strip())\n",
    "        div = div.find_next_sibling('div')\n",
    "\n",
    "    profile['designations'] = designations\n",
    "    \n",
    "    urls = {'dr_ntu':dr_ntu_url}\n",
    "    # scrape all personal websites from personalsiteDiv\n",
    "    div = html.find(name='div',attrs={'id':'personalsiteDiv'})\n",
    "    if div != None:\n",
    "        a_tags = div.find_all(name='a')\n",
    "        for a_tag in a_tags:\n",
    "            url = a_tag.get('href')\n",
    "            text = a_tag.text.lower().replace(' ','_').strip()\n",
    "\n",
    "            if url is None or url =='#':\n",
    "                continue\n",
    "            else:\n",
    "                urls[text] = url\n",
    "\n",
    "    # scrape all personal websites from bibliometric \n",
    "    div = html.find(name='div', attrs={'id':'custombiblio'})\n",
    "    if div != None:\n",
    "        div_list = div.find_all(name='div', attrs={'class':'dynaField'})\n",
    "        for div in div_list:\n",
    "            url = div.find(name='a').get('href')\n",
    "            text = div.find(name='a').find(name='span').text.lower().replace(' ','_').strip()\n",
    "            # if key already exist in urls dict or if url is empty --> skip\n",
    "            if text in urls or url is None:\n",
    "                continue\n",
    "            else:\n",
    "                urls[text] = url\n",
    "    \n",
    "    profile['urls'] = urls\n",
    "    \n",
    "    # scrape biography\n",
    "    biography_text = html.find(name='div',attrs={'id':'biographyDiv'}).text.strip()\n",
    "    profile['biography'] = biography_text\n",
    "\n",
    "    # scrape keywords\n",
    "    keywords = []\n",
    "    div = html.find(name='div',attrs={'id':'researchkeywords','class':'panel'})\n",
    "    if div != None:\n",
    "        spans = html.find_all(name='span',attrs={'class':'rkeyword'})\n",
    "        for span in spans:\n",
    "            keywords.append(span.text.strip())\n",
    "\n",
    "    profile['keywords'] = keywords\n",
    "\n",
    "    # scrape research grants\n",
    "    grants = []\n",
    "    div = html.find(name='div',attrs={'id':'currentgrantsDiv'})\n",
    "    if div != None:\n",
    "        # first case is encapsulated by li tag\n",
    "        ul = div.find(name='ul')\n",
    "        if ul != None:\n",
    "            li_tags = ul.find_all('li')\n",
    "            for li_tag in li_tags:\n",
    "                grants.append(li_tag.text.strip())\n",
    "\n",
    "        elif div.find('br'):\n",
    "            for child in div.children:\n",
    "                if child.name == 'br':\n",
    "                    grants.append(child.next_sibling.strip())\n",
    "        else:\n",
    "            pass\n",
    "    profile['grants'] = grants\n",
    "\n",
    "    dir = './prof_raw_data/'\n",
    "    prefix = 'dr_ntu_'\n",
    "    filename = name.lower().replace(' ','_')\n",
    "    filepath = f\"{dir}{prefix}{filename}.json\"\n",
    "    with open(filepath,'w') as f:\n",
    "        json.dump(profile,f)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scse_profile_df = pd.read_csv('./prof_raw_data/scse_profiles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "      ... \n",
       "81    None\n",
       "82    None\n",
       "83    None\n",
       "84    None\n",
       "85    None\n",
       "Length: 86, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scse_profile_df.apply(extract_dr_ntu,axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CZ4125",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
