{"goog_sch_url": "https://scholar.google.com/citations?hl=en&user=lc45xlcAAAAJ", "name": "Liu Ziwei", "interests": ["Computer Vision", "Machine Learning", "Computer Graphics"], "co_authors_url": [{"name": "Chen Change Loy", "url": "https://scholar.google.com/citations?user=559LF80AAAAJ&hl=en", "aff": "MMLab@NTU, S-Lab, Nanyang Technological University", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [64798, 58524], "h-index": [105, 100], "i10-index": [211, 209]}}, {"name": "Ping Luo (\u7f85\u5e73)", "url": "https://scholar.google.com/citations?user=aXdjxb4AAAAJ&hl=en", "aff": "Associate Professor, The University of Hong Kong", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [41505, 37966], "h-index": [78, 77], "i10-index": [168, 164]}}, {"name": "Dahua Lin", "url": "https://scholar.google.com/citations?user=GMzzRRUAAAAJ&hl=en", "aff": "The Chinese University of Hong Kong", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [35134, 32819], "h-index": [76, 71], "i10-index": [162, 152]}}, {"name": "Xiaoou Tang", "url": "https://scholar.google.com/citations?user=qpBtpGsAAAAJ&hl=en", "aff": "The Chinese University of Hong Kong", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [136280, 96324], "h-index": [143, 106], "i10-index": [390, 260]}}, {"name": "Xiaogang Wang", "url": "https://scholar.google.com/citations?user=-B5JgjsAAAAJ&hl=en", "aff": "Professor of Electronic Engineering, the Chinese University of Hong Kong", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [108656, 89662], "h-index": [146, 129], "i10-index": [259, 243]}}, {"name": "Stella X. Yu", "url": "https://scholar.google.com/citations?user=uqWkLzMAAAAJ&hl=en", "aff": "Professor of EECS, University of Michigan", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [10965, 8326], "h-index": [43, 34], "i10-index": [90, 76]}}, {"name": "Boqing Gong", "url": "https://scholar.google.com/citations?user=lv9ZeVUAAAAJ&hl=en", "aff": "Research Scientist, Google", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [14537, 13067], "h-index": [52, 51], "i10-index": [81, 79]}}, {"name": "Justin Solomon", "url": "https://scholar.google.com/citations?user=pImSVwoAAAAJ&hl=en", "aff": "MIT", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [11796, 10830], "h-index": [42, 39], "i10-index": [84, 81]}}, {"name": "Michael Bronstein", "url": "https://scholar.google.com/citations?user=UU3N6-UAAAAJ&hl=en", "aff": "DeepMind Professor of AI, University of Oxford", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [35597, 25796], "h-index": [82, 63], "i10-index": [236, 180]}}, {"name": "Aseem Agarwala", "url": "https://scholar.google.com/citations?user=JY-WzksAAAAJ&hl=en", "aff": "Adobe", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [13449, 5321], "h-index": [44, 35], "i10-index": [71, 57]}}, {"name": "Raymond A. Yeh", "url": "https://scholar.google.com/citations?user=7HDE1ZwAAAAJ&hl=en", "aff": "Assistant Professor, Purdue University", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [3281, 3157], "h-index": [15, 15], "i10-index": [20, 20]}}, {"name": "Wayne Marcus Getz", "url": "https://scholar.google.com/citations?user=FfULC7gAAAAJ&hl=en", "aff": "Professor of Environmental Sciece, UC Berkeley", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [27269, 11696], "h-index": [77, 51], "i10-index": [243, 150]}}, {"name": "Lu Yuan", "url": "https://scholar.google.com/citations?user=k9TsUVsAAAAJ&hl=en", "aff": "Principal Research Manager, Cognition, Cloud & AI, Microsoft", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [18710, 15780], "h-index": [59, 58], "i10-index": [109, 105]}}, {"name": "Jian Sun", "url": "https://scholar.google.com/citations?user=ALVSZAYAAAAJ&hl=en", "aff": "Chief Scientist of Megvii, Managing Director of Megvii Research", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [435837, 381517], "h-index": [147, 128], "i10-index": [783, 690]}}, {"name": "Dan B Goldman", "url": "https://scholar.google.com/citations?user=4H4bRkcAAAAJ&hl=en", "aff": "Research Scientist, Google", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [11645, 7016], "h-index": [36, 31], "i10-index": [48, 44]}}, {"name": "Zerui Wang", "url": "https://scholar.google.com/citations?user=fWi5tx8AAAAJ&hl=en", "aff": "Cornerstone Robotics Limited, The Chinese University of Hong Kong", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [843, 797], "h-index": [16, 15], "i10-index": [24, 23]}}, {"name": "Russell H. Taylor", "url": "https://scholar.google.com/citations?user=Ph16U5wAAAAJ&hl=en", "aff": "John C. Malone Professor of Computer Science, Johns Hopkins University", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [55325, 24559], "h-index": [107, 69], "i10-index": [526, 375]}}, {"name": "Leonid Sigal", "url": "https://scholar.google.com/citations?user=P2mG6rcAAAAJ&hl=en", "aff": "Professor, University of British Columbia", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [13418, 7035], "h-index": [56, 44], "i10-index": [123, 106]}}, {"name": "Matt Uyttendaele", "url": "https://scholar.google.com/citations?user=7kxzvksAAAAJ&hl=en", "aff": "Meta", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [12431, 4413], "h-index": [48, 31], "i10-index": [85, 55]}}], "citation_table": {"columns": ["All", "Since 2018"], "Citations": [32044, 31140], "h-index": [63, 63], "i10-index": [127, 127]}, "citation_graph": {}, "articles": [{"title": "Deep Learning Face Attributes in the Wild", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:Y0pCki6q_DkC", "authors": ["Ziwei Liu", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang"], "publication_date": "2015", "conference": "IEEE International Conference on Computer Vision (ICCV)", "description": "Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation.(1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies.(2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works.(3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts.", "total_citations": 7552, "citation_graph": {"2016": 102, "2017": 321, "2018": 663, "2019": 950, "2020": 1140, "2021": 1401, "2022": 1560, "2023": 1351}}, {"title": "Dynamic Graph CNN for Learning on Point Clouds", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:0EnyYjriUFMC", "authors": ["Yue Wang", "Yongbin Sun", "Ziwei Liu", "Sanjay E Sarma", "Michael M Bronstein", "Justin M Solomon"], "publication_date": "2019", "journal": "ACM Transactions on Graphics (TOG)", "description": "Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. Point clouds inherently lack topological information, so designing a model to recover topology can enrich the representation power of point clouds. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds, including classification and segmentation. EdgeConv acts on graphs dynamically computed in each layer of the network. It is differentiable and can be plugged into existing architectures \u2026", "total_citations": 4652, "citation_graph": {"2018": 43, "2019": 247, "2020": 628, "2021": 1021, "2022": 1396, "2023": 1290}}, {"title": "MMDetection: Open MMLab Detection Toolbox and Benchmark", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:9ZlFYXVOiuMC", "authors": ["Kai Chen", "Jiaqi Wang", "Jiangmiao Pang", "Yuhang Cao", "Yu Xiong", "Xiaoxiao Li", "Shuyang Sun", "Wansen Feng", "Ziwei Liu", "Jiarui Xu", "Zheng Zhang", "Dazhi Cheng", "Chenchen Zhu", "Tianheng Cheng", "Qijie Zhao", "Buyu Li", "Xin Lu", "Rui Zhu", "Yue Wu", "Jifeng Dai", "Jingdong Wang", "Jianping Shi", "Wanli Ouyang", "Chen Change Loy", "Dahua Lin"], "publication_date": "2019/6/17", "journal": "arXiv preprint arXiv:1906.07155", "description": "We present MMDetection, an object detection toolbox that contains a rich set of object detection and instance segmentation methods as well as related components and modules. The toolbox started from a codebase of MMDet team who won the detection track of COCO Challenge 2018. It gradually evolves into a unified platform that covers many popular detection methods and contemporary modules. It not only includes training and inference codes, but also provides weights for more than 200 network models. We believe this toolbox is by far the most complete detection toolbox. In this paper, we introduce the various features of this toolbox. In addition, we also conduct a benchmarking study on different methods, components, and their hyper-parameters. We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new detectors. Code and models are available at https://github.com/open-mmlab/mmdetection. The project is under active development and we will keep this document updated.", "total_citations": 2284, "citation_graph": {"2019": 43, "2020": 251, "2021": 496, "2022": 806, "2023": 682}}, {"title": "DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:YsMSGLbcyi4C", "authors": ["Ziwei Liu", "Ping Luo", "Shi Qiu", "Xiaogang Wang", "Xiaoou Tang"], "publication_date": "2016", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Recent advances in clothes recognition have been driven by the construction of clothes datasets. Existing datasets are limited in the amount of annotations and are difficult to cope with the various challenges in real-world applications. In this work, we introduce DeepFashion, a large-scale clothes dataset with comprehensive annotations. It contains over 800,000 images, which are richly annotated with massive attributes, clothing landmarks, and correspondence of images taken under different scenarios including store, street snapshot, and consumer. Such rich annotations enable the development of powerful algorithms in clothes recognition and facilitating future researches. To demonstrate the advantages of DeepFashion, we propose a new deep model, namely FashionNet, which learns clothing features by jointly predicting clothing attributes and landmarks. The estimated landmarks are then employed to pool or gate the learned features. It is optimized in an iterative manner. Extensive experiments demonstrate the effectiveness of FashionNet and the usefulness of DeepFashion.", "total_citations": 1811, "citation_graph": {"2016": 8, "2017": 76, "2018": 158, "2019": 255, "2020": 295, "2021": 337, "2022": 360, "2023": 308}}, {"title": "Hybrid Task Cascade for Instance Segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:_kc_bZDykSQC", "authors": ["Kai Chen", "Jiangmiao Pang", "Jiaqi Wang", "Yu Xiong", "Xiaoxiao Li", "Shuyang Sun", "Wansen Feng", "Ziwei Liu", "Jianping Shi", "Wanli Ouyang", "Chen Change Loy", "Dahua Lin"], "publication_date": "2019", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Cascade is a classic yet powerful architecture that has boosted performance on various tasks. However, how to introduce cascade to instance segmentation remains an open question. A simple combination of Cascade R-CNN and Mask R-CNN only brings limited gain. In exploring a more effective approach, we find that the key to a successful instance segmentation cascade is to fully leverage the reciprocal relationship between detection and segmentation. In this work, we propose a new framework, Hybrid Task Cascade (HTC), which differs in two important aspects:(1) instead of performing cascaded refinement on these two tasks separately, it interweaves them for a joint multi-stage processing;(2) it adopts a fully convolutional branch to provide spatial context, which can help distinguishing hard foreground from cluttered background. Overall, this framework can learn more discriminative features progressively while integrating complementary features together in each stage. Without bells and whistles, a single HTC obtains 38.4% and 1.5% improvement over a strong Cascade Mask R-CNN baseline on MSCOCO dataset. Moreover, our overall system achieves 48.6 mask AP on the test-challenge split, ranking 1st in the COCO 2018 Challenge Object Detection Task. Code is available at https://github. com/open-mmlab/mmdetection.", "total_citations": 1184, "citation_graph": {"2019": 43, "2020": 179, "2021": 322, "2022": 346, "2023": 292}}, {"title": "Large-Scale Long-Tailed Recognition in an Open World", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:4DMP91E08xMC", "authors": ["Ziwei Liu", "Zhongqi Miao", "Xiaohang Zhan", "Jiayun Wang", "Boqing Gong", "Stella X Yu"], "publication_date": "2019", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Real world data often have a long-tailed and open-ended distribution. A practical recognition system must classify among majority and minority classes, generalize from a few known instances, and acknowledge novelty upon a never seen instance. We define Open Long-Tailed Recognition (OLTR) as learning from such naturally distributed data and optimizing the classification accuracy over a balanced test set which include head, tail, and open classes. OLTR must handle imbalanced classification, few-shot learning, and open-set recognition in one integrated algorithm, whereas existing classification approaches focus only on one aspect and deliver poorly over the entire class spectrum. The key challenges are how to share visual knowledge between head and tail classes and how to reduce confusion between tail and open classes. We develop an integrated OLTR algorithm that maps an image to a feature space such that visual concepts can easily relate to each other based on a learned metric that respects the closed-world classification while acknowledging the novelty of the open world. Our so-called dynamic meta-embedding combines a direct image feature and an associated memory feature, with the feature norm indicating the familiarity to known classes. On three large-scale OLTR datasets we curate from object-centric ImageNet, scene-centric Places, and face-centric MS1M data, our method consistently outperforms the state-of-the-art. Our code, datasets, and models enable future OLTR research and are publicly available at https://liuziwei7. github. io/projects/LongTail. html.", "total_citations": 950, "citation_graph": {"2019": 14, "2020": 87, "2021": 213, "2022": 307, "2023": 325}}, {"title": "MaskGAN: Towards Diverse and Interactive Facial Image Manipulation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:ZeXyd9-uunAC", "authors": ["Cheng-Han Lee", "Ziwei Liu", "Lingyun Wu", "Ping Luo"], "publication_date": "2020", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Facial image manipulation has achieved great progress in recent years. However, previous methods either operate on a predefined set of face attributes or leave users little freedom to interactively manipulate images. To overcome these drawbacks, we propose a novel framework termed MaskGAN, enabling diverse and interactive face manipulation. Our key insight is that semantic masks serve as a suitable intermediate representation for flexible face manipulation with fidelity preservation. MaskGAN has two main components: 1) Dense Mapping Network (DMN) and 2) Editing Behavior Simulated Training (EBST). Specifically, DMN learns style mapping between a free-form user modified mask and a target image, enabling diverse generation results. EBST models the user editing behavior on the source mask, making the overall framework more robust to various manipulated inputs. Specifically, it introduces dual-editing consistency as the auxiliary supervision signal. To facilitate extensive studies, we construct a large-scale high-resolution face dataset with fine-grained mask annotations named CelebAMask-HQ. MaskGAN is comprehensively evaluated on two challenging tasks: attribute transfer and style copy, demonstrating superior performance over other state-of-the-art methods. The code, models, and dataset are available at https://github. com/switchablenorms/CelebAMask-HQ.", "total_citations": 883, "citation_graph": {"2019": 5, "2020": 94, "2021": 225, "2022": 289, "2023": 266}}, {"title": "Semantic Image Segmentation via Deep Parsing Network", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:IjCSPb-OGe4C", "authors": ["Ziwei Liu", "Xiaoxiao Li", "Ping Luo", "Chen-Change Loy", "Xiaoou Tang"], "publication_date": "2015", "conference": "IEEE International Conference on Computer Vision (ICCV)", "description": "This paper addresses semantic image segmentation by incorporating rich information into Markov Random Field (MRF), including high-order relations and mixture of label contexts. Unlike previous works that optimized MRFs using iterative algorithm, we solve MRF by proposing a Convolutional Neural Network (CNN), namely Deep Parsing Network (DPN), which enables deterministic end-to-end computation in a single forward pass. Specifically, DPN extends a contemporary CNN architecture to model unary terms and additional layers are carefully devised to approximate the mean field algorithm (MF) for pairwise terms. It has several appealing properties. First, different from the recent works that combined CNN and MRF, where many iterations of MF were required for each training image during back-propagation, DPN is able to achieve high performance by approximating one iteration of MF. Second, DPN represents various types of pairwise terms, making many existing works as its special cases. Third, DPN makes MF easier to be parallelized and speeded up in Graphical Processing Unit (GPU). DPN is thoroughly evaluated on the PASCAL VOC 2012 dataset, where a single DPN model yields a new state-of-the-art segmentation accuracy of 77.5%.", "total_citations": 818, "citation_graph": {"2014": 3, "2015": 8, "2016": 72, "2017": 100, "2018": 125, "2019": 132, "2020": 109, "2021": 112, "2022": 83, "2023": 51}}, {"title": "Video Frame Synthesis using Deep Voxel Flow", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:_FxGoFyzp5QC", "authors": ["Ziwei Liu", "Raymond Yeh", "Xiaoou Tang", "Yiming Liu", "Aseem Agarwala"], "publication_date": "2017", "conference": "IEEE International Conference on Computer Vision (ICCV)", "description": "We address the problem of synthesizing new video frames in an existing video, either in-between existing frames (interpolation), or subsequent to them (extrapolation). This problem is challenging because video appearance and motion can be highly complex. Traditional optical-flow-based solutions often fail where flow estimation is challenging, while newer neural-network-based methods that hallucinate pixel values directly often produce blurry results. We combine the advantages of these two methods by training a deep network that learns to synthesize video frames by flowing pixel values from existing ones, which we call deep voxel flow. Our method requires no human supervision, and any video can be used as training data by dropping, and then learning to predict, existing frames. The technique is efficient, and can be applied at any video resolution. We demonstrate that our method produces results that both quantitatively and qualitatively improve upon the state-of-the-art.", "total_citations": 765, "citation_graph": {"2017": 17, "2018": 81, "2019": 121, "2020": 145, "2021": 152, "2022": 145, "2023": 101}}, {"title": "Learning to Prompt for Vision-Language Models", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:dfsIfKJdRG4C", "authors": ["Kaiyang Zhou", "Jingkang Yang", "Chen Change Loy", "Ziwei Liu"], "publication_date": "2022", "journal": "International Journal of Computer Vision (IJCV)", "description": "Large pre-trained vision-language models like CLIP have shown great potential in learning representations that are transferable across a wide range of downstream tasks. Different from the traditional representation learning that is based mostly on discretized labels, vision-language pre-training aligns images and texts in a common feature space, which allows zero-shot transfer to a downstream task via prompting, i.e., classification weights are synthesized from natural language describing classes of interest. In this work, we show that a major challenge for deploying such models in practice is prompt engineering, which requires domain expertise and is extremely time-consuming\u2014one needs to spend a significant amount of time on words tuning since a slight change in wording could have a huge impact on performance. Inspired by recent advances in prompt learning research in natural language processing (NLP \u2026", "total_citations": 716, "citation_graph": {"2021": 7, "2022": 167, "2023": 540}}, {"title": "Domain Generalization: A Survey", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:lSLTfruPkqcC", "authors": ["Kaiyang Zhou", "Ziwei Liu", "Yu Qiao", "Tao Xiang", "Chen Change Loy"], "publication_date": "2022", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)", "description": "Generalization to out-of-distribution (OOD) data is a capability natural to humans yet challenging for machines to reproduce. This is because most learning algorithms strongly rely on the i.i.d. assumption on source/target data, which is often violated in practice due to domain shift. Domain generalization (DG) aims to achieve OOD generalization by using only source data for model learning. Over the last ten years, research in DG has made great progress, leading to a broad spectrum of methodologies, e.g., those based on domain alignment, meta-learning, data augmentation, or ensemble learning, to name a few; DG has also been studied in various application areas including computer vision, speech recognition, natural language processing, medical imaging, and reinforcement learning. In this paper, for the first time a comprehensive literature review in DG is provided to summarize the developments over the past \u2026", "total_citations": 513, "citation_graph": {"2021": 42, "2022": 168, "2023": 300}}, {"title": "Large-scale Celebfaces Attributes (CelebA) Dataset", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:4TOpqqG69KYC", "authors": ["Ziwei Liu", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang"], "publication_date": "2018", "journal": "Retrieved August", "description": "CelebFaces Attributes Dataset (CelebA) is a large-scale face attributes dataset with more than 200K celebrity images, each with 40 attribute annotations. The images in this dataset cover large pose variations and background clutter. CelebA has large diversities, large quantities, and rich annotations, including", "total_citations": 428, "citation_graph": {"2018": 11, "2019": 22, "2020": 70, "2021": 108, "2022": 107, "2023": 110}}, {"title": "Generalized Out-of-Distribution Detection: A Survey", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:D03iK_w7-QYC", "authors": ["Jingkang Yang", "Kaiyang Zhou", "Yixuan Li", "Ziwei Liu"], "publication_date": "2021/10/21", "journal": "arXiv preprint arXiv:2110.11334", "description": "Out-of-distribution (OOD) detection is critical to ensuring the reliability and safety of machine learning systems. For instance, in autonomous driving, we would like the driving system to issue an alert and hand over the control to humans when it detects unusual scenes or objects that it has never seen during training time and cannot make a safe decision. The term, OOD detection, first emerged in 2017 and since then has received increasing attention from the research community, leading to a plethora of methods developed, ranging from classification-based to density-based to distance-based ones. Meanwhile, several other problems, including anomaly detection (AD), novelty detection (ND), open set recognition (OSR), and outlier detection (OD), are closely related to OOD detection in terms of motivation and methodology. Despite common goals, these topics develop in isolation, and their subtle differences in definition and problem setting often confuse readers and practitioners. In this survey, we first present a unified framework called generalized OOD detection, which encompasses the five aforementioned problems, i.e., AD, ND, OSR, OOD detection, and OD. Under our framework, these five problems can be seen as special cases or sub-tasks, and are easier to distinguish. We then review each of these five areas by summarizing their recent technical developments, with a special focus on OOD detection methodologies. We conclude this survey with open challenges and potential research directions.", "total_citations": 394, "citation_graph": {"2020": 2, "2021": 8, "2022": 141, "2023": 242}}, {"title": "Conditional Prompt Learning for Vision-Language Models", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:OU6Ihb5iCvQC", "authors": ["Kaiyang Zhou", "Jingkang Yang", "Chen Change Loy", "Ziwei Liu"], "publication_date": "2022", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "With the rise of powerful pre-trained vision-language models like CLIP, it becomes essential to investigate ways to adapt these models to downstream datasets. A recently proposed method named Context Optimization (CoOp) introduces the concept of prompt learning---a recent trend in NLP---to the vision domain for adapting pre-trained vision-language models. Specifically, CoOp turns context words in a prompt into a set of learnable vectors and, with only a few labeled images for learning, can achieve huge improvements over intensively-tuned manual prompts. In our study we identify a critical problem of CoOp: the learned context is not generalizable to wider unseen classes within the same dataset, suggesting that CoOp overfits base classes observed during training. To address the problem, we propose Conditional Context Optimization (CoCoOp), which extends CoOp by further learning a lightweight neural network to generate for each image an input-conditional token (vector). Compared to CoOp's static prompts, our dynamic prompts adapt to each instance and are thus less sensitive to class shift. Extensive experiments show that CoCoOp generalizes much better than CoOp to unseen classes, even showing promising transferability beyond a single dataset; and yields stronger domain generalization performance as well. Code is available at https://github. com/KaiyangZhou/CoOp.", "total_citations": 392, "citation_graph": {"2021": 1, "2022": 72, "2023": 319}}, {"title": "Incorporating Convolution Designs into Visual Transformers", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:RYcK_YlVTxYC", "authors": ["Kun Yuan", "Shaopeng Guo", "Ziwei Liu", "Aojun Zhou", "Fengwei Yu", "Wei Wu"], "publication_date": "2021", "conference": "IEEE International Conference on Computer Vision (ICCV)", "description": "Motivated by the success of Transformers in natural language processing (NLP) tasks, there exist some attempts (eg, ViT and DeiT) to apply Transformers to the vision domain. However, pure Transformer architectures often require a large amount of training data or extra supervision to obtain comparable performance with convolutional neural networks (CNNs). To overcome these limitations, we analyze the potential drawbacks when directly borrowing Transformer architectures from NLP. Then we propose a new Convolution-enhanced image Transformer (CeiT) which combines the advantages of CNNs in extracting low-level features, strengthening locality, and the advantages of Transformers in establishing long-range dependencies. Three modifications are made to the original Transformer: 1) instead of the straightforward tokenization from raw input images, we design an Image-to-Tokens (I2T) module that extracts patches from generated low-level features; 2) the feed-froward network in each encoder block is replaced with a Locally-enhanced Feed-Forward (LeFF) layer that promotes the correlation among neighboring tokens in the spatial dimension; 3) a Layer-wise Class token Attention (LCA) is attached at the top of the Transformer that utilizes the multi-level representations. Experimental results on ImageNet and seven downstream tasks show the effectiveness and generalization ability compared with previous Transformers and state-of-the-art CNNs, without requiring a large amount of training data and extra CNN teachers. Besides, CeiT models also demonstrate better convergence with 3xfewer training iterations, which can reduce the \u2026", "total_citations": 361, "citation_graph": {"2021": 34, "2022": 145, "2023": 180}}, {"title": "Talking Face Generation by Adversarially Disentangled Audio-Visual Representation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:kNdYIx-mwKoC", "authors": ["Hang Zhou", "Yu Liu", "Ziwei Liu", "Ping Luo", "Xiaogang Wang"], "publication_date": "2019", "conference": "AAAI Conference on Artificial Intelligence (AAAI)", "description": "Talking face generation aims to synthesize a sequence of face images that correspond to a clip of speech. This is a challenging task because face appearance variation and semantics of speech are coupled together in the subtle movements of the talking face regions. Existing works either construct specific face appearance model on specific subjects or model the transformation between lip motion and speech. In this work, we integrate both aspects and enable arbitrary-subject talking face generation by learning disentangled audio-visual representation. We find that the talking face sequence is actually a composition of both subject-related information and speech-related information. These two spaces are then explicitly disentangled through a novel associative-and-adversarial training process. This disentangled representation has an advantage where both audio and video can serve as inputs for generation. Extensive experiments show that the proposed approach generates realistic talking face sequences on arbitrary subjects with much clearer lip motion patterns than previous work. We also demonstrate the learned audio-visual representation is extremely useful for the tasks of automatic lip reading and audio-video retrieval.", "total_citations": 360, "citation_graph": {"2018": 2, "2019": 22, "2020": 50, "2021": 80, "2022": 103, "2023": 103}}, {"title": "CARAFE: Content-Aware ReAssembly of FEatures", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:Wp0gIr-vW9MC", "authors": ["Jiaqi Wang", "Kai Chen", "Rui Xu", "Ziwei Liu", "Chen Change Loy", "Dahua Lin"], "publication_date": "2019", "conference": "IEEE International Conference on Computer Vision (ICCV)", "description": "Feature upsampling is a key operation in a number of modern convolutional network architectures, eg feature pyramids. Its design is critical for dense prediction tasks such as object detection and semantic/instance segmentation. In this work, we propose Content-Aware ReAssembly of FEatures (CARAFE), a universal, lightweight and highly effective operator to fulfill this goal. CARAFE has several appealing properties:(1) Large field of view. Unlike previous works (eg bilinear interpolation) that only exploit subpixel neighborhood, CARAFE can aggregate contextual information within a large receptive field.(2) Content-aware handling. Instead of using a fixed kernel for all samples (eg deconvolution), CARAFE enables instance-specific content-aware handling, which generates adaptive kernels on-the-fly.(3) Lightweight and fast to compute. CARAFE introduces little computational overhead and can be readily integrated into modern network architectures. We conduct comprehensive evaluations on standard benchmarks in object detection, instance/semantic segmentation and inpainting. CARAFE shows consistent and substantial gains across all the tasks (1.2% AP, 1.3% AP, 1.8% mIoU, 1.1 dB respectively) with negligible computational overhead. It has great potential to serve as a strong building block for future research. Code and models are available at https://github. com/open-mmlab/mmdetection.", "total_citations": 320, "citation_graph": {"2019": 2, "2020": 34, "2021": 64, "2022": 92, "2023": 128}}, {"title": "Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:LkGwnXOMwfcC", "authors": ["Xiaoxiao Li", "Ziwei Liu", "Ping Luo", "Chen Change Loy", "Xiaoou Tang"], "publication_date": "2017", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "We propose a novel deep layer cascade (LC) method to improve the accuracy and speed of semantic segmentation. Unlike the conventional model cascade (MC) that is composed of multiple independent models, LC treats a single deep model as a cascade of several sub-models. Earlier sub-models are trained to handle easy and confident regions, and they progressively feed-forward harder regions to the next sub-model for processing. Convolutions are only calculated on these regions to reduce computations. The proposed method possesses several advantages. First, LC classifies most of the easy regions in the shallow stage and makes deeper stage focuses on a few hard regions. Such an adaptive and'difficulty-aware'learning improves segmentation performance. Second, LC accelerates both training and testing of deep network thanks to early decisions in the shallow stage. Third, in comparison to MC, LC is an end-to-end trainable framework, allowing joint learning of all sub-models. We evaluate our method on PASCAL VOC and Cityscapes datasets, achieving state-of-the-art performance and fast speed.", "total_citations": 295, "citation_graph": {"2017": 6, "2018": 40, "2019": 55, "2020": 63, "2021": 51, "2022": 41, "2023": 38}}, {"title": "Long-tailed Recognition by Routing Diverse Distribution-Aware Experts", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:YFjsv_pBGBYC", "authors": ["Xudong Wang", "Long Lian", "Zhongqi Miao", "Ziwei Liu", "Stella X Yu"], "publication_date": "2021", "conference": "International Conference on Learning Representations (ICLR)", "description": "Natural data are often long-tail distributed over semantic classes. Existing recognition methods tackle this imbalanced classification by placing more emphasis on the tail data, through class re-balancing/re-weighting or ensembling over different data groups, resulting in increased tail accuracies but reduced head accuracies. We take a dynamic view of the training data and provide a principled model bias and variance analysis as the training data fluctuates: Existing long-tail classifiers invariably increase the model variance and the head-tail model bias gap remains large, due to more and larger confusion with hard negatives for the tail. We propose a new long-tailed classifier called RoutIng Diverse Experts (RIDE). It reduces the model variance with multiple experts, reduces the model bias with a distribution-aware diversity loss, reduces the computational cost with a dynamic expert routing module. RIDE outperforms the state-of-the-art by 5% to 7% on CIFAR100-LT, ImageNet-LT and iNaturalist 2018 benchmarks. It is also a universal framework that is applicable to various backbone networks, long-tailed algorithms, and training mechanisms for consistent performance gains. Our code is available at: https://github.com/frank-xwang/RIDE-LongTailRecognition.", "total_citations": 244, "citation_graph": {"2021": 21, "2022": 97, "2023": 125}}, {"title": "Knowledge Distillation Meets Self-Supervision", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:iH-uZ7U-co4C", "authors": ["Guodong Xu", "Ziwei Liu", "Xiaoxiao Li", "Chen Change Loy"], "publication_date": "2020", "conference": "European Conference on Computer Vision (ECCV)", "description": "Knowledge distillation, which involves extracting the \u201cdark knowledge\u201d from a teacher network to guide the learning of a student network, has emerged as an important technique for model compression and transfer learning. Unlike previous works that exploit architecture-specific cues such as activation and attention for distillation, here we wish to explore a more general and model-agnostic approach for extracting \u201cricher dark knowledge\u201d from the pre-trained teacher model. We show that the seemingly different self-supervision task can serve as a simple yet powerful solution. For example, when performing contrastive learning between transformed entities, the noisy predictions of the teacher network reflect its intrinsic composition of semantic and pose information. By exploiting the similarity between those self-supervision signals as an auxiliary task, one can effectively transfer the hidden information from the \u2026", "total_citations": 228, "citation_graph": {"2020": 8, "2021": 52, "2022": 97, "2023": 70}}, {"title": "Face Model Compression by Distilling Knowledge from Neurons", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:W7OEmFMy1HYC", "authors": ["Ping Luo", "Zhenyao Zhu", "Ziwei Liu", "Xiaogang Wang", "Xiaoou Tang"], "publication_date": "2016", "conference": "AAAI Conference on Artificial Intelligence (AAAI)", "description": "The recent advanced face recognition systems werebuilt on large Deep Neural Networks (DNNs) or theirensembles, which have millions of parameters. However, the expensive computation of DNNs make theirdeployment difficult on mobile and embedded devices. This work addresses model compression for face recognition, where the learned knowledge of a large teachernetwork or its ensemble is utilized as supervisionto train a compact student network. Unlike previousworks that represent the knowledge by the soften labelprobabilities, which are difficult to fit, we represent theknowledge by using the neurons at the higher hiddenlayer, which preserve as much information as the label probabilities, but are more compact. By leveragingthe essential characteristics (domain knowledge) of thelearned face representation, a neuron selection methodis proposed to choose neurons that are most relevant toface recognition. Using the selected neurons as supervisionto mimic the single networks of DeepID2+ andDeepID3, which are the state-of-the-art face recognition systems, a compact student with simple network structure achieves better verification accuracy on LFW than its teachers, respectively. When using an ensemble of DeepID2+ as teacher, a mimicked student is able to outperform it and achieves 51.6 times compression ratio and 90 times speed-up in inference, making this cumbersome model applicable on portable devices.", "total_citations": 227, "citation_graph": {"2017": 13, "2018": 17, "2019": 37, "2020": 54, "2021": 41, "2022": 38, "2023": 26}}, {"title": "DPatch: An Adversarial Patch Attack on Object Detectors", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:Zph67rFs4hoC", "authors": ["Xin Liu", "Huanrui Yang", "Ziwei Liu", "Linghao Song", "Hai Li", "Yiran Chen"], "publication_date": "2019", "conference": "AAAI Workshop on Artificial Intelligence Safety (SafeAI)", "description": "Object detectors have emerged as an indispensable module in modern computer vision systems. In this work, we propose DPatch -- a black-box adversarial-patch-based attack towards mainstream object detectors (i.e. Faster R-CNN and YOLO). Unlike the original adversarial patch that only manipulates image-level classifier, our DPatch simultaneously attacks the bounding box regression and object classification so as to disable their predictions. Compared to prior works, DPatch has several appealing properties: (1) DPatch can perform both untargeted and targeted effective attacks, degrading the mAP of Faster R-CNN and YOLO from 75.10% and 65.7% down to below 1%, respectively. (2) DPatch is small in size and its attacking effect is location-independent, making it very practical to implement real-world attacks. (3) DPatch demonstrates great transferability among different detectors as well as training datasets. For example, DPatch that is trained on Faster R-CNN can effectively attack YOLO, and vice versa. Extensive evaluations imply that DPatch can perform effective attacks under black-box setup, i.e., even without the knowledge of the attacked network's architectures and parameters. Successful realization of DPatch also illustrates the intrinsic vulnerability of the modern detector architectures to such patch-based adversarial attacks.", "total_citations": 225, "citation_graph": {"2018": 1, "2019": 2, "2020": 36, "2021": 49, "2022": 66, "2023": 71}}, {"title": "Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:g5m5HwL7SMYC", "authors": ["Hang Zhou", "Yasheng Sun", "Wayne Wu", "Chen Change Loy", "Xiaogang Wang", "Ziwei Liu"], "publication_date": "2021", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "While accurate lip synchronization has been achieved for arbitrary-subject audio-driven talking face generation, the problem of how to efficiently drive the head pose remains. Previous methods rely on pre-estimated structural information such as landmarks and 3D parameters, aiming to generate personalized rhythmic movements. However, the inaccuracy of such estimated information under extreme conditions would lead to degradation problems. In this paper, we propose a clean yet effective framework to generate pose-controllable talking faces. We operate on non-aligned raw face images, using only a single photo as an identity reference. The key is to modularize audio-visual representations by devising an implicit low-dimension pose code. Substantially, both speech content and head pose information lie in a joint non-identity embedding space. While speech content information can be defined by learning the intrinsic synchronization between audio-visual modalities, we identify that a pose code will be complementarily learned in a modulated convolution-based reconstruction framework. Extensive experiments show that our method generates accurately lip-synced talking faces whose poses are controllable by other videos. Moreover, our model has multiple advanced capabilities including extreme view robustness and talking face frontalization.", "total_citations": 213, "citation_graph": {"2021": 14, "2022": 78, "2023": 121}}, {"title": "Fashion Landmark Detection in the Wild", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:eQOLeE2rZwMC", "authors": ["Ziwei Liu", "Sijie Yan", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang"], "publication_date": "2016", "conference": "European Conference on Computer Vision (ECCV)", "description": "Visual fashion analysis has attracted many attentions in the recent years. Previous work represented clothing regions by either bounding boxes or human joints. This work presents fashion landmark detection or fashion alignment, which is to predict the positions of functional key points defined on the fashion items, such as the corners of neckline, hemline, and cuff. To encourage future studies, we introduce a fashion landmark dataset (The dataset is available at http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/LandmarkDetection.html .) with over 120K images, where each image is labeled with eight landmarks. With this dataset, we study fashion alignment by cascading multiple convolutional neural networks in three stages. These stages gradually improve the accuracies of landmark predictions. Extensive experiments demonstrate the effectiveness of the proposed \u2026", "total_citations": 209, "citation_graph": {"2016": 1, "2017": 9, "2018": 32, "2019": 40, "2020": 38, "2021": 36, "2022": 31, "2023": 19}}, {"title": "Adaptive Affinity Field for Semantic Segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:5nxA0vEk-isC", "authors": ["Tsung-Wei Ke", "Jyh-Jing Hwang", "Ziwei Liu", "Stella X Yu"], "publication_date": "2018", "conference": "European Conference on Computer Vision (ECCV)", "description": "Existing semantic segmentation methods mostly rely on per-pixel supervision, unable to capture structural regularity present in natural images. Instead of learning to enforce semantic labels on individual pixels, we propose to enforce affinity field patterns in individual pixel neighbourhoods, ie, the semantic label patterns of whether neighbour pixels are in the same segment should match between the prediction and the ground-truth. The affinity fields thus characterize the intrinsic geometric relationships inside a given scene, such as``motorcycles have round wheels''. We further develop a novel method for learning the optimal neighbourhood size for each semantic category, with an adversarial loss that optimizes over worst-case scenarios. Unlike the popular Conditional Random Field approaches, our adaptive affinity field method has no extra parameters during inference, and is also less sensitive to input appearance changes. Extensive evaluations on Cityscapes, PASCAL VOC 2012 and GTA5 datasets demonstrate AAF provides an effective, efficient, and robust solution for semantic segmentation.", "total_citations": 208, "citation_graph": {"2018": 1, "2019": 36, "2020": 39, "2021": 63, "2022": 39, "2023": 30}}, {"title": "Seesaw Loss for Long-Tailed Instance Segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:blknAaTinKkC", "authors": ["Jiaqi Wang", "Wenwei Zhang", "Yuhang Zang", "Yuhang Cao", "Jiangmiao Pang", "Tao Gong", "Kai Chen", "Ziwei Liu", "Chen Change Loy", "Dahua Lin"], "publication_date": "2021", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Instance segmentation has witnessed a remarkable progress on class-balanced benchmarks. However, they fail to perform as accurately in real-world scenarios, where the category distribution of objects naturally comes with a long tail. Instances of head classes dominate a long-tailed dataset and they serve as negative samples of tail categories. The overwhelming gradients of negative samples on tail classes lead to a biased learning process for classifiers. Consequently, objects of tail categories are more likely to be misclassified as backgrounds or head categories. To tackle this problem, we propose Seesaw Loss to dynamically re-balance gradients of positive and negative samples for each category, with two complementary factors, ie, mitigation factor and compensation factor. The mitigation factor reduces punishments to tail categories wrt the ratio of cumulative training instances between different categories. Meanwhile, the compensation factor increases the penalty of misclassified instances to avoid false positives of tail categories. We conduct extensive experiments on Seesaw Loss with mainstream frameworks and different data sampling strategies. With a simple end-to-end training pipeline, Seesaw Loss obtains significant gains over Cross-Entropy Loss, and achieves state-of-the-art performance on LVIS dataset without bells and whistles. Code is available at https://github. com/open-mmlab/mmdetection.", "total_citations": 188, "citation_graph": {"2020": 1, "2021": 24, "2022": 75, "2023": 88}}, {"title": "Distribution-Balanced Loss for Multi-Label Classification in Long-Tailed Datasets", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:maZDTaKrznsC", "authors": ["Tong Wu", "Qingqiu Huang", "Ziwei Liu", "Yu Wang", "Dahua Lin"], "publication_date": "2020", "conference": "European Conference on Computer Vision (ECCV)", "description": "We present a new loss function called Distribution-Balanced Loss for the multi-label recognition problems that exhibit long-tailed class distributions. Compared to conventional single-label classification problem, multi-label recognition problems are often more challenging due to two significant issues, namely the co-occurrence of labels and the dominance of negative labels (when treated as multiple binary classification problems). The Distribution-Balanced Loss tackles these issues through two key modifications to the standard binary cross-entropy loss: 1) a new way to re-balance the weights that takes into account the impact caused by label co-occurrence, and 2) a negative tolerant regularization to mitigate the over-suppression of negative labels. Experiments on both Pascal VOC and COCO show that the models trained with this new loss function achieve significant performance gains over existing \u2026", "total_citations": 177, "citation_graph": {"2019": 1, "2020": 6, "2021": 36, "2022": 67, "2023": 66}}, {"title": "Online Deep Clustering for Unsupervised Representation Learning", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:r0BpntZqJG4C", "authors": ["Xiaohang Zhan", "Jiahao Xie", "Ziwei Liu", "Yew-Soon Ong", "Chen Change Loy"], "publication_date": "2020", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Joint clustering and feature learning methods have shown remarkable performance in unsupervised representation learning. However, the training schedule alternating between feature clustering and network parameters update leads to unstable learning of visual representations. To overcome this challenge, we propose Online Deep Clustering (ODC) that performs clustering and network update simultaneously rather than alternatingly. Our key insight is that the cluster centroids should evolve steadily in keeping the classifier stably updated. Specifically, we design and maintain two dynamic memory modules, ie, samples memory to store samples' labels and features, and centroids memory for centroids evolution. We break down the abrupt global clustering into steady memory update and batch-wise label re-assignment. The process is integrated into network update iterations. In this way, labels and the network evolve shoulder-to-shoulder rather than alternatingly. Extensive experiments demonstrate that ODC stabilizes the training process and boosts the performance effectively.", "total_citations": 176, "citation_graph": {"2020": 14, "2021": 46, "2022": 67, "2023": 49}}, {"title": "Deep Learning Markov Random Field for Semantic Segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:WF5omc3nYNoC", "authors": ["Ziwei Liu", "Xiaoxiao Li", "Ping Luo", "Chen Change Loy", "Xiaoou Tang"], "publication_date": "2017", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)", "description": "Semantic segmentation tasks can be well modeled by Markov Random Field (MRF). This paper addresses semantic segmentation by incorporating high-order relations and mixture of label contexts into MRF. Unlike previous works that optimized MRFs using iterative algorithm, we solve MRF by proposing a Convolutional Neural Network (CNN), namely Deep Parsing Network (DPN), which enables deterministic end-to-end computation in a single forward pass. Specifically, DPN extends a contemporary CNN to model unary terms and additional layers are devised to approximate the mean field (MF) algorithm for pairwise terms. It has several appealing properties. First, different from the recent works that required many iterations of MF during back-propagation, DPN is able to achieve high performance by approximating one iteration of MF. Second, DPN represents various types of pairwise terms, making many \u2026", "total_citations": 175, "citation_graph": {"2016": 1, "2017": 9, "2018": 17, "2019": 29, "2020": 22, "2021": 35, "2022": 41, "2023": 18}}, {"title": "PointGrow: Autoregressively Learned Point Cloud Generation with Self-Attention", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:ULOm3_A8WrAC", "authors": ["Yongbin Sun", "Yue Wang", "Ziwei Liu", "Joshua E Siegel", "Sanjay E Sarma"], "publication_date": "2020", "conference": "Winter Conference on Applications of Computer Vision (WACV)", "description": "Generating 3D point clouds is challenging yet highly desired. This work presents a novel autoregressive model, PointGrow, which can generate diverse and realistic point cloud samples from scratch or conditioned on semantic contexts. This model operates recurrently, with each point sampled according to a conditional distribution given its previously-generated points, allowing inter-point correlations to be well-exploited and 3D shape generative processes to be better interpreted. Since point cloud object shapes are typically encoded by long-range dependencies, we augment our model with dedicated self-attention modules to capture such relations. Extensive evaluations show that PointGrow achieves satisfying performance on both unconditional and conditional point cloud generation tasks, with respect to realism and diversity. Several important applications, such as unsupervised feature learning and shape arithmetic operations, are also demonstrated.", "total_citations": 155, "citation_graph": {"2019": 11, "2020": 22, "2021": 38, "2022": 42, "2023": 40}}, {"title": "When NAS Meets Robustness: In Search of Robust Architectures against Adversarial Attacks", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:TQgYirikUcIC", "authors": ["Minghao Guo", "Yuzhe Yang", "Rui Xu", "Ziwei Liu", "Dahua Lin"], "publication_date": "2020", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Recent advances in adversarial attacks uncover the intrinsic vulnerability of modern deep neural networks. Since then, extensive efforts have been devoted to enhancing the robustness of deep networks via specialized learning algorithms and loss functions. In this work, we take an architectural perspective and investigate the patterns of network architectures that are resilient to adversarial attacks. To obtain the large number of networks needed for this study, we adopt one-shot neural architecture search, training a large network for once and then finetuning the sub-networks sampled therefrom. The sampled architectures together with the accuracies they achieve provide a rich basis for our study. Our\" robust architecture Odyssey\" reveals several valuable observations: 1) densely connected patterns result in improved robustness; 2) under computational budget, adding convolution operations to direct connection edge is effective; 3) flow of solution procedure (FSP) matrix is a good indicator of network robustness. Based on these observations, we discover a family of robust architectures (RobNets). On various datasets, including CIFAR, SVHN, Tiny-ImageNet, and ImageNet, RobNets exhibit superior robustness performance to other widely used architectures. Notably, RobNets substantially improve the robust accuracy (5% absolute gains) under both white-box and black-box attacks, even with fewer parameter numbers. Code is available at https://github. com/gmh14/RobNets.", "total_citations": 149, "citation_graph": {"2020": 12, "2021": 48, "2022": 41, "2023": 47}}, {"title": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:uLbwQdceFCQC", "authors": ["Ce Zhou", "Qian Li", "Chen Li", "Jun Yu", "Yixin Liu", "Guangjing Wang", "Kai Zhang", "Cheng Ji", "Qiben Yan", "Lifang He", "Hao Peng", "Jianxin Li", "Jia Wu", "Ziwei Liu", "Pengtao Xie", "Caiming Xiong", "Jian Pei", "Philip S Yu", "Lichao Sun"], "publication_date": "2023/2/18", "description": "The Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A pretrained foundation model, such as BERT, GPT-3, MAE, DALLE-E, and ChatGPT, is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. The idea of pretraining behind PFMs plays an important role in the application of large models. Different from previous methods that apply convolution and recurrent modules for feature extractions, the generative pre-training (GPT) method applies Transformer as the feature extractor and is trained on large datasets with an autoregressive paradigm. Similarly, the BERT apples transformers to train on large datasets as a contextual language model. Recently, the ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few show prompting. With the extraordinary success of PFMs, AI has made waves in a variety of fields over the past few years. Considerable methods, datasets, and evaluation metrics have been proposed in the literature, the need is raising for an updated survey. This study provides a comprehensive review of recent research advancements, current and future challenges, and opportunities for PFMs in text, image, graph, as well as other data modalities. We first review the basic components and existing pretraining in natural language processing, computer vision, and graph learning. We then discuss other advanced PFMs for other data modalities and unified PFMs considering the data quality and quantity. Besides \u2026", "total_citations": 143, "citation_graph": {"2022": 1, "2023": 140}}, {"title": "CelebA-Spoof: Large-Scale Face Anti-Spoofing Dataset with Rich Annotations", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:M3NEmzRMIkIC", "authors": ["Yuanhan Zhang", "Zhenfei Yin", "Yidong Li", "Guojun Yin", "Junjie Yan", "Jing Shao", "Ziwei Liu"], "publication_date": "2020", "conference": "European Conference on Computer Vision (ECCV)", "description": "As facial interaction systems are prevalently deployed, security and reliability of these systems become a critical issue, with substantial research efforts devoted. Among them, face anti-spoofing emerges as an important area, whose objective is to identify whether a presented face is live or spoof. Though promising progress has been achieved, existing works still have difficulty in handling complex spoof attacks and generalizing to real-world scenarios. The main reason is that current face anti-spoofing datasets are limited in both quantity and diversity. To overcome these obstacles, we contribute a large-scale face anti-spoofing dataset, CelebA-Spoof, with the following appealing properties: 1) Quantity: CelebA-Spoof comprises of 625,537 pictures of 10,177 subjects, significantly larger than the existing datasets. 2) Diversity: The spoof images are captured from 8 scenes (2 environments * 4 illumination \u2026", "total_citations": 131, "citation_graph": {"2020": 3, "2021": 25, "2022": 56, "2023": 46}}, {"title": "Open Compound Domain Adaptation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:R3hNpaxXUhUC", "authors": ["Ziwei Liu", "Zhongqi Miao", "Xingang Pan", "Xiaohang Zhan", "Dahua Lin", "Stella X Yu", "Boqing Gong"], "publication_date": "2020", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "A typical domain adaptation approach is to adapt models trained on the annotated data in a source domain (eg, sunny weather) for achieving high performance on the test data in a target domain (eg, rainy weather). Whether the target contains a single homogeneous domain or multiple heterogeneous domains, existing works always assume that there exist clear distinctions between the domains, which is often not true in practice (eg, changes in weather). We study an open compound domain adaptation (OCDA) problem, in which the target is a compound of multiple homogeneous domains without domain labels, reflecting realistic data collection from mixed and novel situations. We propose a new approach based on two technical insights into OCDA: 1) a curriculum domain adaptation strategy to bootstrap generalization across domains in a data-driven self-organizing fashion and 2) a memory module to increase the model's agility towards novel domains. Our experiments on digit classification, facial expression recognition, semantic segmentation, and reinforcement learning demonstrate the effectiveness of our approach.", "total_citations": 130, "citation_graph": {"2019": 1, "2020": 15, "2021": 34, "2022": 39, "2023": 40}}, {"title": "MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:HE397vMXCloC", "authors": ["Mingyuan Zhang", "Zhongang Cai", "Liang Pan", "Fangzhou Hong", "Xinying Guo", "Lei Yang", "Ziwei Liu"], "publication_date": "2022/8/31", "journal": "arXiv preprint arXiv:2208.15001", "description": "Human motion modeling is important for many modern graphics applications, which typically require professional skills. In order to remove the skill barriers for laymen, recent motion generation methods can directly generate human motions conditioned on natural languages. However, it remains challenging to achieve diverse and fine-grained motion generation with various text inputs. To address this problem, we propose MotionDiffuse, the first diffusion model-based text-driven motion generation framework, which demonstrates several desired properties over existing methods. 1) Probabilistic Mapping. Instead of a deterministic language-motion mapping, MotionDiffuse generates motions through a series of denoising steps in which variations are injected. 2) Realistic Synthesis. MotionDiffuse excels at modeling complicated data distribution and generating vivid motion sequences. 3) Multi-Level Manipulation. MotionDiffuse responds to fine-grained instructions on body parts, and arbitrary-length motion synthesis with time-varied text prompts. Our experiments show MotionDiffuse outperforms existing SoTA methods by convincing margins on text-driven motion generation and action-conditioned motion generation. A qualitative analysis further demonstrates MotionDiffuse's controllability for comprehensive motion generation. Homepage: https://mingyuan-zhang.github.io/projects/MotionDiffuse.html", "total_citations": 123, "citation_graph": {"2021": 1, "2022": 6, "2023": 116}}, {"title": "Fast Burst Images Denoising", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:zYLM7Y9cAGgC", "authors": ["Ziwei Liu", "Lu Yuan", "Xiaoou Tang", "Matt Uyttendaele", "Jian Sun"], "publication_date": "2014/11/14", "journal": "ACM Transactions on Graphics (TOG)", "description": "This paper presents a fast denoising method that produces a clean image from a burst of noisy images. We accelerate alignment of the images by introducing a lightweight camera motion representation called homography flow. The aligned images are then fused to create a denoised output with rapid per-pixel operations in temporal and spatial domains. To handle scene motion during the capture, a mechanism of selecting consistent pixels for temporal fusion is proposed to \"synthesize\" a clean, ghost-free image, which can largely reduce the computation of tracking motion between frames. Combined with these efficient solutions, our method runs several orders of magnitude faster than previous work, while the denoising quality is comparable. A smartphone prototype demonstrates that our method is practical and works well on a large variety of real examples.", "total_citations": 118, "citation_graph": {"2015": 2, "2016": 7, "2017": 8, "2018": 14, "2019": 18, "2020": 22, "2021": 16, "2022": 18, "2023": 12}}, {"title": "Variational Relational Point Completion Network", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:M05iB0D1s5AC", "authors": ["Liang Pan", "Xinyi Chen", "Zhongang Cai", "Junzhe Zhang", "Haiyu Zhao", "Shuai Yi", "Ziwei Liu"], "publication_date": "2021", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Real-scanned point clouds are often incomplete due to viewpoint, occlusion, and noise. Existing point cloud completion methods tend to generate global shape skeletons and hence lack fine local details. Furthermore, they mostly learn a deterministic partial-to-complete mapping, but overlook structural relations in man-made objects. To tackle these challenges, this paper proposes a variational framework, Variational Relational point Completion network (VRCNet) with two appealing properties: 1) Probabilistic Modeling. In particular, we propose a dual-path architecture to enable principled probabilistic modeling across partial and complete clouds. One path consumes complete point clouds for reconstruction by learning a point VAE. The other path generates complete shapes for partial point clouds, whose embedded distribution is guided by distribution obtained from the reconstruction path during training. 2) Relational Enhancement. Specifically, we carefully design point self-attention kernel and point selective kernel module to exploit relational point features, which refines local shape details conditioned on the coarse completion. In addition, we contribute a multi-view partial point cloud dataset (MVP dataset) containing over 100,000 high-quality scans, which renders partial 3D shapes from 26 uniformly distributed camera poses for each 3D CAD model. Extensive experiments demonstrate that VRCNet outperforms state-of-the-art methods on all standard point cloud completion benchmarks. Notably, VRCNet shows great generalizability and robustness on real-world point cloud scans.", "total_citations": 115, "citation_graph": {"2021": 6, "2022": 60, "2023": 49}}, {"title": "Self-Supervised Scene De-occlusion", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:4JMBOYKVnBMC", "authors": ["Xiaohang Zhan", "Xingang Pan", "Bo Dai", "Ziwei Liu", "Dahua Lin", "Chen Change Loy"], "publication_date": "2020", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Natural scene understanding is a challenging task, particularly when encountering images of multiple objects that are partially occluded. This obstacle is given rise by varying object ordering and positioning. Existing scene understanding paradigms are able to parse only the visible parts, resulting in incomplete and unstructured scene interpretation. In this paper, we investigate the problem of scene de-occlusion, which aims to recover the underlying occlusion ordering and complete the invisible parts of occluded objects. We make the first attempt to address the problem through a novel and unified framework that recovers hidden scene structures without ordering and amodal annotations as supervisions. This is achieved via Partial Completion Network (PCNet)-mask (M) and-content (C), that learn to recover fractions of object masks and contents, respectively, in a self-supervised manner. Based on PCNet-M and PCNet-C, we devise a novel inference scheme to accomplish scene de-occlusion, via progressive ordering recovery, amodal completion and content completion. Extensive experiments on real-world scenes demonstrate the superior performance of our approach to other alternatives. Remarkably, our approach that is trained in a self-supervised manner achieves comparable results to fully-supervised methods. The proposed scene de-occlusion framework benefits many applications, including high-quality and controllable image manipulation and scene recomposition (see Fig. 1), as well as the conversion of existing modal mask annotations to amodal mask annotations.", "total_citations": 109, "citation_graph": {"2020": 8, "2021": 19, "2022": 46, "2023": 36}}, {"title": "AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:sSrBHYA8nusC", "authors": ["Fangzhou Hong", "Mingyuan Zhang", "Liang Pan", "Zhongang Cai", "Lei Yang", "Ziwei Liu"], "publication_date": "2022", "journal": "ACM Transactions on Graphics (TOG)", "description": "3D avatar creation plays a crucial role in the digital age. However, the whole production process is prohibitively time-consuming and labor-intensive. To democratize this technology to a larger audience, we propose AvatarCLIP, a zero-shot text-driven framework for 3D avatar generation and animation. Unlike professional software that requires expert knowledge, AvatarCLIP empowers layman users to customize a 3D avatar with the desired shape and texture, and drive the avatar with the described motions using solely natural languages. Our key insight is to take advantage of the powerful vision-language model CLIP for supervising neural human generation, in terms of 3D geometry, texture and animation. Specifically, driven by natural language descriptions, we initialize 3D human geometry generation with a shape VAE network. Based on the generated 3D human shapes, a volume rendering model is utilized to further facilitate geometry sculpting and texture generation. Moreover, by leveraging the priors learned in the motion VAE, a CLIP-guided reference-based motion synthesis method is proposed for the animation of the generated 3D avatar. Extensive qualitative and quantitative experiments validate the effectiveness and generalizability of AvatarCLIP on a wide range of avatars. Remarkably, AvatarCLIP can generate unseen 3D avatars with novel animations, achieving superior zero-shot capability.", "total_citations": 106, "citation_graph": {"2021": 1, "2022": 15, "2023": 89}}, {"title": "Rotate-and-Render: Unsupervised Photorealistic Face Rotation from Single-View Images", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:_Qo2XoVZTnwC", "authors": ["Hang Zhou", "Jihao Liu", "Ziwei Liu", "Yu Liu", "Xiaogang Wang"], "publication_date": "2020", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Though face rotation has achieved rapid progress in recent years, the lack of high-quality paired training data remains a great hurdle for existing methods. The current generative models heavily rely on datasets with multi-view images of the same person. Thus, their generated results are restricted by the scale and domain of the data source. To overcome these challenges, we propose a novel unsupervised framework that can synthesize photo-realistic rotated faces using only single-view image collections in the wild. Our key insight is that rotating faces in the 3D space back and forth, and re-rendering them to the 2D plane can serve as a strong self-supervision. We leverage the recent advances in 3D face modeling and high-resolution GAN to constitute our building blocks. Since the 3D rotation-and-render on faces can be applied to arbitrary angles without losing details, our approach is extremely suitable for in-the-wild scenarios (ie no paired data are available), where existing methods fall short. Extensive experiments demonstrate that our approach has superior synthesis quality as well as identity preservation over the state-of-the-art methods, across a wide range of poses and domains. Furthermore, we validate that our rotate-and-render framework naturally can act as an effective data augmentation engine for boosting modern face recognition systems even on strong baseline models", "total_citations": 103, "citation_graph": {"2020": 8, "2021": 34, "2022": 33, "2023": 27}}, {"title": "Insights and Approaches using Deep Learning to Classify Wildlife", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:mVmsd5A6BfQC", "authors": ["Zhongqi Miao", "Kaitlyn M Gaynor", "Jiayun Wang", "Ziwei Liu", "Oliver Muellerklein", "Mohammad Sadegh Norouzzadeh", "Alex McInturff", "Rauri CK Bowie", "Ran Nathan", "X Yu Stella", "Wayne M Getz"], "publication_date": "2019/5/31", "journal": "Nature - Scientific Reports", "description": "The implementation of intelligent software to identify and classify objects and individuals in visual fields is a technology of growing importance to operatives in many fields, including wildlife conservation and management. To non-experts, the methods can be abstruse and the results mystifying. Here, in the context of applying cutting edge methods to classify wildlife species from camera-trap data, we shed light on the methods themselves and types of features these methods extract to make efficient identifications and reliable classifications. The current state of the art is to employ convolutional neural networks (CNN) encoded within deep-learning algorithms. We outline these methods and present results obtained in training a CNN to classify 20 African wildlife species with an overall accuracy of 87.5% from a dataset containing 111,467 images. We demonstrate the application of a gradient-weighted class-activation \u2026", "total_citations": 101, "citation_graph": {"2018": 1, "2019": 2, "2020": 22, "2021": 23, "2022": 33, "2023": 20}}, {"title": "Do 2D GANs Know 3D Shape? Unsupervised 3D shape reconstruction from 2D Image GANs", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:GnPB-g6toBAC", "authors": ["Xingang Pan", "Bo Dai", "Ziwei Liu", "Chen Change Loy", "Ping Luo"], "publication_date": "2021", "conference": "International Conference on Learning Representations (ICLR)", "description": "Natural images are projections of 3D objects on a 2D image plane. While state-of-the-art 2D generative models like GANs show unprecedented quality in modeling the natural image manifold, it is unclear whether they implicitly capture the underlying 3D object structures. And if so, how could we exploit such knowledge to recover the 3D shapes of objects in the images? To answer these questions, in this work, we present the first attempt to directly mine 3D geometric cues from an off-the-shelf 2D GAN that is trained on RGB images only. Through our investigation, we found that such a pre-trained GAN indeed contains rich 3D knowledge and thus can be used to recover 3D shape from a single 2D image in an unsupervised manner. The core of our framework is an iterative strategy that explores and exploits diverse viewpoint and lighting variations in the GAN image manifold. The framework does not require 2D keypoint or 3D annotations, or strong assumptions on object shapes (e.g. shapes are symmetric), yet it successfully recovers 3D shapes with high precision for human faces, cats, cars, and buildings. The recovered 3D shapes immediately allow high-quality image editing like relighting and object rotation. We quantitatively demonstrate the effectiveness of our approach compared to previous methods in both 3D shape reconstruction and face rotation. Our code is available at https://github.com/XingangPan/GAN2Shape.", "total_citations": 97, "citation_graph": {"2019": 1, "2020": 0, "2021": 24, "2022": 37, "2023": 35}}, {"title": "Vision-Infused Deep Audio Inpainting", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:mB3voiENLucC", "authors": ["Hang Zhou", "Ziwei Liu", "Xudong Xu", "Ping Luo", "Xiaogang Wang"], "publication_date": "2019", "conference": "IEEE International Conference on Computer Vision (ICCV)", "description": "Multi-modality perception is essential to develop interactive intelligence. In this work, we consider a new task of visual information-infused audio inpainting, ie, synthesizing missing audio segments that correspond to their accompanying videos. We identify two key aspects for a successful inpainter:(1) It is desirable to operate on spectrograms instead of raw audios. Recent advances in deep semantic image inpainting could be leveraged to go beyond the limitations of traditional audio inpainting.(2) To synthesize visually indicated audio, a visual-audio joint feature space needs to be learned with synchronization of audio and video. To facilitate a large-scale study, we collect a new multi-modality instrument-playing dataset called MUSIC-Extra-Solo (MUSICES) by enriching MUSIC dataset. Extensive experiments demonstrate that our framework is capable of inpainting realistic and varying audio segments with or without visual contexts. More importantly, our synthesized audio segments are coherent with their video counterparts, showing the effectiveness of our proposed Vision-Infused Audio Inpainter (VIAI).", "total_citations": 88, "citation_graph": {"2020": 16, "2021": 23, "2022": 33, "2023": 16}}, {"title": "Consensus-Driven Propagation in Massive Unlabeled Data for Face Recognition", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:KlAtU1dfN6UC", "authors": ["Xiaohang Zhan", "Ziwei Liu", "Junjie Yan", "Dahua Lin", "Chen Change Loy"], "publication_date": "2018", "conference": "Proceedings of the European Conference on Computer Vision (ECCV)", "description": "Face recognition has witnessed great progresses in recent years, mainly attributed to the high-capacity model designed and the abundant labeled data collected. However, it becomes more and more prohibitive to scale up the current million-level identity annotations. In this work, we show that unlabeled face data can be as effective as the labeled ones. Here, we consider a setting closely mimicking the real-world scenario, where the unlabeled data are collected from unconstrained environment and their identities are exclusive from the labeled ones. Our main insight is that although the class information is not available, we can still faithfully approximate these semantic relationship by constructing a relational graph in a bottom-up manner. We propose Consensus-Driven Propagation (CDP) to tackle this challenging problem with two well-designed modules, the\" committee\" and the\" mediator\", which select positive face pairs robustly by carefully aggregating multi-view information. Extensive experiments validate the effectiveness of both modules to discard outliers and mine hard positives. With CDP, we achieve a compelling 78.18% on MegaFace identification challenge by using only 9% of the labels, comparing to 61.78% when no unlabeled data are used and 78.52% when all the labels are employed.", "total_citations": 88, "citation_graph": {"2018": 1, "2019": 5, "2020": 20, "2021": 24, "2022": 20, "2023": 15}}, {"title": "Video Object Segmentation with Re-identification", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:roLk4NBRz8UC", "authors": ["Xiaoxiao Li", "Yuankai Qi", "Zhe Wang", "Kai Chen", "Ziwei Liu", "Jianping Shi", "Ping Luo", "Xiaoou Tang", "Chen Change Loy"], "publication_date": "2017", "conference": "CVPR Workshop on DAVIS Video Segmentation Challenge 2017", "description": "Conventional video segmentation methods often rely on temporal continuity to propagate masks. Such an assumption suffers from issues like drifting and inability to handle large displacement. To overcome these issues, we formulate an effective mechanism to prevent the target from being lost via adaptive object re-identification. Specifically, our Video Object Segmentation with Re-identification (VS-ReID) model includes a mask propagation module and a ReID module. The former module produces an initial probability map by flow warping while the latter module retrieves missing instances by adaptive matching. With these two modules iteratively applied, our VS-ReID records a global mean (Region Jaccard and Boundary F measure) of 0.699, the best performance in 2017 DAVIS Challenge.", "total_citations": 82, "citation_graph": {"2017": 2, "2018": 23, "2019": 21, "2020": 14, "2021": 7, "2022": 10, "2023": 5}}, {"title": "Semantic Facial Expression Editing using Autoencoded Flow", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:ufrVoPGSRksC", "authors": ["Raymond Yeh", "Ziwei Liu", "Dan B Goldman", "Aseem Agarwala"], "publication_date": "2016/11/30", "journal": "arXiv preprint arXiv:1611.09961", "description": "High-level manipulation of facial expressions in images --- such as changing a smile to a neutral expression --- is challenging because facial expression changes are highly non-linear, and vary depending on the appearance of the face. We present a fully automatic approach to editing faces that combines the advantages of flow-based face manipulation with the more recent generative capabilities of Variational Autoencoders (VAEs). During training, our model learns to encode the flow from one expression to another over a low-dimensional latent space. At test time, expression editing can be done simply using latent vector arithmetic. We evaluate our methods on two applications: 1) single-image facial expression editing, and 2) facial expression interpolation between two images. We demonstrate that our method generates images of higher perceptual quality than previous VAE and flow-based methods.", "total_citations": 80, "citation_graph": {"2017": 6, "2018": 18, "2019": 18, "2020": 15, "2021": 10, "2022": 9, "2023": 3}}, {"title": "Unsupervised Feature Learning by Cross-Level Instance-Group Discrimination", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:JV2RwH3_ST0C", "authors": ["Xudong Wang", "Ziwei Liu", "Stella X Yu"], "publication_date": "2021", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Unsupervised feature learning has made great strides with contrastive learning based on instance discrimination and invariant mapping, as benchmarked on curated class-balanced datasets. However, natural data could be highly correlated and long-tail distributed. Natural between-instance similarity conflicts with the presumed instance distinction, causing unstable training and poor performance. Our idea is to discover and integrate between-instance similarity into contrastive learning, not directly by instance grouping, but by cross-level discrimination (CLD) between instances and local instance groups. While invariant mapping of each instance is imposed by attraction within its augmented views, between-instance similarity emerges from common repulsion against instance groups. Our batch-wise and cross-view comparisons also greatly improve the positive/negative sample ratio of contrastive learning and achieve better invariant mapping. To effect both grouping and discrimination objectives, we impose them on features separately derived from a shared representation. In addition, we propose normalized projection heads and unsupervised hyper-parameter tuning for the first time. Our extensive experimentation demonstrates that CLD is a lean and powerful add-on to existing methods (eg, NPID, MoCo, InfoMin, BYOL) on highly correlated, long-tail, or balanced datasets. It not only achieves new state-of-the-art on self-supervision, semi-supervision, and transfer learning benchmarks, but also beats MoCo v2 and SimCLR on every reported performance attained with a much larger compute. CLD effectively extends unsupervised learning to \u2026", "total_citations": 75, "citation_graph": {"2021": 14, "2022": 38, "2023": 23}}, {"title": "LiDAR-based Panoptic Segmentation via Dynamic Shifting Network", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:O3NaXMp0MMsC", "authors": ["Fangzhou Hong", "Hui Zhou", "Xinge Zhu", "Hongsheng Li", "Ziwei Liu"], "publication_date": "2021", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "With the rapid advances of autonomous driving, it becomes critical to equip its sensing system with more holistic 3D perception. However, existing works focus on parsing either the objects (eg cars and pedestrians) or scenes (eg trees and buildings) from the LiDAR sensor. In this work, we address the task of LiDAR-based panoptic segmentation, which aims to parse both objects and scenes in a unified manner. As one of the first endeavors towards this new challenging task, we propose the Dynamic Shifting Network (DS-Net), which serves as an effective panoptic segmentation framework in the point cloud realm. In particular, DS-Net has three appealing properties: 1) strong backbone design. DS-Net adopts the cylinder convolution that is specifically designed for LiDAR point clouds. The extracted features are shared by the semantic branch and the instance branch which operates in a bottom-up clustering style. 2) Dynamic Shifting for complex point distributions. We observe that commonly-used clustering algorithms like BFS or DBSCAN are incapable of handling complex autonomous driving scenes with non-uniform point cloud distributions and varying instance sizes. Thus, we present an efficient learnable clustering module, dynamic shifting, which adapts kernel functions on-the-fly for different instances. 3) Consensus-driven Fusion. Finally, consensus-driven fusion is used to deal with the disagreement between semantic and instance predictions. To comprehensively evaluate the performance of LiDAR-based panoptic segmentation, we construct and curate benchmarks from two large-scale autonomous driving LiDAR datasets, SemanticKITTI \u2026", "total_citations": 72, "citation_graph": {"2021": 9, "2022": 31, "2023": 32}}, {"title": "Vision-Based Calibration of Dual RCM-Based Robot Arms in Human-Robot Collaborative Minimally Invasive Surgery", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:UebtZRa9Y70C", "authors": ["Zerui Wang", "Ziwei Liu", "Qianli Ma", "Alexis Cheng", "Yun-Hui Liu", "Sungmin Kim", "Anton Deguet", "Austin Reiter", "Kazanzides Peter", "Russell H Taylor"], "publication_date": "2017", "journal": "IEEE Robotics and Automation Letters (RA-L)", "description": "This letter reports the development of a vision-based calibration method for dual remote center-of-motion (RCM) based robot arms in a human-robot collaborative minimally invasive surgery (MIS) scenario. The method does not require any external tracking sensors and directly uses images captured by the endoscopic camera and the robot encoder readings as calibration data, which leads to a minimal and practical system in the operating room. By taking advantage of the motion constraints imposed by the RCM-based kinematics of the robotic surgical tools and cameras, we can find unique relationships between the endoscope and the surgical tool using camera perspective projection geometry without the geometric information of the tool. A customized vision-based centerline detection algorithm is also proposed, which provides robust estimation of centerline positions for a variety of settings. We validate the \u2026", "total_citations": 71, "citation_graph": {"2017": 1, "2018": 7, "2019": 11, "2020": 11, "2021": 17, "2022": 12, "2023": 10}}, {"title": "Unconstrained Fashion Landmark Detection via Hierarchical Recurrent Transformer Networks", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:Se3iqnhoufwC", "authors": ["Sijie Yan", "Ziwei Liu", "Ping Luo", "Shi Qiu", "Xiaogang Wang", "Xiaoou Tang"], "publication_date": "2017", "conference": "ACM Multimedia (ACM MM)", "description": "Fashion landmarks are functional key points defined on clothes, such as corners of neckline, hemline, and cuff. They have been recently introduced [18]as an effective visual representation for fashion image understanding. However, detecting fashion landmarks are challenging due to background clutters, human poses, and scales. To remove the above variations, previous works usually assumed bounding boxes of clothes are provided in training and test as additional annotations, which are expensive to obtain and inapplicable in practice. This work addresses unconstrained fashion landmark detection, where clothing bounding boxes are not provided in both training and test. To this end, we present a novel Deep LAndmark Network (DLAN), where bounding boxes and landmarks are jointly estimated and trained iteratively in an end-to-end manner. DLAN contains two dedicated modules, including a Selective \u2026", "total_citations": 71, "citation_graph": {"2018": 8, "2019": 16, "2020": 15, "2021": 17, "2022": 9, "2023": 6}}, {"title": "OpenOOD: Benchmarking Generalized Out-of-Distribution Detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:SdhP9T11ey4C", "authors": ["Jingkang Yang", "Pengyun Wang", "Dejian Zou", "Zitang Zhou", "Kunyuan Ding", "Wenxuan Peng", "Haoqi Wang", "Guangyao Chen", "Bo Li", "Yiyou Sun", "Xuefeng Du", "Kaiyang Zhou", "Wayne Zhang", "Dan Hendrycks", "Yixuan Li", "Ziwei Liu"], "publication_date": "2022", "conference": "NeurIPS (Datasets and Benchmarks Track)", "description": "Out-of-distribution (OOD) detection is vital to safety-critical machine learning applications and has thus been extensively studied, with a plethora of methods developed in the literature. However, the field currently lacks a unified, strictly formulated, and comprehensive benchmark, which often results in unfair comparisons and inconclusive results. From the problem setting perspective, OOD detection is closely related to neighboring fields including anomaly detection (AD), open set recognition (OSR), and model uncertainty, since methods developed for one domain are often applicable to each other. To help the community to improve the evaluation and advance, we build a unified, well-structured codebase called OpenOOD, which implements over 30 methods developed in relevant fields and provides a comprehensive benchmark under the recently proposed generalized OOD detection framework. With a comprehensive comparison of these methods, we are gratified that the field has progressed significantly over the past few years, where both preprocessing methods and the orthogonal post-hoc methods show strong potential.", "total_citations": 69, "citation_graph": {"2022": 6, "2023": 63}}, {"title": "Density-aware Chamfer Distance as a Comprehensive Metric for Point Cloud Completion", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:xtRiw3GOFMkC", "authors": ["Tong Wu", "Liang Pan", "Junzhe Zhang", "WANG Tai", "Ziwei Liu", "Dahua Lin"], "publication_date": "2021/5/21", "conference": "Thirty-Fifth Conference on Neural Information Processing Systems (NeurIPS)", "description": "Chamfer Distance (CD) and Earth Mover's Distance (EMD) are two broadly adopted metrics for measuring the similarity between two point sets. However, CD is usually insensitive to mismatched local density, and EMD is usually dominated by global distribution while overlooks the fidelity of detailed structures. Besides, their unbounded value range induces a heavy influence from the outliers. These defects prevent them from providing a consistent evaluation. To tackle these problems, we propose a new similarity measure named Density-aware Chamfer Distance (DCD). It is derived from CD and benefits from several desirable properties: 1) it can detect disparity of density distributions and is thus a more intensive measure of similarity compared to CD; 2) it is stricter with detailed structures and significantly more computationally efficient than EMD; 3) the bounded value range encourages a more stable and reasonable evaluation over the whole test set. We adopt DCD to evaluate the point cloud completion task, where experimental results show that DCD pays attention to both the overall structure and local geometric details and provides a more reliable evaluation even when CD and EMD contradict each other. We can also use DCD as the training loss, which outperforms the same model trained with CD loss on all three metrics. In addition, we propose a novel point discriminator module that estimates the priority for another guided down-sampling step, and it achieves noticeable improvements under DCD together with competitive results for both CD and EMD. We hope our work could pave the way for a more comprehensive and practical point \u2026", "total_citations": 69, "citation_graph": {"2021": 1, "2022": 21, "2023": 47}}, {"title": "ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:J_g5lzvAfSwC", "authors": ["Yinan He", "Bei Gan", "Siyu Chen", "Yichun Zhou", "Guojun Yin", "Luchuan Song", "Lu Sheng", "Jing Shao", "Ziwei Liu"], "publication_date": "2021", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "The rapid progress of photorealistic synthesis techniques has reached at a critical point where the boundary between real and manipulated images starts to blur. Thus, benchmarking and advancing digital forgery analysis have become a pressing issue. However, existing face forgery datasets either have limited diversity or only support coarse-grained analysis. To counter this emerging threat, we construct the ForgeryNet dataset, an extremely large face forgery dataset with unified annotations in image-and video-level data across four tasks: 1) Image Forgery Classification, including two-way (real/fake), three-way (real/fake with identity-replaced forgery approaches/fake with identity-remained forgery approaches), and n-way (real and 15 respective forgery approaches) classification. 2) Spatial Forgery Localization, which segments the manipulated area of fake images compared to their corresponding source real images. 3) Video Forgery Classification, which re-defines the video-level forgery classification with manipulated frames in random positions. This task is important because attackers in real world are free to manipulate any target frame. and 4) Temporal Forgery Localization, to localize the temporal segments which are manipulated. ForgeryNet is by far the largest publicly available deep face forgery dataset in terms of data-scale (2.9 million images, 221,247 videos), manipulations (7 image-level approaches, 8 video-level approaches), perturbations (36 independent and more mixed perturbations) and annotations (6.3 million classification labels, 2.9 million manipulated area annotations and 221,247 temporal forgery segment labels). We \u2026", "total_citations": 69, "citation_graph": {"2021": 2, "2022": 25, "2023": 42}}, {"title": "Sep-Stereo: Visually Guided Stereophonic Audio Generation by Associating Source Separation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:k_IJM867U9cC", "authors": ["Hang Zhou", "Xudong Xu", "Dahua Lin", "Xiaogang Wang", "Ziwei Liu"], "publication_date": "2020", "conference": "European Conference on Computer Vision (ECCV)", "description": "Stereophonic audio is an indispensable ingredient to enhance human auditory experience. Recent research has explored the usage of visual information as guidance to generate binaural or ambisonic audio from mono ones with stereo supervision. However, this fully supervised paradigm suffers from an inherent drawback: the recording of stereophonic audio usually requires delicate devices that are expensive for wide accessibility. To overcome this challenge, we propose to leverage the vastly available mono data to facilitate the generation of stereophonic audio. Our key observation is that the task of visually indicated audio separation also maps independent audios to their corresponding visual positions, which shares a similar objective with stereophonic audio generation. We integrate both stereo generation and source separation into a unified framework, Sep-Stereo, by considering source separation \u2026", "total_citations": 69, "citation_graph": {"2020": 5, "2021": 20, "2022": 24, "2023": 20}}, {"title": "Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:Tiz5es2fbqcC", "authors": ["Li Siyao", "Weijiang Yu", "Tianpei Gu", "Chunze Lin", "Quan Wang", "Chen Qian", "Chen Change Loy", "Ziwei Liu"], "publication_date": "2022", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Driving 3D characters to dance following a piece of music is highly challenging due to the spatial constraints applied to poses by choreography norms. In addition, the generated dance sequence also needs to maintain temporal coherency with different music genres. To tackle these challenges, we propose a novel music-to-dance framework, Bailando, with two powerful components: 1) a choreographic memory that learns to summarize meaningful dancing units from 3D pose sequence to a quantized codebook, 2) an actor-critic Generative Pre-trained Transformer (GPT) that composes these units to a fluent dance coherent to the music. With the learned choreographic memory, dance generation is realized on the quantized units that meet high choreography standards, such that the generated dancing sequences are confined within the spatial constraints. To achieve synchronized alignment between diverse motion tempos and music beats, we introduce an actor-critic-based reinforcement learning scheme to the GPT with a newly-designed beat-align reward function. Extensive experiments on the standard benchmark demonstrate that our proposed framework achieves state-of-the-art performance both qualitatively and quantitatively. Notably, the learned choreographic memory is shown to discover human-interpretable dancing-style poses in an unsupervised manner.", "total_citations": 66, "citation_graph": {"2022": 9, "2023": 57}}, {"title": "Deep Animation Video Interpolation in the Wild", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:35N4QoGY0k4C", "authors": ["Li Siyao", "Shiyu Zhao", "Weijiang Yu", "Wenxiu Sun", "Dimitris N Metaxas", "Chen Change Loy", "Ziwei Liu"], "publication_date": "2021", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "In the animation industry, cartoon videos are usually produced at low frame rate since hand drawing of such frames is costly and time-consuming. Therefore, it is desirable to develop computational models that can automatically interpolate the in-between animation frames. However, existing video interpolation methods fail to produce satisfying results on animation data. Compared to natural videos, animation videos possess two unique characteristics that make frame interpolation difficult: 1) cartoons comprise lines and smooth color pieces. The smooth areas lack textures and make it difficult to estimate accurate motions on animation videos. 2) cartoons express stories via exaggeration. Some of the motions are non-linear and extremely large. In this work, we formally define and study the animation video interpolation problem for the first time. To address the aforementioned challenges, we propose an effective framework, AnimeInterp, with two dedicated modules in a coarse-to-fine manner. Specifically, 1) Segment-Guided Matching resolves the\" lack of textures\" challenge by exploiting global matching among color pieces that are piece-wise coherent. 2) Recurrent Flow Refinement resolves the\" non-linear and extremely large motion\" challenge by recurrent predictions using a transformer-like architecture. To facilitate comprehensive training and evaluations, we build a large-scale animation triplet dataset, ATD-12K, which comprises 12,000 triplets with rich annotations. Extensive experiments demonstrate that our approach outperforms existing state-of-the-art interpolation methods for animation videos. Notably, AnimeInterp shows favorable \u2026", "total_citations": 65, "citation_graph": {"2021": 2, "2022": 28, "2023": 35}}, {"title": "Neural Prompt Search", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:fQNAKQ3IYiAC", "authors": ["Yuanhan Zhang", "Kaiyang Zhou", "Ziwei Liu"], "publication_date": "2022/6/9", "journal": "arXiv preprint arXiv:2206.04673", "description": "Many recent efforts center on assessing the ability of real-world evidence (RWE) generated from nonrandomized observational data to provide results that are compatible with those from randomized controlled trials (RCTs). One noticeable endeavor is the RCT DUPLICATE initiative (Franklin et al., 2020). To better reconcile findings from observational and trial data, it is desirable to eliminate differences between the RCT and corresponding observational study populations. We outline an efficient, network-flow-based statistical matching algorithm that designs well-matched pairs from observational data that mimic the covariates' distribution of a target population, e.g., the RCT study population or a population of scientific interest. We demonstrate the usefulness of the method by revisiting the inconsistency regarding a cardioprotective effect of the hormone replacement therapy (HRT) in the Women's Health Initiative (WHI) clinical trial and corresponding observational study. We found that the discrepancy between the trial and observational study persisted in a design that adjusted for study populations' cardiovascular risk profile, but seemed to disappear in a study design that further adjusted for the HRT initiation age and previous estrogen-plus-progestin use. The proposed method is integrated into the R package match2C.", "total_citations": 64, "citation_graph": {"2022": 11, "2023": 53}}, {"title": "StyleGAN-Human: A Data-Centric Odyssey of Human Generation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:tS2w5q8j5-wC", "authors": ["Jianglin Fu", "Shikai Li", "Yuming Jiang", "Kwan-Yee Lin", "Chen Qian", "Chen Change Loy", "Wayne Wu", "Ziwei Liu"], "publication_date": "2022", "conference": "European Conference on Computer Vision (ECCV)", "description": "Unconditional human image generation is an important task in vision and graphics, enabling various applications in the creative industry. Existing studies in this field mainly focus on \u201cnetwork engineering\u201d such as designing new components and objective functions. This work takes a data-centric perspective and investigates multiple critical aspects in \u201cdata engineering\u201d, which we believe would complement the current practice. To facilitate a comprehensive study, we collect and annotate a large-scale human image dataset with over 230K samples capturing diverse poses and textures. Equipped with this large dataset, we rigorously investigate three essential factors in data engineering for StyleGAN-based human generation, namely data size, data distribution, and data alignment. Extensive experiments reveal several valuable observations w.r.t. these aspects: 1) Large-scale data, more than 40K images, are \u2026", "total_citations": 64, "citation_graph": {"2022": 12, "2023": 50}}, {"title": "TCTrack: Temporal Contexts for Aerial Tracking", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:uWQEDVKXjbEC", "authors": ["Ziang Cao", "Ziyuan Huang", "Liang Pan", "Shiwei Zhang", "Ziwei Liu", "Changhong Fu"], "publication_date": "2022", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Temporal contexts among consecutive frames are far from being fully utilized in existing visual trackers. In this work, we present TCTrack, a comprehensive framework to fully exploit temporal contexts for aerial tracking. The temporal contexts are incorporated at two levels: the extraction of features and the refinement of similarity maps. Specifically, for feature extraction, an online temporally adaptive convolution is proposed to enhance the spatial features using temporal information, which is achieved by dynamically calibrating the convolution weights according to the previous frames. For similarity map refinement, we propose an adaptive temporal transformer, which first effectively encodes temporal knowledge in a memory-efficient way, before the temporal knowledge is decoded for accurate adjustment of the similarity map. TCTrack is effective and efficient: evaluation on four aerial tracking benchmarks shows its impressive performance; real-world UAV tests show its high speed of over 27 FPS on NVIDIA Jetson AGX Xavier.", "total_citations": 64, "citation_graph": {"2022": 17, "2023": 46}}, {"title": "Talk-to-Edit: Fine-Grained Facial Editing via Dialog", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:cFHS6HbyZ2cC", "authors": ["Yuming Jiang", "Ziqi Huang", "Xingang Pan", "Chen Change Loy", "Ziwei Liu"], "publication_date": "2021", "conference": "IEEE International Conference on Computer Vision (ICCV)", "description": "Facial editing is an important task in vision and graphics with numerous applications. However, existing works are incapable to deliver a continuous and fine-grained editing mode (eg, editing a slightly smiling face to a big laughing one) with natural interactions with users. In this work, we propose Talk-to-Edit, an interactive facial editing framework that performs fine-grained attribute manipulation through dialog between the user and the system. Our key insight is to model a continual\"\" semantic field\"\" in the GAN latent space. 1) Unlike previous works that regard the editing as traversing straight lines in the latent space, here the fine-grained editing is formulated as finding a curving trajectory that respects fine-grained attribute landscape on the semantic field. 2) The curvature at each step is location-specific and determined by the input image as well as the users' language requests. 3) To engage the users in a meaningful dialog, our system generates language feedback by considering both the user request and the current state of the semantic field. We also contribute CelebA-Dialog, a visual-language facial editing dataset to facilitate large-scale study. Specifically, each image has manually annotated fine-grained attribute annotations as well as template-based textual descriptions in natural language. Extensive quantitative and qualitative experiments demonstrate the superiority of our framework in terms of 1) the smoothness of fine-grained editing, 2) the identity/attribute preservation, and 3) the visual photorealism and dialog fluency. Notably, user study validates that our overall system is consistently favored by around 80% of the participants.", "total_citations": 64, "citation_graph": {"2021": 1, "2022": 23, "2023": 38}}, {"title": "Delving Deep Into Hybrid Annotations for 3D Human Recovery in the Wild", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:IWHjjKOFINEC", "authors": ["Yu Rong", "Ziwei Liu", "Cheng Li", "Kaidi Cao", "Chen Change Loy"], "publication_date": "2019", "conference": "IEEE International Conference on Computer Vision (ICCV)", "description": "Though much progress has been achieved in single-image 3D human recovery, estimating 3D model for in-the-wild images remains a formidable challenge. The reason lies in the fact that obtaining high-quality 3D annotations for in-the-wild images is an extremely hard task that consumes enormous amount of resources and manpower. To tackle this problem, previous methods adopt a hybrid training strategy that exploits multiple heterogeneous types of annotations including 3D and 2D while leaving the efficacy of each annotation not thoroughly investigated. In this work, we aim to perform a comprehensive study on cost and effectiveness trade-off between different annotations. Specifically, we focus on the challenging task of in-the-wild 3D human recovery from single images when paired 3D annotations are not fully available. Through extensive experiments, we obtain several observations: 1) 3D annotations are efficient, whereas traditional 2D annotations such as 2D keypoints and body part segmentation are less competent in guiding 3D human recovery. 2) Dense Correspondence such as DensePose is effective. When there are no paired in-the-wild 3D annotations available, the model exploiting dense correspondence can achieve 92% of the performance compared to a model trained with paired 3D data. We show that incorporating dense correspondence into in-the-wild 3D human recovery is promising and competitive due to its high efficiency and relatively low annotating cost. Our model trained with dense correspondence can serve as a strong reference for future research.", "total_citations": 64, "citation_graph": {"2020": 12, "2021": 25, "2022": 16, "2023": 11}}, {"title": "Semantically Coherent Out-of-Distribution Detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:4OULZ7Gr8RgC", "authors": ["Jingkang Yang", "Haoqi Wang", "Litong Feng", "Xiaopeng Yan", "Huabin Zheng", "Wayne Zhang", "Ziwei Liu"], "publication_date": "2021", "conference": "IEEE International Conference on Computer Vision (ICCV)", "description": "Current out-of-distribution (OOD) detection benchmarks are commonly built by defining one dataset as in-distribution (ID) and all others as OOD. However, these benchmarks unfortunately introduce some unwanted and impractical goals, eg, to perfectly distinguish CIFAR dogs from ImageNet dogs, even though they have the same semantics and negligible covariate shifts. These unrealistic goals will result in an extremely narrow range of model capabilities, greatly limiting their use in real applications. To overcome these drawbacks, we re-design the benchmarks and propose the semantically coherent out-of-distribution detection (SC-OOD). On the SC-OOD benchmarks, existing methods suffer from large performance degradation, suggesting that they are extremely sensitive to low-level discrepancy between data sources while ignoring their inherent semantics. To develop an effective SC-OOD detection approach, we leverage an external un-labeled set and design a concise framework featured by unsupervised dual grouping (UDG) for the joint modeling of ID and OOD data. The proposed UDG can not only enrich the semantic knowledge of the model by exploiting unlabeled data in an unsupervised manner but also distinguish ID/OOD samples to enhance ID classification and OOD detection tasks simultaneously. Extensive experiments demonstrate that our approach achieves state-of-the-art performance on SC-OOD benchmarks. Code and benchmarks are provided on our project page: https://jingkang50. github. io/projects/scood.", "total_citations": 63, "citation_graph": {"2021": 2, "2022": 26, "2023": 35}}, {"title": "Unsupervised 3D Human Pose Representation with Viewpoint and Pose Disentanglement", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:BqipwSGYUEgC", "authors": ["Qiang Nie", "Ziwei Liu", "Yunhui Liu"], "publication_date": "2020", "conference": "European Conference on Computer Vision (ECCV)", "description": "Learning a good 3D human pose representation is important for human pose related tasks, e.g. human 3D pose estimation and action recognition. Within all these problems, preserving the intrinsic pose information and adapting to view variations are two critical issues. In this work, we propose a novel Siamese denoising autoencoder to learn a 3D pose representation by disentangling the pose-dependent and view-dependent feature from the human skeleton data, in a fully unsupervised manner. These two disentangled features are utilized together as the representation of the 3D pose. To consider both the kinematic and geometric dependencies, a sequential bidirectional recursive network (SeBiReNet) is further proposed to model the human skeleton data. Extensive experiments demonstrate that the learned representation 1) preserves the intrinsic information of human pose, 2) shows good transferability \u2026", "total_citations": 63, "citation_graph": {"2021": 12, "2022": 20, "2023": 31}}, {"title": "Unsupervised Domain Adaptive 3D Detection with Multi-Level Consistency", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:zA6iFVUQeVQC", "authors": ["Zhipeng Luo", "Zhongang Cai", "Changqing Zhou", "Gongjie Zhang", "Haiyu Zhao", "Shuai Yi", "Shijian Lu", "Hongsheng Li", "Shanghang Zhang", "Ziwei Liu"], "publication_date": "2021", "conference": "IEEE International Conference on Computer Vision (ICCV)", "description": "Deep learning-based 3D object detection has achieved unprecedented success with the advent of large-scale autonomous driving datasets. However, drastic performance degradation remains a critical challenge for cross-domain deployment. In addition, existing 3D domain adaptive detection methods often assume prior access to the target domain annotations, which is rarely feasible in the real world. To address this challenge, we study a more realistic setting, unsupervised 3D domain adaptive detection, which only utilizes source domain annotations. 1) We first comprehensively investigate the major underlying factors of the domain gap in 3D detection. Our key insight is that geometric mismatch is the key factor of domain shift. 2) Then, we propose a novel and unified framework, Multi-Level Consistency Network (MLC-Net), which employs a teacher-student paradigm to generate adaptive and reliable pseudo-targets. MLC-Net exploits point-, instance-and neural statistics-level consistency to facilitate cross-domain transfer. Extensive experiments demonstrate that MLC-Net outperforms existing state-of-the-art methods (including those using additional target domain information) on standard benchmarks. Notably, our approach is detector-agnostic, which achieves consistent gains on both single-and two-stage 3D detectors. Code will be released.", "total_citations": 62, "citation_graph": {"2021": 5, "2022": 27, "2023": 30}}, {"title": "Otter: A Multi-Modal Model with In-Context Instruction Tuning", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:nrtMV_XWKgEC", "authors": ["Bo Li", "Yuanhan Zhang", "Liangyu Chen", "Jinghao Wang", "Jingkang Yang", "Ziwei Liu"], "publication_date": "2023/5/5", "journal": "arXiv preprint arXiv:2305.03726", "description": "Large language models (LLMs) have demonstrated significant universal capabilities as few/zero-shot learners in various tasks due to their pre-training on vast amounts of text data, as exemplified by GPT-3, which boosted to InstrctGPT and ChatGPT, effectively following natural language instructions to accomplish real-world tasks. In this paper, we propose to introduce instruction tuning into multi-modal models, motivated by the Flamingo model's upstream interleaved format pretraining dataset. We adopt a similar approach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT) dataset. We then introduce Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning. We also optimize OpenFlamingo's implementation for researchers, democratizing the required training resources from 1 A100 GPU to 4 RTX-3090 GPUs, and integrate both OpenFlamingo and Otter into Huggingface Transformers for more researchers to incorporate the models into their customized training and inference pipelines.", "total_citations": 58, "citation_graph": {"2023": 58}}, {"title": "Adversarial Robustness under Long-Tailed Distribution", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:vV6vV6tmYwMC", "authors": ["Tong Wu", "Ziwei Liu", "Qingqiu Huang", "Yu Wang", "Dahua Lin"], "publication_date": "2021", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Adversarial robustness has attracted extensive studies recently by revealing the vulnerability and intrinsic characteristics of deep networks. However, existing works on adversarial robustness mainly focus on balanced datasets, while real-world data usually exhibits a long-tailed distribution. To push adversarial robustness towards more realistic scenarios, in this work we investigate the adversarial vulnerability as well as defense under long-tailed distributions. In particular, we first reveal the negative impacts induced by imbalanced data on both recognition performance and adversarial robustness, uncovering the intrinsic challenges of this problem. We then perform a systematic study on existing long-tailed recognition methods in conjunction with the adversarial training framework. Several valuable observations are obtained: 1) natural accuracy is relatively easy to improve, 2) fake gain of robust accuracy exists under unreliable evaluation, and 3) boundary error limits the promotion of robustness. Inspired by these observations, we propose a clean yet effective framework, RoBal, which consists of two dedicated modules, a scale-invariant classifier and data re-balancing via both margin engineering at training stage and boundary adjustment during inference. Extensive experiments demonstrate the superiority of our approach over other state-of-the-art defense methods. To our best knowledge, we are the first to tackle adversarial robustness under long-tailed distributions, which we believe would be a significant step towards real-world robustness. Our code is available at: https://github. com/wutong16/Adversarial_Long-Tail.", "total_citations": 57, "citation_graph": {"2020": 1, "2021": 5, "2022": 19, "2023": 31}}, {"title": "Text2Human: Text-Driven Controllable Human Image Generation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:8AbLer7MMksC", "authors": ["Yuming Jiang", "Shuai Yang", "Haonan Qiu", "Wayne Wu", "Chen Change Loy", "Ziwei Liu"], "publication_date": "2022", "journal": "ACM Transactions on Graphics (TOG)", "description": "Generating high-quality and diverse human images is an important yet challenging task in vision and graphics. However, existing generative models often fall short under the high diversity of clothing shapes and textures. Furthermore, the generation process is even desired to be intuitively controllable for layman users. In this work, we present a text-driven controllable framework, Text2Human, for a high-quality and diverse human generation. We synthesize full-body human images starting from a given human pose with two dedicated steps. 1) With some texts describing the shapes of clothes, the given human pose is first translated to a human parsing map. 2) The final human image is then generated by providing the system with more attributes about the textures of clothes. Specifically, to model the diversity of clothing textures, we build a hierarchical texture-aware codebook that stores multi-scale neural \u2026", "total_citations": 54, "citation_graph": {"2021": 1, "2022": 8, "2023": 43}}, {"title": "Balanced MSE for Imbalanced Visual Regression", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:K3LRdlH-MEoC", "authors": ["Jiawei Ren", "Mingyuan Zhang", "Cunjun Yu", "Ziwei Liu"], "publication_date": "2022", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Data imbalance exists ubiquitously in real-world visual regressions, eg, age estimation and pose estimation, hurting the model's generalizability and fairness. Thus, imbalanced regression gains increasing research attention recently. Compared to imbalanced classification, imbalanced regression focuses on continuous labels, which can be boundless and high-dimensional and hence more challenging. In this work, we identify that the widely used Mean Square Error (MSE) loss function can be ineffective in imbalanced regression. We revisit MSE from a statistical view and propose a novel loss function, Balanced MSE, to accommodate the imbalanced training label distribution. We further design multiple implementations of Balanced MSE to tackle different real-world scenarios, particularly including the one that requires no prior knowledge about the training label distribution. Moreover, to the best of our knowledge, Balanced MSE is the first general solution to high-dimensional imbalanced regression. Extensive experiments on both synthetic and three real-world benchmarks demonstrate the effectiveness of Balanced MSE.", "total_citations": 54, "citation_graph": {"2021": 1, "2022": 9, "2023": 44}}, {"title": "Delving Deep into the Generalization of Vision Transformers under Distribution Shifts", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:3s1wT3WcHBgC", "authors": ["Chongzhi Zhang", "Mingyuan Zhang", "Shanghang Zhang", "Daisheng Jin", "Qiang Zhou", "Zhongang Cai", "Haiyu Zhao", "Shuai Yi", "Xianglong Liu", "Ziwei Liu"], "publication_date": "2022", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Recently, Vision Transformers have achieved impressive results on various Vision tasks. Yet, their generalization ability under different distribution shifts is poorly understood. In this work, we provide a comprehensive study on the out-of-distribution generalization of Vision Transformers. To support a systematic investigation, we first present a taxonomy of distribution shifts by categorizing them into five conceptual levels: corruption shift, background shift, texture shift, destruction shift, and style shift. Then we perform extensive evaluations of Vision Transformer variants under different levels of distribution shifts and compare their generalization ability with Convolutional Neural Network (CNN) models. Several important observations are obtained: 1) Vision Transformers generalize better than CNNs under multiple distribution shifts. With the same or less amount of parameters, Vision Transformers are ahead of corresponding CNNs by more than 5% in top-1 accuracy under most types of distribution shift. In particular, Vision Transformers lead by more than 10% under the corruption shifts. 2) larger Vision Transformers gradually narrow the in-distribution (ID) and out-of-distribution (OOD) performance gap. To further improve the generalization of Vision Transformers, we design the enhanced Vision Transformers through self-supervised learning, information theory, and adversarial learning. By investigating these three types of generalization-enhanced Transformers, we observe the gradient-sensitivity of Vision Transformers and design a smoother learning strategy to achieve a stable training process. With modified training schemes, we achieve \u2026", "total_citations": 53, "citation_graph": {"2021": 2, "2022": 23, "2023": 28}}, {"title": "Unsupervised Object-Level Representation Learning from Scene Images", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:rO6llkc54NcC", "authors": ["Jiahao Xie", "Xiaohang Zhan", "Ziwei Liu", "Yew Soon Ong", "Chen Change Loy"], "publication_date": "2021", "conference": "Advances in Neural Information Processing Systems (NeurIPS)", "description": "Contrastive self-supervised learning has largely narrowed the gap to supervised pre-training on ImageNet. However, its success highly relies on the object-centric priors of ImageNet, ie, different augmented views of the same image correspond to the same object. Such a heavily curated constraint becomes immediately infeasible when pre-trained on more complex scene images with many objects. To overcome this limitation, we introduce Object-level Representation Learning (ORL), a new self-supervised learning framework towards scene images. Our key insight is to leverage image-level self-supervised pre-training as the prior to discover object-level semantic correspondence, thus realizing object-level representation learning from scene images. Extensive experiments on COCO show that ORL significantly improves the performance of self-supervised learning on scene images, even surpassing supervised ImageNet pre-training on several downstream tasks. Furthermore, ORL improves the downstream performance when more unlabeled scene images are available, demonstrating its great potential of harnessing unlabeled data in the wild. We hope our approach can motivate future research on more general-purpose unsupervised representation learning from scene data.", "total_citations": 53, "citation_graph": {"2021": 1, "2022": 25, "2023": 27}}, {"title": "Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:WbkHhVStYXYC", "authors": ["Shuai Yang", "Liming Jiang", "Ziwei Liu", "Chen Change Loy"], "publication_date": "2022", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Recent studies on StyleGAN show high performance on artistic portrait generation by transfer learning with limited data. In this paper, we explore more challenging exemplar-based high-resolution portrait style transfer by introducing a novel DualStyleGAN with flexible control of dual styles of the original face domain and the extended artistic portrait domain. Different from StyleGAN, DualStyleGAN provides a natural way of style transfer by characterizing the content and style of a portrait with an intrinsic style path and a new extrinsic style path, respectively. The delicately designed extrinsic style path enables our model to modulate both the color and complex structural styles hierarchically to precisely pastiche the style example. Furthermore, a novel progressive fine-tuning scheme is introduced to smoothly transform the generative space of the model to the target domain, even with the above modifications on the network architecture. Experiments demonstrate the superiority of DualStyleGAN over state-of-the-art methods in high-quality portrait style transfer and flexible style control.", "total_citations": 49, "citation_graph": {"2022": 14, "2023": 34}}, {"title": "Few-Shot Object Detection via Association and DIscrimination", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:eJXPG6dFmWUC", "authors": ["Yuhang Cao", "Jiaqi Wang", "Ying Jin", "Tong Wu", "Kai Chen", "Ziwei Liu", "Dahua Lin"], "publication_date": "2021", "conference": "Advances in Neural Information Processing Systems (NeurIPS)", "description": "Object detection has achieved substantial progress in the last decade. However, detecting novel classes with only few samples remains challenging, since deep learning under low data regime usually leads to a degraded feature space. Existing works employ a holistic fine-tuning paradigm to tackle this problem, where the model is first pre-trained on all base classes with abundant samples, and then it is used to carve the novel class feature space. Nonetheless, this paradigm is still imperfect. Durning fine-tuning, a novel class may implicitly leverage the knowledge of multiple base classes to construct its feature space, which induces a scattered feature space, hence violating the inter-class separability. To overcome these obstacles, we propose a two-step fine-tuning framework, Few-shot object detection via Association and DIscrimination (FADI), which builds up a discriminative feature space for each novel class with two integral steps. 1) In the association step, in contrast to implicitly leveraging multiple base classes, we construct a compact novel class feature space via explicitly imitating a specific base class feature space. Specifically, we associate each novel class with a base class according to their semantic similarity. After that, the feature space of a novel class can readily imitate the well-trained feature space of the associated base class. 2) In the discrimination step, to ensure the separability between the novel classes and associated base classes, we disentangle the classification branches for base and novel classes. To further enlarge the inter-class separability between all classes, a set-specialized margin loss is imposed. Extensive \u2026", "total_citations": 49, "citation_graph": {"2021": 1, "2022": 18, "2023": 30}}, {"title": "One-shot Face Reenactment", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:hFOr9nPyWt4C", "authors": ["Yunxuan Zhang", "Siwei Zhang", "Yue He", "Cheng Li", "Chen Change Loy", "Ziwei Liu"], "publication_date": "2019", "conference": "British Machine Vision Conference (BMVC)", "description": "To enable realistic shape (e.g. pose and expression) transfer, existing face reenactment methods rely on a set of target faces for learning subject-specific traits. However, in real-world scenario end-users often only have one target face at hand, rendering existing methods inapplicable. In this work, we bridge this gap by proposing a novel one-shot face reenactment learning framework. Our key insight is that the one-shot learner should be able to disentangle and compose appearance and shape information for effective modeling. Specifically, the target face appearance and the source face shape are first projected into latent spaces with their corresponding encoders. Then these two latent spaces are associated by learning a shared decoder that aggregates multi-level features to produce the final reenactment results. To further improve the synthesizing quality on mustache and hair regions, we additionally propose FusionNet which combines the strengths of our learned decoder and the traditional warping method. Extensive experiments show that our one-shot face reenactment system achieves superior transfer fidelity as well as identity preserving capability than alternatives. More remarkably, our approach trained with only one target image per subject achieves competitive results to those using a set of target images, demonstrating the practical merit of this work. Code, models and an additional set of reenacted faces have been publicly released at the project page.", "total_citations": 49, "citation_graph": {"2020": 11, "2021": 11, "2022": 14, "2023": 13}}, {"title": "BiBERT: Accurate Fully Binarized BERT", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:UxriW0iASnsC", "authors": ["Haotong Qin", "Yifu Ding", "Mingyuan Zhang", "YAN Qinghua", "Aishan Liu", "Qingqing Dang", "Ziwei Liu", "Xianglong Liu"], "publication_date": "2022", "conference": "International Conference on Learning Representations (ICLR)", "description": "The large pre-trained BERT has achieved remarkable performance on Natural Language Processing (NLP) tasks but is also computation and memory expensive. As one of the powerful compression approaches, binarization extremely reduces the computation and memory consumption by utilizing 1-bit parameters and bitwise operations. Unfortunately, the full binarization of BERT (i.e., 1-bit weight, embedding, and activation) usually suffer a significant performance drop, and there is rare study addressing this problem. In this paper, with the theoretical justification and empirical analysis, we identify that the severe performance drop can be mainly attributed to the information degradation and optimization direction mismatch respectively in the forward and backward propagation, and propose BiBERT, an accurate fully binarized BERT, to eliminate the performance bottlenecks. Specifically, BiBERT introduces an efficient Bi-Attention structure for maximizing representation information statistically and a Direction-Matching Distillation (DMD) scheme to optimize the full binarized BERT accurately. Extensive experiments show that BiBERT outperforms both the straightforward baseline and existing state-of-the-art quantized BERTs with ultra-low bit activations by convincing margins on the NLP benchmark. As the first fully binarized BERT, our method yields impressive 56.3 times and 31.2 times saving on FLOPs and model size, demonstrating the vast advantages and potential of the fully binarized BERT model in real-world resource-constrained scenarios.", "total_citations": 45, "citation_graph": {"2021": 1, "2022": 12, "2023": 32}}, {"title": "Mix-and-Match Tuning for Self-Supervised Semantic Segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:hqOjcs7Dif8C", "authors": ["Xiaohang Zhan", "Ziwei Liu", "Ping Luo", "Xiaoou Tang", "Chen Change Loy"], "publication_date": "2018", "conference": "AAAI Conference on Artificial Intelligence (AAAI)", "description": "Deep convolutional networks for semantic image segmentation typically require large-scale labeled data, eg, ImageNet and MS COCO, for network pre-training. To reduce annotation efforts, self-supervised semantic segmentation is recently proposed to pre-train a network without any human-provided labels. The key of this new form of learning is to design a proxy task (eg, image colorization), from which a discriminative loss can be formulated on unlabeled data. Many proxy tasks, however, lack the critical supervision signals that could induce discriminative representation for the target image segmentation task. Thus self-supervision\u2019s performance is still far from that of supervised pre-training. In this study, we overcome this limitation by incorporating a\" mix-and-match\"(M&M) tuning stage in the self-supervision pipeline. The proposed approach is readily pluggable to many self-supervision methods and does not use more annotated samples than the original process. Yet, it is capable of boosting the performance of target image segmentation task to surpass fully-supervised pre-trained counterpart. The improvement is made possible by better harnessing the limited pixel-wise annotations in the target dataset. Specifically, we first introduce the\" mix\" stage, which sparsely samples and mixes patches from the target set to reflect rich and diverse local patch statistics of target images. A \u2018match\u2019stage then forms a class-wise connected graph, which can be used to derive a strong triplet-based discriminative loss for finetuning the network. Our paradigm follows the standard practice in existing self-supervised studies and no extra data or label is required \u2026", "total_citations": 45, "citation_graph": {"2018": 1, "2019": 4, "2020": 11, "2021": 9, "2022": 10, "2023": 10}}, {"title": "Robust Reference-based Super-Resolution via C2-Matching", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:ZHo1McVdvXMC", "authors": ["Yuming Jiang", "Kelvin CK Chan", "Xintao Wang", "Chen Change Loy", "Ziwei Liu"], "publication_date": "2021", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Reference-based Super-Resolution (Ref-SR) has recently emerged as a promising paradigm to enhance a low-resolution (LR) input image by introducing an additional high-resolution (HR) reference image. Existing Ref-SR methods mostly rely on implicit correspondence matching to borrow HR textures from reference images to compensate for the information loss in input images. However, performing local transfer is difficult because of two gaps between input and reference images: the transformation gap (eg scale and rotation) and the resolution gap (eg HR and LR). To tackle these challenges, we propose C^ 2-Matching in this work, which produces explicit robust matching crossing transformation and resolution. 1) For the transformation gap, we propose a contrastive correspondence network, which learns transformation-robust correspondences using augmented views of the input image. 2) For the resolution gap, we adopt a teacher-student correlation distillation, which distills knowledge from the easier HR-HR matching to guide the more ambiguous LR-HR matching. 3) Finally, we design a dynamic aggregation module to address the potential misalignment issue. In addition, to faithfully evaluate the performance of Ref-SR under a realistic setting, we contribute the Webly-Referenced SR (WR-SR) dataset, mimicking the practical usage scenario. Extensive experiments demonstrate that our proposed C^ 2-Matching significantly outperforms current state-of-the-art methods by over 1dB on the standard CUFED5 benchmark. Notably, it also shows great generalizability on WR-SR dataset as well as robustness across large scale and rotation \u2026", "total_citations": 42, "citation_graph": {"2021": 1, "2022": 21, "2023": 20}}, {"title": "EVA3D: Compositional 3D Human Generation from 2D Image Collections", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:WqliGbK-hY8C", "authors": ["Fangzhou Hong", "Zhaoxi Chen", "Yushi Lan", "Liang Pan", "Ziwei Liu"], "publication_date": "2023", "conference": "International Conference on Learning Representations (ICLR)", "description": "Inverse graphics aims to recover 3D models from 2D observations. Utilizing differentiable rendering, recent 3D-aware generative models have shown impressive results of rigid object generation using 2D images. However, it remains challenging to generate articulated objects, like human bodies, due to their complexity and diversity in poses and appearances. In this work, we propose, EVA3D, an unconditional 3D human generative model learned from 2D image collections only. EVA3D can sample 3D humans with detailed geometry and render high-quality images (up to 512x256) without bells and whistles (e.g. super resolution). At the core of EVA3D is a compositional human NeRF representation, which divides the human body into local parts. Each part is represented by an individual volume. This compositional representation enables 1) inherent human priors, 2) adaptive allocation of network parameters, 3) efficient training and rendering. Moreover, to accommodate for the characteristics of sparse 2D human image collections (e.g. imbalanced pose distribution), we propose a pose-guided sampling strategy for better GAN learning. Extensive experiments validate that EVA3D achieves state-of-the-art 3D human generation performance regarding both geometry and texture quality. Notably, EVA3D demonstrates great potential and scalability to \"inverse-graphics\" diverse human bodies with a clean framework.", "total_citations": 40, "citation_graph": {"2022": 3, "2023": 37}}, {"title": "Self-Supervised Learning via Conditional Motion Propagation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:qxL8FJ1GzNcC", "authors": ["Xiaohang Zhan", "Xingang Pan", "Ziwei Liu", "Dahua Lin", "Chen Change Loy"], "publication_date": "2019", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Intelligent agent naturally learns from motion. Various self-supervised algorithms have leveraged the motion cues to learn effective visual representations. The hurdle here is that motion is both ambiguous and complex, rendering previous works either suffer from degraded learning efficacy, or resort to strong assumptions on object motions. In this work, we design a new learning-from-motion paradigm to bridge these gaps. Instead of explicitly modeling the motion probabilities, we design the pretext task as a conditional motion propagation problem. Given an input image and several sparse flow guidance on it, our framework seeks to recover the full-image motion. Compared to other alternatives, our framework has several appealing properties:(1) Using sparse flow guidance during training resolves the inherent motion ambiguity, and thus easing feature learning.(2) Solving the pretext task of conditional motion propagation encourages the emergence of kinematically-sound representations that poss greater expressive power. Extensive experiments demonstrate that our framework learns structural and coherent features; and achieves state-of-the-art self-supervision performance on several downstream tasks including semantic segmentation, instance segmentation and human parsing. Furthermore, our framework is successfully extended to several useful applications such as semi-automatic pixel-level annotation.", "total_citations": 40, "citation_graph": {"2019": 1, "2020": 11, "2021": 15, "2022": 9, "2023": 4}}, {"title": "Visually Informed Binaural Audio Generation without Binaural Audios", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:2P1L_qKh6hAC", "authors": ["Xudong Xu", "Hang Zhou", "Ziwei Liu", "Bo Dai", "Xiaogang Wang", "Dahua Lin"], "publication_date": "2021", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Stereophonic audio, especially binaural audio, plays an essential role in immersive viewing environments. Recent research has explored generating stereophonic audios guided by visual cues and multi-channel audio collections in a fully-supervised manner. However, due to the requirement of professional recording devices, existing datasets are limited in scale and variety, which impedes the generalization of supervised methods to real-world scenarios. In this work, we propose PseudoBinaural, an effective pipeline that is free of binaural recordings. The key insight is to carefully build pseudo visual-stereo pairs with mono data for training. Specifically, we leverage spherical harmonic decomposition and head-related impulse response (HRIR) to identify the relationship between the location of a sound source and the received binaural audio. Then in the visual modality, corresponding visual cues of the mono data are manually placed at sound source positions to form the pairs. Compared to fully-supervised paradigms, our binaural-recording-free pipeline shows great stability in the cross-dataset evaluation and comparable performance under subjective preference. Moreover, combined with binaural recorded data, our method is able to further boost the performance of binaural audio generation under supervised settings.", "total_citations": 38, "citation_graph": {"2021": 7, "2022": 14, "2023": 17}}, {"title": "Im2Avatar: Colorful 3D Reconstruction from a Single Image", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:8k81kl-MbHgC", "authors": ["Yongbin Sun", "Ziwei Liu", "Yue Wang", "Sanjay E Sarma"], "publication_date": "2018/4/17", "journal": "arXiv preprint arXiv:1804.06375", "description": "Existing works on single-image 3D reconstruction mainly focus on shape recovery. In this work, we study a new problem, that is, simultaneously recovering 3D shape and surface color from a single image, namely \"colorful 3D reconstruction\". This problem is both challenging and intriguing because the ability to infer textured 3D model from a single image is at the core of visual understanding. Here, we propose an end-to-end trainable framework, Colorful Voxel Network (CVN), to tackle this problem. Conditioned on a single 2D input, CVN learns to decompose shape and surface color information of a 3D object into a 3D shape branch and a surface color branch, respectively. Specifically, for the shape recovery, we generate a shape volume with the state of its voxels indicating occupancy. For the surface color recovery, we combine the strength of appearance hallucination and geometric projection by concurrently learning a regressed color volume and a 2D-to-3D flow volume, which are then fused into a blended color volume. The final textured 3D model is obtained by sampling color from the blended color volume at the positions of occupied voxels in the shape volume. To handle the severe sparse volume representations, a novel loss function, Mean Squared False Cross-Entropy Loss (MSFCEL), is designed. Extensive experiments demonstrate that our approach achieves significant improvement over baselines, and shows great generalization across diverse object categories and arbitrary viewpoints.", "total_citations": 38, "citation_graph": {"2018": 1, "2019": 6, "2020": 9, "2021": 9, "2022": 5, "2023": 7}}, {"title": "HuMMan: Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:4fKUyHm3Qg0C", "authors": ["Zhongang Cai", "Daxuan Ren", "Ailing Zeng", "Zhengyu Lin", "Tao Yu", "Wenjia Wang", "Xiangyu Fan", "Yang Gao", "Yifan Yu", "Liang Pan", "Fangzhou Hong", "Mingyuan Zhang", "Chen Change Loy", "Lei Yang", "Ziwei Liu"], "publication_date": "2022", "conference": "European Conference on Computer Vision (ECCV)", "description": "4D human sensing and modeling are fundamental tasks in vision and graphics with numerous applications. With the advances of new sensors and algorithms, there is an increasing demand for more versatile datasets. In this work, we contribute HuMMan, a large-scale multi-modal 4D human dataset with 1000 human subjects, 400k sequences and 60M frames. HuMMan has several appealing properties: 1) multi-modal data and annotations including color images, point clouds, keypoints, SMPL parameters, and textured meshes; 2) popular mobile device is included in the sensor suite; 3) a set of 500 actions, designed to cover fundamental movements; 4) multiple tasks such as action recognition, pose estimation, parametric human recovery, and textured mesh reconstruction are supported and evaluated. Extensive experiments on HuMMan voice the need for further study on challenges such as fine-grained action \u2026", "total_citations": 37, "citation_graph": {"2021": 1, "2022": 5, "2023": 29}}, {"title": "Benchmarking and Analyzing Point Cloud Classification under Corruptions", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:dshw04ExmUIC", "authors": ["Jiawei Ren", "Liang Pan", "Ziwei Liu"], "publication_date": "2022", "conference": "International Conference on Machine Learning (ICML)", "description": "3D perception, especially point cloud classification, has achieved substantial progress. However, in real-world deployment, point cloud corruptions are inevitable due to the scene complexity, sensor inaccuracy, and processing imprecision. In this work, we aim to rigorously benchmark and analyze point cloud classification under corruptions. To conduct a systematic investigation, we first provide a taxonomy of common 3D corruptions and identify the atomic corruptions. Then, we perform a comprehensive evaluation on a wide range of representative point cloud models to understand their robustness and generalizability. Our benchmark results show that although point cloud classification performance improves over time, the state-of-the-art methods are on the verge of being less robust. Based on the obtained observations, we propose several effective techniques to enhance point cloud classifier robustness. We hope our comprehensive benchmark, in-depth analysis, and proposed techniques could spark future research in robust 3D perception.", "total_citations": 37, "citation_graph": {"2022": 6, "2023": 31}}, {"title": "Deep Class-Incremental Learning: A Survey", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:UHK10RUVsp4C", "authors": ["Da-Wei Zhou", "Qi-Wei Wang", "Zhi-Hong Qi", "Han-Jia Ye", "De-Chuan Zhan", "Ziwei Liu"], "publication_date": "2023/2/7", "journal": "arXiv preprint arXiv:2302.03648", "description": "Deep models, e.g., CNNs and Vision Transformers, have achieved impressive achievements in many vision tasks in the closed world. However, novel classes emerge from time to time in our ever-changing world, requiring a learning system to acquire new knowledge continually. For example, a robot needs to understand new instructions, and an opinion monitoring system should analyze emerging topics every day. Class-Incremental Learning (CIL) enables the learner to incorporate the knowledge of new classes incrementally and build a universal classifier among all seen classes. Correspondingly, when directly training the model with new class instances, a fatal problem occurs -- the model tends to catastrophically forget the characteristics of former ones, and its performance drastically degrades. There have been numerous efforts to tackle catastrophic forgetting in the machine learning community. In this paper, we survey comprehensively recent advances in deep class-incremental learning and summarize these methods from three aspects, i.e., data-centric, model-centric, and algorithm-centric. We also provide a rigorous and unified evaluation of 16 methods in benchmark image classification tasks to find out the characteristics of different algorithms empirically. Furthermore, we notice that the current comparison protocol ignores the influence of memory budget in model storage, which may result in unfair comparison and biased results. Hence, we advocate fair comparison by aligning the memory budget in evaluation, as well as several memory-agnostic performance measures. The source code to reproduce these evaluations is available at \u2026", "total_citations": 36, "citation_graph": {"2022": 2, "2023": 34}}, {"title": "Delving into Inter-Image Invariance for Unsupervised Visual Representations", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:hMod-77fHWUC", "authors": ["Jiahao Xie", "Xiaohang Zhan", "Ziwei Liu", "Yew Soon Ong", "Chen Change Loy"], "publication_date": "2022", "journal": "International Journal of Computer Vision (IJCV)", "description": "Contrastive learning has recently shown immense potential in unsupervised visual representation learning. Existing studies in this track mainly focus on intra-image invariance learning. The learning typically uses rich intra-image transformations to construct positive pairs and then maximizes agreement using a contrastive loss. The merits of inter-image invariance, conversely, remain much less explored. One major obstacle to exploit inter-image invariance is that it is unclear how to reliably construct inter-image positive pairs, and further derive effective supervision from them since no pair annotations are available. In this work, we present a comprehensive empirical study to better understand the role of inter-image invariance learning from three main constituting components: pseudo-label maintenance, sampling strategy, and decision boundary design. To facilitate the study, we introduce a unified and generic \u2026", "total_citations": 36, "citation_graph": {"2021": 9, "2022": 7, "2023": 16}}, {"title": "TAda! Temporally-Adaptive Convolutions for Video Understanding", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:yD5IFk8b50cC", "authors": ["Ziyuan Huang", "Shiwei Zhang", "Liang Pan", "Zhiwu Qing", "Mingqian Tang", "Ziwei Liu", "Marcelo H Ang Jr"], "publication_date": "2022", "conference": "International Conference on Learning Representations (ICLR)", "description": "Spatial convolutions are widely used in numerous deep video models. It fundamentally assumes spatio-temporal invariance, i.e., using shared weights for every location in different frames. This work presents Temporally-Adaptive Convolutions (TAdaConv) for video understanding, which shows that adaptive weight calibration along the temporal dimension is an efficient way to facilitate modelling complex temporal dynamics in videos. Specifically, TAdaConv empowers the spatial convolutions with temporal modelling abilities by calibrating the convolution weights for each frame according to its local and global temporal context. Compared to previous temporal modelling operations, TAdaConv is more efficient as it operates over the convolution kernels instead of the features, whose dimension is an order of magnitude smaller than the spatial resolutions. Further, the kernel calibration brings an increased model capacity. We construct TAda2D and TAdaConvNeXt networks by replacing the 2D convolutions in ResNet and ConvNeXt with TAdaConv, which leads to at least on par or better performance compared to state-of-the-art approaches on multiple video action recognition and localization benchmarks. We also demonstrate that as a readily plug-in operation with negligible computation overhead, TAdaConv can effectively improve many existing video models with a convincing margin.", "total_citations": 31, "citation_graph": {"2022": 8, "2023": 23}}, {"title": "Instance-level Facial Attributes Transfer with Geometry-Aware Flow", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:YOwf2qJgpHMC", "authors": ["Weidong Yin", "Ziwei Liu", "Chen Change Loy"], "publication_date": "2019", "conference": "AAAI Conference on Artificial Intelligence (AAAI)", "description": "We address the problem of instance-level facial attribute transfer without paired training data, eg, faithfully transferring the exact mustache from a source face to a target face. This is a more challenging task than the conventional semantic-level attribute transfer, which only preserves the generic attribute style instead of instance-level traits. We propose the use of geometry-aware flow, which serves as a wellsuited representation for modeling the transformation between instance-level facial attributes. Specifically, we leverage the facial landmarks as the geometric guidance to learn the differentiable flows automatically, despite of the large pose gap existed. Geometry-aware flow is able to warp the source face attribute into the target face context and generate a warp-and-blend result. To compensate for the potential appearance gap between source and target faces, we propose a hallucination sub-network that produces an appearance residual to further refine the warp-and-blend result. Finally, a cycle-consistency framework consisting of both attribute transfer module and attribute removal module is designed, so that abundant unpaired face images can be used as training data. Extensive evaluations validate the capability of our approach in transferring instance-level facial attributes faithfully across large pose and appearance gaps. Thanks to the flow representation, our approach can readily be applied to generate realistic details on high-resolution images 1.", "total_citations": 31, "citation_graph": {"2019": 6, "2020": 9, "2021": 11, "2022": 2, "2023": 3}}, {"title": "Panoptic Scene Graph Generation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:V3AGJWp-ZtQC", "authors": ["Jingkang Yang", "Yi Zhe Ang", "Zujin Guo", "Kaiyang Zhou", "Wayne Zhang", "Ziwei Liu"], "publication_date": "2022", "conference": "European Conference on Computer Vision (ECCV)", "description": "Existing research addresses scene graph generation (SGG)\u2014a critical technology for scene understanding in images\u2014from a detection perspective, i.e., objects are detected using bounding boxes followed by prediction of their pairwise relationships. We argue that such a paradigm causes several problems that impede the progress of the field. For instance, bounding box-based labels in current datasets usually contain redundant classes like hairs, and leave out background information that is crucial to the understanding of context. In this work, we introduce panoptic scene graph generation (PSG), a new problem task that requires the model to generate a more comprehensive scene graph representation based on panoptic segmentations rather than rigid bounding boxes. A high-quality PSG dataset, which contains 49k well-annotated overlapping images from COCO and Visual Genome, is created for the \u2026", "total_citations": 30, "citation_graph": {"2021": 1, "2022": 3, "2023": 26}}, {"title": "Energy-Based Open-World Uncertainty Modeling for Confidence Calibration", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:u_35RYKgDlwC", "authors": ["Yezhen Wang", "Bo Li", "Tong Che", "Kaiyang Zhou", "Ziwei Liu", "Dongsheng Li"], "publication_date": "2021", "conference": "IEEE International Conference on Computer Vision (ICCV)", "description": "Confidence calibration is of great importance to ensure the reliability of decisions made by machine learning systems. However, discriminative classifiers based on deep neural networks are often criticized for producing overconfident predictions that fail to reflect the true correctness likelihood of classification accuracy. We argue that such an inability to model uncertainty is mainly caused by the closed-world nature in softmax: a model trained by the cross-entropy loss will be forced to classify the input into one of K pre-defined categories with high probability. To address this problem, we for the first time propose a novel K+ 1-way softmax formulation, which incorporates the modeling of open-world uncertainty as to the extra dimension. To unify the learning of the original K-way classification task and the extra dimension that models uncertainty, we (1) propose a novel energy-based objective function, and moreover,(2) theoretically prove that optimizing such an objective essentially forces the extra dimension to capture the marginal data distribution. Extensive experiments show that our approach, Energy-based Open-World Softmax (EOW-Softmax), is superior to existing state-of-the-art methods in improving confidence calibration.", "total_citations": 30, "citation_graph": {"2021": 1, "2022": 14, "2023": 15}}, {"title": "CARAFE++: Unified Content-Aware ReAssembly of FEatures", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:ns9cj8rnVeAC", "authors": ["Jiaqi Wang", "Kai Chen", "Rui Xu", "Ziwei Liu", "Chen Change Loy", "Dahua Lin"], "publication_date": "2021", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)", "description": "Feature reassembly, i.e. feature downsampling and upsampling, is a key operation in a number of modern convolutional network architectures, e.g., residual networks and feature pyramids. Its design is critical for dense prediction tasks such as object detection and semantic/instance segmentation. In this work, we propose unified Content-Aware ReAssembly of FEatures (CARAFE++), a universal, lightweight, and highly effective operator to fulfill this goal. CARAFE++ has several appealing properties: (1) Unlike conventional methods such as pooling and interpolation that only exploit sub-pixel neighborhood, CARAFE++ aggregates contextual information within a large receptive field. (2) Instead of using a fixed kernel for all samples (e.g. convolution and deconvolution), CARAFE++ generates adaptive kernels on-the-fly to enable instance-specific content-aware handling. (3) CARAFE++ introduces little computational \u2026", "total_citations": 28, "citation_graph": {"2021": 6, "2022": 9, "2023": 13}}, {"title": "CelebV-HQ: A Large-Scale Video Facial Attributes Dataset", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:1qzjygNMrQYC", "authors": ["Hao Zhu", "Wayne Wu", "Wentao Zhu", "Liming Jiang", "Siwei Tang", "Li Zhang", "Ziwei Liu", "Chen Change Loy"], "publication_date": "2022", "conference": "European Conference on Computer Vision (ECCV)", "description": "Large-scale datasets have played indispensable roles in the recent success of face generation/editing and significantly facilitated the advances of emerging research fields. However, the academic community still lacks a video dataset with diverse facial attribute annotations, which is crucial for the research on face-related videos. In this work, we propose a large-scale, high-quality, and diverse video dataset with rich facial attribute annotations, named the High-Quality Celebrity Video Dataset (CelebV-HQ). CelebV-HQ contains 35, 666 video clips with the resolution of at least, involving 15, 653 identities. All clips are labeled manually with 83 facial attributes, covering appearance, action, and emotion. We conduct a comprehensive analysis in terms of age, ethnicity, brightness stability, motion smoothness, head pose diversity, and data quality to demonstrate the diversity and temporal coherence of CelebV-HQ \u2026", "total_citations": 27, "citation_graph": {"2022": 1, "2023": 26}}, {"title": "Unsupervised Image-to-Image Translation with Generative Prior", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:l7t_Zn2s7bgC", "authors": ["Shuai Yang", "Liming Jiang", "Ziwei Liu", "Chen Change Loy"], "publication_date": "2022", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Unsupervised image-to-image translation aims to learn the translation between two visual domains without paired data. Despite the recent progress in image translation models, it remains challenging to build mappings between complex domains with drastic visual discrepancies. In this work, we present a novel framework, Generative Prior-guided UNsupervised Image-to-image Translation (GP-UNIT), to improve the overall quality and applicability of the translation algorithm. Our key insight is to leverage the generative prior from pre-trained class-conditional GANs (eg, BigGAN) to learn rich content correspondences across various domains. We propose a novel coarse-to-fine scheme: we first distill the generative prior to capture a robust coarse-level content representation that can link objects at an abstract semantic level, based on which fine-level content features are adaptively learned for more accurate multi-level content correspondences. Extensive experiments demonstrate the superiority of our versatile framework over state-of-the-art methods in robust, high-quality and diversified translations, even for challenging and distant domains.", "total_citations": 26, "citation_graph": {"2022": 3, "2023": 23}}, {"title": "Learning Diverse Fashion Collocation by Neural Graph Filtering", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:RHpTSmoSYBkC", "authors": ["Xin Liu", "Yongbin Sun", "Ziwei Liu", "Dahua Lin"], "publication_date": "2020", "journal": "IEEE Transactions on Multimedia (TMM)", "description": "Fashion recommendation systems are highly desired by customers to find visually-collocated fashion items, such as clothes, shoes, bags, etc. While existing methods demonstrate promising results, they remain lacking in flexibility and diversity, e.g. assuming a fixed number of items or favoring safe but boring recommendations. In this paper, we propose a novel fashion collocation framework, Neural Graph Filtering , that models a flexible set of fashion items via a graph neural network. Specifically, we consider the visual embeddings of each garment as a node in the graph, and describe the inter-garment relationship as the edge between nodes. By applying symmetric operations on the edge vectors, this framework allows varying numbers of inputs/outputs and is invariant to their ordering. We further include a style classifier augmented with focal loss to enable the collocation of significantly diverse styles, which are \u2026", "total_citations": 24, "citation_graph": {"2021": 9, "2022": 9, "2023": 6}}, {"title": "Monocular 3D Reconstruction of Interacting Hands via Collision-Aware Factorized Refinements", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:abG-DnoFyZgC", "authors": ["Yu Rong", "Jingbo Wang", "Ziwei Liu", "Chen Change Loy"], "publication_date": "2021", "conference": "International Conference on 3D Vision (3DV)", "description": "3D interacting hand reconstruction is essential to facilitate human-machine interaction and human behaviors understanding. Previous works in this field either rely on auxiliary inputs such as depth images or they can only handle a single hand if monocular single RGB images are used. Single-hand methods tend to generate collided hand meshes, when applied to closely interacting hands, since they cannot model the interactions between two hands explicitly. In this paper, we make the first attempt to reconstruct 3D interacting hands from monocular single RGB images. Our method can generate 3D hand meshes with both precise 3D poses and minimal collisions. This is made possible via a two-stage framework. Specifically, the first stage adopts a convolutional neural network to generate coarse predictions that tolerate collisions but encourage pose-accurate hand meshes. The second stage progressively \u2026", "total_citations": 23, "citation_graph": {"2022": 7, "2023": 16}}, {"title": "Iterative Human and Automated Identification of Wildlife Images", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:pqnbT2bcN3wC", "authors": ["Zhongqi Miao", "Ziwei Liu", "Kaitlyn M Gaynor", "Meredith S Palmer", "Stella X Yu", "Wayne M Getz"], "publication_date": "2021", "journal": "Nature - Machine Intelligence", "description": "Camera trapping is increasingly being used to monitor wildlife, but this technology typically requires extensive data annotation. Recently, deep learning has substantially advanced automatic wildlife recognition. However, current methods are hampered by a dependence on large static datasets, whereas wildlife data are intrinsically dynamic and involve long-tailed distributions. These drawbacks can be overcome through a hybrid combination of machine learning and humans in the loop. Our proposed iterative human and automated identification approach is capable of learning from wildlife imagery data with a long-tailed distribution. Additionally, it includes self-updating learning, which facilitates capturing the community dynamics of rapidly changing natural systems. Extensive experiments show that our approach can achieve an ~90% accuracy employing only ~20% of the human annotations of existing \u2026", "total_citations": 23, "citation_graph": {"2022": 11, "2023": 12}}, {"title": "MMFashion: An Open-Source Toolbox for Visual Fashion Analysis", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:j3f4tGmQtD8C", "authors": ["Xin Liu", "Jiancheng Li", "Jiaqi Wang", "Ziwei Liu"], "publication_date": "2021", "conference": "ACM Multimedia (ACM MM), Open Source Software Competition", "description": "We present MMFashion, a comprehensive, flexible and user-friendly open-source visual fashion analysis toolbox based on PyTorch. This toolbox supports a wide spectrum of fashion analysis tasks, including Fashion Attribute Prediction, Fashion Recognition and Retrieval, Fashion Landmark Detection, Fashion Parsing and Segmentation and Fashion Compatibility and Recommendation. It covers almost all the mainstream tasks in fashion analysis community. MMFashion has several appealing properties. Firstly, MMFashion follows the principle of modular design. The framework is decomposed into different components so that it is easily extensible for diverse customized modules. In addition, detailed documentations, demo scripts and off-the-shelf models are available, which ease the burden of layman users to leverage the recent advances in deep learning-based fashion analysis. Our proposed MMFashion is \u2026", "total_citations": 23, "citation_graph": {"2020": 2, "2021": 4, "2022": 8, "2023": 8}}, {"title": "Transformer-Based Visual Segmentation: A Survey", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:VLnqNzywnoUC", "authors": ["Xiangtai Li", "Henghui Ding", "Wenwei Zhang", "Haobo Yuan", "Jiangmiao Pang", "Guangliang Cheng", "Kai Chen", "Ziwei Liu", "Chen Change Loy"], "publication_date": "2023/4/19", "journal": "arXiv preprint arXiv:2304.09854", "description": "Visual segmentation seeks to partition images, video frames, or point clouds into multiple segments or groups. This technique has numerous real-world applications, such as autonomous driving, image editing, robot sensing, and medical analysis. Over the past decade, deep learning-based methods have made remarkable strides in this area. Recently, transformers, a type of neural network based on self-attention originally designed for natural language processing, have considerably surpassed previous convolutional or recurrent approaches in various vision processing tasks. Specifically, vision transformers offer robust, unified, and even simpler solutions for various segmentation tasks. This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements. We first review the background, encompassing problem definitions, datasets, and prior convolutional methods. Next, we summarize a meta-architecture that unifies all recent transformer-based approaches. Based on this meta-architecture, we examine various method designs, including modifications to the meta-architecture and associated applications. We also present several closely related settings, including 3D point cloud segmentation, foundation model tuning, domain-aware segmentation, efficient segmentation, and medical segmentation. Additionally, we compile and re-evaluate the reviewed methods on several well-established datasets. Finally, we identify open challenges in this field and propose directions for future research. The project page can be found at https://github.com/lxtGH/Awesome-Segmenation-With-Transformer. We \u2026", "total_citations": 22, "citation_graph": {"2023": 22}}, {"title": "Voxurf: Voxel-based Efficient and Accurate Neural Surface Reconstruction", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:t6usbXjVLHcC", "authors": ["Tong Wu", "Jiaqi Wang", "Xingang Pan", "Xudong Xu", "Christian Theobalt", "Ziwei Liu", "Dahua Lin"], "publication_date": "2023", "conference": "International Conference on Learning Representations (ICLR)", "description": "Neural surface reconstruction aims to reconstruct accurate 3D surfaces based on multi-view images. Previous methods based on neural volume rendering mostly train a fully implicit model, and they require hours of training for a single scene. Recent efforts explore the explicit volumetric representation, which substantially accelerates the optimization process by memorizing significant information in learnable voxel grids. However, these voxel-based methods often struggle in reconstructing fine-grained geometry. Through empirical studies, we found that high-quality surface reconstruction hinges on two key factors: the capability of constructing a coherent shape and the precise modeling of color-geometry dependency. In particular, the latter is the key to the accurate reconstruction of fine details. Inspired by these findings, we develop Voxurf, a voxel-based approach for efficient and accurate neural surface reconstruction, which consists of two stages: 1) leverage a learnable feature grid to construct the color field and obtain a coherent coarse shape, and 2) refine detailed geometry with a dual color network that captures precise color-geometry dependency. We further introduce a hierarchical geometry feature to enable information sharing across voxels. Our experiments show that Voxurf achieves high efficiency and high quality at the same time. On the DTU benchmark, Voxurf achieves higher reconstruction quality compared to state-of-the-art methods, with 20x speedup in training.", "total_citations": 22, "citation_graph": {"2023": 22}}, {"title": "Playing for 3D Human Recovery", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:a0OBvERweLwC", "authors": ["Zhongang Cai", "Mingyuan Zhang", "Jiawei Ren", "Chen Wei", "Daxuan Ren", "Jiatong Li", "Zhengyu Lin", "Haiyu Zhao", "Shuai Yi", "Lei Yang", "Chen Change Loy", "Ziwei Liu"], "publication_date": "2021/10/14", "journal": "arXiv preprint arXiv:2110.07588", "description": "Image- and video-based 3D human recovery (i.e., pose and shape estimation) have achieved substantial progress. However, due to the prohibitive cost of motion capture, existing datasets are often limited in scale and diversity. In this work, we obtain massive human sequences by playing the video game with automatically annotated 3D ground truths. Specifically, we contribute GTA-Human, a large-scale 3D human dataset generated with the GTA-V game engine, featuring a highly diverse set of subjects, actions, and scenarios. More importantly, we study the use of game-playing data and obtain five major insights. First, game-playing data is surprisingly effective. A simple frame-based baseline trained on GTA-Human outperforms more sophisticated methods by a large margin. For video-based methods, GTA-Human is even on par with the in-domain training set. Second, we discover that synthetic data provides critical complements to the real data that is typically collected indoor. Our investigation into domain gap provides explanations for our data mixture strategies that are simple yet useful. Third, the scale of the dataset matters. The performance boost is closely related to the additional data available. A systematic study reveals the model sensitivity to data density from multiple key aspects. Fourth, the effectiveness of GTA-Human is also attributed to the rich collection of strong supervision labels (SMPL parameters), which are otherwise expensive to acquire in real datasets. Fifth, the benefits of synthetic data extend to larger models such as deeper convolutional neural networks (CNNs) and Transformers, for which a significant impact is also \u2026", "total_citations": 22, "citation_graph": {"2022": 7, "2023": 14}}, {"title": "MIMIC-IT: Multi-Modal In-Context Instruction Tuning", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:epqYDVWIO7EC", "authors": ["Bo Li", "Yuanhan Zhang", "Liangyu Chen", "Jinghao Wang", "Fanyi Pu", "Jingkang Yang", "Chunyuan Li", "Ziwei Liu"], "publication_date": "2023/6/8", "journal": "arXiv preprint arXiv:2306.05425", "description": "High-quality instructions and responses are essential for the zero-shot performance of large language models on interactive natural language tasks. For interactive vision-language tasks involving intricate visual scenes, a large quantity of diverse and creative instruction-response pairs should be imperative to tune vision-language models (VLMs). Nevertheless, the current availability of vision-language instruction-response pairs in terms of quantity, diversity, and creativity remains limited, posing challenges to the generalization of interactive VLMs. Here we present MultI-Modal In-Context Instruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodal instruction-response pairs, with 2.2 million unique instructions derived from images and videos. Each pair is accompanied by multi-modal in-context information, forming conversational contexts aimed at empowering VLMs in perception, reasoning, and planning. The instruction-response collection process, dubbed as Syphus, is scaled using an automatic annotation pipeline that combines human expertise with GPT's capabilities. Using the MIMIC-IT dataset, we train a large VLM named Otter. Based on extensive evaluations conducted on vision-language benchmarks, it has been observed that Otter demonstrates remarkable proficiency in multi-modal perception, reasoning, and in-context learning. Human evaluation reveals it effectively aligns with the user's intentions. We release the MIMIC-IT dataset, instruction-response collection pipeline, benchmarks, and the Otter model.", "total_citations": 21, "citation_graph": {"2023": 21}}, {"title": "Semi-Supervised Domain Generalization with Stochastic StyleMatch", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&pagesize=100&citation_for_view=lc45xlcAAAAJ:SeFeTyx0c_EC", "authors": ["Kaiyang Zhou", "Chen Change Loy", "Ziwei Liu"], "publication_date": "2023", "journal": "International Journal of Computer Vision (IJCV)", "description": "Ideally, visual learning algorithms should be generalizable, for dealing with any unseen domain shift when deployed in a new target environment; and data-efficient, for reducing development costs by using as little labels as possible. To this end, we study semi-supervised domain generalization (SSDG), which aims to learn a domain-generalizable model using multi-source, partially-labeled training data. We design two benchmarks that cover state-of-the-art methods developed in two related fields, i.e., domain generalization (DG) and semi-supervised learning (SSL). We find that the DG methods, which by design are unable to handle unlabeled data, perform poorly with limited labels in SSDG; the SSL methods, especially FixMatch, obtain much better results but are still far away from the basic vanilla model trained using full labels. We propose StyleMatch, a simple approach that extends FixMatch with a couple of \u2026", "total_citations": 21, "citation_graph": {"2022": 8, "2023": 11}}, {"title": "LaserMix for Semi-Supervised LiDAR Semantic Segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:D_sINldO8mEC", "authors": ["Lingdong Kong", "Jiawei Ren", "Liang Pan", "Ziwei Liu"], "publication_date": "2023", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Densely annotating LiDAR point clouds is costly, which often restrains the scalability of fully-supervised learning methods. In this work, we study the underexplored semi-supervised learning (SSL) in LiDAR semantic segmentation. Our core idea is to leverage the strong spatial cues of LiDAR point clouds to better exploit unlabeled data. We propose LaserMix to mix laser beams from different LiDAR scans and then encourage the model to make consistent and confident predictions before and after mixing. Our framework has three appealing properties. 1) Generic: LaserMix is agnostic to LiDAR representations (eg, range view and voxel), and hence our SSL framework can be universally applied. 2) Statistically grounded: We provide a detailed analysis to theoretically explain the applicability of the proposed framework. 3) Effective: Comprehensive experimental analysis on popular LiDAR segmentation datasets (nuScenes, SemanticKITTI, and ScribbleKITTI) demonstrates our effectiveness and superiority. Notably, we achieve competitive results over fully-supervised counterparts with 2x to 5x fewer labels and improve the supervised-only baseline significantly by relatively 10.8%. We hope this concise yet high-performing framework could facilitate future research in semi-supervised LiDAR segmentation. Code is publicly available.", "total_citations": 19, "citation_graph": {"2022": 2, "2023": 16}}, {"title": "Masked Frequency Modeling for Self-Supervised Visual Pre-Training", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:VOx2b1Wkg3QC", "authors": ["Jiahao Xie", "Wei Li", "Xiaohang Zhan", "Ziwei Liu", "Yew Soon Ong", "Chen Change Loy"], "publication_date": "2023", "conference": "International Conference on Learning Representations (ICLR)", "description": "We present Masked Frequency Modeling (MFM), a unified frequency-domain-based approach for self-supervised pre-training of visual models. Instead of randomly inserting mask tokens to the input embeddings in the spatial domain, in this paper, we shift the perspective to the frequency domain. Specifically, MFM first masks out a portion of frequency components of the input image and then predicts the missing frequencies on the frequency spectrum. Our key insight is that predicting masked components in the frequency domain is more ideal to reveal underlying image patterns rather than predicting masked patches in the spatial domain, due to the heavy spatial redundancy. Our findings suggest that with the right configuration of mask-and-predict strategy, both the structural information within high-frequency components and the low-level statistics among low-frequency counterparts are useful in learning good representations. For the first time, MFM demonstrates that, for both ViT and CNN, a simple non-Siamese framework can learn meaningful representations even using none of the following: (i) extra data, (ii) extra model, (iii) mask token. Experimental results on ImageNet and several robustness benchmarks show the competitive performance and advanced robustness of MFM compared with recent masked image modeling approaches. Furthermore, we also comprehensively investigate the effectiveness of classical image restoration tasks for representation learning from a unified frequency perspective and reveal their intriguing relations with our MFM approach. Project page: https://www.mmlab-ntu.com/project/mfm/index.html.", "total_citations": 19, "citation_graph": {"2022": 3, "2023": 16}}, {"title": "Computation-Efficient Knowledge Distillation via Uncertainty-Aware Mixup", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:70eg2SAEIzsC", "authors": ["Guodong Xu", "Ziwei Liu", "Chen Change Loy"], "publication_date": "2023", "journal": "Pattern Recognition (PR)", "description": "Knowledge distillation (KD) has emerged as an essential technique not only for model compression, but also other learning tasks such as continual learning. Given the richer application spectrum and potential online usage of KD, knowledge distillation efficiency becomes a pivotal component. In this work, we study this little-explored but important topic. Unlike previous works that focus solely on the accuracy of student network, we attempt to achieve a harder goal \u2013 to obtain a performance comparable to conventional KD with a lower computation cost during the transfer. To this end, we present UNcertainty-aware mIXup (UNIX), an effective approach that can reduce transfer cost by 20% to 30% and yet maintain comparable or achieve even better student performance than conventional KD. This is made possible via effective uncertainty sampling and a novel adaptive mixup approach that select informative samples \u2026", "total_citations": 18, "citation_graph": {"2022": 6, "2023": 11}}, {"title": "VToonify: Controllable High-Resolution Portrait Video Style Transfer", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:dTyEYWd-f8wC", "authors": ["Shuai Yang", "Liming Jiang", "Ziwei Liu", "Chen Change Loy"], "publication_date": "2022", "journal": "ACM Transactions on Graphics (SIGGRAPH Asia)", "description": "Generating high-quality artistic portrait videos is an important and desirable task in computer graphics and vision. Although a series of successful portrait image toonification models built upon the powerful StyleGAN have been proposed, these image-oriented methods have obvious limitations when applied to videos, such as the fixed frame size, the requirement of face alignment, missing non-facial details and temporal inconsistency. In this work, we investigate the challenging controllable high-resolution portrait video style transfer by introducing a novel VToonify framework. Specifically, VToonify leverages the mid- and high-resolution layers of StyleGAN to render high-quality artistic portraits based on the multi-scale content features extracted by an encoder to better preserve the frame details. The resulting fully convolutional architecture accepts non-aligned faces in videos of variable size as input, contributing to \u2026", "total_citations": 17, "citation_graph": {"2023": 17}}, {"title": "Text2Light: Zero-Shot Text-Driven HDR Panorama Generation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:JoZmwDi-zQgC", "authors": ["Zhaoxi Chen", "Guangcong Wang", "Ziwei Liu"], "publication_date": "2022", "journal": "ACM Transactions on Graphics (SIGGRAPH Asia)", "description": "High-quality HDRIs (High Dynamic Range Images), typically HDR panoramas, are one of the most popular ways to create photorealistic lighting and 360-degree reflections of 3D scenes in graphics. Given the difficulty of capturing HDRIs, a versatile and controllable generative model is highly desired, where layman users can intuitively control the generation process. However, existing state-of-the-art methods still struggle to synthesize high-quality panoramas for complex scenes. In this work, we propose a zero-shot text-driven framework, Text2Light, to generate 4K+ resolution HDRIs without paired training data. Given a free-form text as the description of the scene, we synthesize the corresponding HDRI with two dedicated steps: 1) text-driven panorama generation in low dynamic range (LDR) and low resolution (LR), and 2) super-resolution inverse tone mapping to scale up the LDR panorama both in resolution \u2026", "total_citations": 17, "citation_graph": {"2022": 1, "2023": 16}}, {"title": "Visual Sound Localization in the Wild by Cross-Modal Interference Erasing", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:SP6oXDckpogC", "authors": ["Xian Liu", "Rui Qian", "Hang Zhou", "Di Hu", "Weiyao Lin", "Ziwei Liu", "Bolei Zhou", "Xiaowei Zhou"], "publication_date": "2022", "conference": "AAAI Conference on Artificial Intelligence (AAAI)", "description": "The task of audiovisual sound source localization has been well studied under constrained scenes, where the audio recordings are clean. However, in real world scenarios, audios are usually contaminated by off screen sound and background noise. They will interfere with the procedure of identifying desired sources and building visual sound connections, making previous studies nonapplicable. In this work, we propose the Interference Eraser (IEr) framework, which tackles the problem of audiovisual sound source localization in the wild. The key idea is to eliminate the interference by redefining and carving discriminative audio representations. Specifically, we observe that the previous practice of learning only a single audio representation is insufficient due to the additive nature of audio signals. We thus extend the audio representation with our Audio Instance Identifier module, which clearly distinguishes sounding instances when audio signals of different volumes are unevenly mixed. Then we erase the influence of the audible but off screen sounds and the silent but visible objects by a Cross modal Referrer module with cross modality distillation. Quantitative and qualitative evaluations demonstrate that our framework achieves superior results on sound localization tasks, especially under real world scenarios.", "total_citations": 17, "citation_graph": {"2022": 7, "2023": 10}}, {"title": "What Makes Good Examples for Visual In-Context Learning?", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:PR6Y55bgFSsC", "authors": ["Yuanhan Zhang", "Kaiyang Zhou", "Ziwei Liu"], "publication_date": "2023", "conference": "Neural Information Processing Systems (NeurIPS)", "description": "Large-scale models trained on broad data have recently become the mainstream architecture in computer vision due to their strong generalization performance. In this paper, the main focus is on an emergent ability in large vision models, known as in-context learning, which allows inference on unseen tasks by conditioning on in-context examples (a.k.a.~prompt) without updating the model parameters. This concept has been well-known in natural language processing but has only been studied very recently for large vision models. We for the first time provide a comprehensive investigation on the impact of in-context examples in computer vision, and find that the performance is highly sensitive to the choice of in-context examples. To overcome the problem, we propose a prompt retrieval framework to automate the selection of in-context examples. Specifically, we present (1) an unsupervised prompt retrieval method based on nearest example search using an off-the-shelf model, and (2) a supervised prompt retrieval method, which trains a neural network to choose examples that directly maximize in-context learning performance. The results demonstrate that our methods can bring non-trivial improvements to visual in-context learning in comparison to the commonly-used random selection.", "total_citations": 16, "citation_graph": {"2023": 16}}, {"title": "OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:1yQoGdGgb4wC", "authors": ["Tong Wu", "Jiarui Zhang", "Xiao Fu", "Yuxin Wang", "Jiawei Ren", "Liang Pan", "Wayne Wu", "Lei Yang", "Jiaqi Wang", "Chen Qian", "Dahua Lin", "Ziwei Liu"], "publication_date": "2023", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Recent advances in modeling 3D objects mostly rely on synthetic datasets due to the lack of large-scale real-scanned 3D databases. To facilitate the development of 3D perception, reconstruction, and generation in the real world, we propose OmniObject3D, a large vocabulary 3D object dataset with massive high-quality real-scanned 3D objects. OmniObject3D has several appealing properties: 1) Large Vocabulary: It comprises 6,000 scanned objects in 190 daily categories, sharing common classes with popular 2D datasets (eg, ImageNet and LVIS), benefiting the pursuit of generalizable 3D representations. 2) Rich Annotations: Each 3D object is captured with both 2D and 3D sensors, providing textured meshes, point clouds, multiview rendered images, and multiple real-captured videos. 3) Realistic Scans: The professional scanners support high-quality object scans with precise shapes and realistic appearances. With the vast exploration space offered by OmniObject3D, we carefully set up four evaluation tracks: a) robust 3D perception, b) novel-view synthesis, c) neural surface reconstruction, and d) 3D object generation. Extensive studies are performed on these four benchmarks, revealing new observations, challenges, and opportunities for future research in realistic 3D vision.", "total_citations": 16, "citation_graph": {"2023": 16}}, {"title": "Sparse Mixture-of-Experts are Domain Generalizable Learners", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:LPZeul_q3PIC", "authors": ["Bo Li", "Yifei Shen", "Jingkang Yang", "Yezhen Wang", "Jiawei Ren", "Tong Che", "Jun Zhang", "Ziwei Liu"], "publication_date": "2023", "conference": "International Conference on Learning Representations (ICLR)", "description": "Domain generalization (DG) aims at learning generalizable models under distribution shifts to avoid redundantly overfitting massive training data. Previous works with complex loss design and gradient constraint have not yet led to empirical success on large-scale benchmarks. In this work, we reveal the mixture-of-experts (MoE) model's generalizability on DG by leveraging to distributively handle multiple aspects of the predictive features across domains. To this end, we propose Sparse Fusion Mixture-of-Experts (SF-MoE), which incorporates sparsity and fusion mechanisms into the MoE framework to keep the model both sparse and predictive. SF-MoE has two dedicated modules: 1) sparse block and 2) fusion block, which disentangle and aggregate the diverse learned signals of an object, respectively. Extensive experiments demonstrate that SF-MoE is a domain-generalizable learner on large-scale benchmarks. It outperforms state-of-the-art counterparts by more than 2% across 5 large-scale DG datasets (e.g., DomainNet), with the same or even lower computational costs. We further reveal the internal mechanism of SF-MoE from distributed representation perspective (e.g., visual attributes). We hope this framework could facilitate future research to push generalizable object recognition to the real world. Code and models are released at https://github.com/Luodian/SF-MoE-DG.", "total_citations": 15, "citation_graph": {"2022": 2, "2023": 13}}, {"title": "StyleSwap: Style-Based Generator Empowers Robust Face Swapping", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:tkaPQYYpVKoC", "authors": ["Zhiliang Xu", "Hang Zhou", "Zhibin Hong", "Ziwei Liu", "Jiaming Liu", "Zhizhi Guo", "Junyu Han", "Jingtuo Liu", "Errui Ding", "Jingdong Wang"], "publication_date": "2022", "conference": "European Conference on Computer Vision (ECCV)", "description": "Numerous attempts have been made to the task of person-agnostic face swapping given its wide applications. While existing methods mostly rely on tedious network and loss designs, they still struggle in the information balancing between the source and target faces, and tend to produce visible artifacts. In this work, we introduce a concise and effective framework named StyleSwap. Our core idea is to leverage a style-based generator to empower high-fidelity and robust face swapping, thus the generator\u2019s advantage can be adopted for optimizing identity similarity. We identify that with only minimal modifications, a StyleGAN2 architecture can successfully handle the desired information from both source and target. Additionally, inspired by the ToRGB layers, a Swapping-Driven Mask Branch is further devised to improve information blending. Furthermore, the advantage of StyleGAN inversion can be adopted \u2026", "total_citations": 15, "citation_graph": {"2022": 1, "2023": 14}}, {"title": "Relighting4D: Neural Relightable Human from Videos", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:q3oQSFYPqjQC", "authors": ["Zhaoxi Chen", "Ziwei Liu"], "publication_date": "2022", "conference": "European Conference on Computer Vision (ECCV)", "description": "Human relighting is a highly desirable yet challenging task. Existing works either require expensive one-light-at-a-time (OLAT) captured data using light stage or cannot freely change the viewpoints of the rendered body. In this work, we propose a principled framework, Relighting4D, that enables free-viewpoints relighting from only human videos under unknown illuminations. Our key insight is that the space-time varying geometry and reflectance of the human body can be decomposed as a set of neural fields of normal, occlusion, diffuse, and specular maps. These neural fields are further integrated into reflectance-aware physically based rendering, where each vertex in the neural field absorbs and reflects the light from the environment. The whole framework can be learned from videos in a self-supervised manner, with physically informed priors designed for regularization. Extensive experiments on both real and \u2026", "total_citations": 15, "citation_graph": {"2022": 1, "2023": 14}}, {"title": "MMBench: Is Your Multi-modal Model an All-around Player?", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:Z5m8FVwuT1cC", "authors": ["Yuan Liu", "Haodong Duan", "Yuanhan Zhang", "Bo Li", "Songyang Zhang", "Wangbo Zhao", "Yike Yuan", "Jiaqi Wang", "Conghui He", "Ziwei Liu", "Kai Chen", "Dahua Lin"], "publication_date": "2023/7/12", "journal": "arXiv preprint arXiv:2307.06281", "description": "Large vision-language models have recently achieved remarkable progress, exhibiting great perception and reasoning abilities concerning visual information. However, how to effectively evaluate these large vision-language models remains a major obstacle, hindering future model development. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but suffer from a lack of fine-grained ability assessment and non-robust evaluation metrics. Recent subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model's abilities by incorporating human labor, but they are not scalable and display significant bias. In response to these challenges, we propose MMBench, a novel multi-modality benchmark. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of two elements. The first element is a meticulously curated dataset that surpasses existing similar benchmarks in terms of the number and variety of evaluation questions and abilities. The second element introduces a novel CircularEval strategy and incorporates the use of ChatGPT. This implementation is designed to convert free-form predictions into pre-defined choices, thereby facilitating a more robust evaluation of the model's predictions. MMBench is a systematically-designed objective benchmark for robustly evaluating the various abilities of vision-language models. We hope MMBench will assist the research community in better evaluating their models and encourage future advancements in this domain. Project page: https://opencompass.org.cn/mmbench.", "total_citations": 14, "citation_graph": {"2023": 14}}, {"title": "Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:NJ774b8OgUMC", "authors": ["Lingting Zhu", "Xian Liu", "Xuanyu Liu", "Rui Qian", "Ziwei Liu", "Lequan Yu"], "publication_date": "2023", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Animating virtual avatars to make co-speech gestures facilitates various applications in human-machine interaction. The existing methods mainly rely on generative adversarial networks (GANs), which typically suffer from notorious mode collapse and unstable training, thus making it difficult to learn accurate audio-gesture joint distributions. In this work, we propose a novel diffusion-based framework, named Diffusion Co-Speech Gesture (DiffGesture), to effectively capture the cross-modal audio-to-gesture associations and preserve temporal coherence for high-fidelity audio-driven co-speech gesture generation. Specifically, we first establish the diffusion-conditional generation process on clips of skeleton sequences and audio to enable the whole framework. Then, a novel Diffusion Audio-Gesture Transformer is devised to better attend to the information from multiple modalities and model the long-term temporal dependency. Moreover, to eliminate temporal inconsistency, we propose an effective Diffusion Gesture Stabilizer with an annealed noise sampling strategy. Benefiting from the architectural advantages of diffusion models, we further incorporate implicit classifier-free guidance to trade off between diversity and gesture quality. Extensive experiments demonstrate that DiffGesture achieves state-of-the-art performance, which renders coherent gestures with better mode coverage and stronger audio correlations. Code is available at https://github. com/Advocate99/DiffGesture.", "total_citations": 14, "citation_graph": {"2023": 14}}, {"title": "Full-Spectrum Out-of-Distribution Detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:tOudhMTPpwUC", "authors": ["Jingkang Yang", "Kaiyang Zhou", "Ziwei Liu"], "publication_date": "2023", "journal": "International Journal of Computer Vision (IJCV)", "description": "Existing out-of-distribution (OOD) detection literature clearly defines semantic shift as a sign of OOD but does not have a consensus over covariate shift. Samples experiencing covariate shift but not semantic shift from the in-distribution (ID) are either excluded from the test set or treated as OOD, which contradicts the primary goal in machine learning\u2014being able to generalize beyond the training distribution. In this paper, we take into account both shift types and introduce full-spectrum OOD (F-OOD) detection, a more realistic problem setting that considers both detecting semantic shift and being tolerant to covariate shift; and design three benchmarks. These new benchmarks have a more fine-grained categorization of distributions (i.elet@tokeneonedot, training ID, covariate-shifted ID, near-OOD, and far-OOD) for the purpose of more comprehensively evaluating the pros and cons of algorithms. To address the F \u2026", "total_citations": 14, "citation_graph": {"2022": 3, "2023": 10}}, {"title": "Garment4D: Garment Reconstruction from Point Cloud Sequences", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:NhqRSupF_l8C", "authors": ["Fangzhou Hong", "Liang Pan", "Zhongang Cai", "Ziwei Liu"], "publication_date": "2021/5/21", "conference": "Thirty-Fifth Conference on Neural Information Processing Systems (NeurIPS)", "description": "Learning to reconstruct 3D garments is important for dressing 3D human bodies of different shapes in different poses. Previous works typically rely on 2D images as input, which however suffer from the scale and pose ambiguities. To circumvent the problems caused by 2D images, we propose a principled framework, Garment4D, that uses 3D point cloud sequences of dressed humans for garment reconstruction. Garment4D has three dedicated steps: sequential garments registration, canonical garment estimation, and posed garment reconstruction. The main challenges are two-fold: 1) effective 3D feature learning for fine details, and 2) capture of garment dynamics caused by the interaction between garments and the human body, especially for loose garments like skirts. To unravel these problems, we introduce a novel Proposal-Guided Hierarchical Feature Network and Iterative Graph Convolution Network, which integrate both high-level semantic features and low-level geometric features for fine details reconstruction. Furthermore, we propose a Temporal Transformer for smooth garment motions capture. Unlike non-parametric methods, the reconstructed garment meshes by our method are separable from the human body and have strong interpretability, which is desirable for downstream tasks. As the first attempt at this task, high-quality reconstruction results are qualitatively and quantitatively illustrated through extensive experiments. Codes are available at https://github. com/hongfz16/Garment4D.", "total_citations": 14, "citation_graph": {"2022": 6, "2023": 7}}, {"title": "Benchmarking and Analyzing 3D Human Pose and Shape Estimation Beyond Algorithms", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:9vf0nzSNQJEC", "authors": ["Hui En Pang", "Zhongang Cai", "Lei Yang", "Tianwei Zhang", "Ziwei Liu"], "publication_date": "2022", "conference": "NeurIPS (Datasets and Benchmarks Track)", "description": "3D human pose and shape estimation (aka``human mesh recovery'') has achieved substantial progress. Researchers mainly focus on the development of novel algorithms, while less attention has been paid to other critical factors involved. This could lead to less optimal baselines, hindering the fair and faithful evaluations of newly designed methodologies. To address this problem, this work presents the\\textit {first} comprehensive benchmarking study from three under-explored perspectives beyond algorithms.\\emph {1) Datasets.} An analysis on 31 datasets reveals the distinct impacts of data samples: datasets featuring critical attributes (\\emph {ie} diverse poses, shapes, camera characteristics, backbone features) are more effective. Strategical selection and combination of high-quality datasets can yield a significant boost to the model performance.\\emph {2) Backbones.} Experiments with 10 backbones, ranging from CNNs to transformers, show the knowledge learnt from a proximity task is readily transferable to human mesh recovery.\\emph {3) Training strategies.} Proper augmentation techniques and loss designs are crucial. With the above findings, we achieve a PA-MPJPE of 47.3 (mm) on the 3DPW test set with a relatively simple model. More importantly, we provide strong baselines for fair comparisons of algorithms, and recommendations for building effective training configurations in the future. Codebase is available at\\url {https://github. com/smplbody/hmr-benchmarks}.", "total_citations": 13, "citation_graph": {"2023": 13}}, {"title": "Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:vDijr-p_gm4C", "authors": ["Shuai Yang", "Yifan Zhou", "Ziwei Liu", "Chen Change Loy"], "publication_date": "2023/6/13", "journal": "arXiv preprint arXiv:2306.07954", "description": "Large text-to-image diffusion models have exhibited impressive proficiency in generating high-quality images. However, when applying these models to video domain, ensuring temporal consistency across video frames remains a formidable challenge. This paper proposes a novel zero-shot text-guided video-to-video translation framework to adapt image models to videos. The framework includes two parts: key frame translation and full video translation. The first part uses an adapted diffusion model to generate key frames, with hierarchical cross-frame constraints applied to enforce coherence in shapes, textures and colors. The second part propagates the key frames to other frames with temporal-aware patch matching and frame blending. Our framework achieves global style and local texture temporal consistency at a low cost (without re-training or optimization). The adaptation is compatible with existing image diffusion techniques, allowing our framework to take advantage of them, such as customizing a specific subject with LoRA, and introducing extra spatial guidance with ControlNet. Extensive experimental results demonstrate the effectiveness of our proposed framework over existing methods in rendering high-quality and temporally-coherent videos.", "total_citations": 12, "citation_graph": {"2023": 12}}, {"title": "Robo3D: Towards Robust and Reliable 3D Perception against Corruptions", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:_Re3VWB3Y0AC", "authors": ["Lingdong Kong", "Youquan Liu", "Xin Li", "Runnan Chen", "Wenwei Zhang", "Jiawei Ren", "Liang Pan", "Kai Chen", "Ziwei Liu"], "publication_date": "2023", "conference": "IEEE International Conference on Computer Vision (ICCV)", "description": "The robustness of 3D perception systems under natural corruptions from environments and sensors is pivotal for safety-critical applications. Existing large-scale 3D perception datasets often contain data that are meticulously cleaned. Such configurations, however, cannot reflect the reliability of perception models during the deployment stage. In this work, we present Robo3D, the first comprehensive benchmark heading toward probing the robustness of 3D detectors and segmentors under out-of-distribution scenarios against natural corruptions that occur in real-world environments. Specifically, we consider eight corruption types stemming from severe weather conditions, external disturbances, and internal sensor failure. We uncover that, although promising results have been progressively achieved on standard benchmarks, state-of-the-art 3D perception models are at risk of being vulnerable to corruptions. We draw key observations on the use of data representations, augmentation schemes, and training strategies, that could severely affect the model's performance. To pursue better robustness, we propose a density-insensitive training framework along with a simple flexible voxelization strategy to enhance the model resiliency. We hope our benchmark and approach could inspire future research in designing more robust and reliable 3D perception models. Our robustness benchmark suite is publicly available.", "total_citations": 12, "citation_graph": {"2022": 1, "2023": 11}}, {"title": "Rethinking Range View Representation for LiDAR Segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:kzcrU_BdoSEC", "authors": ["Lingdong Kong", "Youquan Liu", "Runnan Chen", "Yuexin Ma", "Xinge Zhu", "Yikang Li", "Yuenan Hou", "Yu Qiao", "Ziwei Liu"], "publication_date": "2023", "conference": "IEEE International Conference on Computer Vision (ICCV)", "description": "LiDAR segmentation is crucial for autonomous driving perception. Recent trends favor point-or voxel-based methods as they often yield better performance than the traditional range view representation. In this work, we unveil several key factors in building powerful range view models. We observe that the\" many-to-one\" mapping, semantic incoherence, and shape deformation are possible impediments against effective learning from range view projections. We present RangeFormer--a full-cycle framework comprising novel designs across network architecture, data augmentation, and post-processing--that better handles the learning and processing of LiDAR point clouds from the range view. We further introduce a Scalable Training from Range view (STR) strategy that trains on arbitrary low-resolution 2D range images, while still maintaining satisfactory 3D segmentation accuracy. We show that, for the first time, a range view method is able to surpass the point, voxel, and multi-view fusion counterparts in the competing LiDAR semantic and panoptic segmentation benchmarks, ie, SemanticKITTI, nuScenes, and ScribbleKITTI.", "total_citations": 12, "citation_graph": {"2022": 1, "2023": 11}}, {"title": "Chasing the Tail in Monocular 3D Human Reconstruction with Prototype Memory", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:ldfaerwXgEUC", "authors": ["Yu Rong", "Ziwei Liu", "Chen Change Loy"], "publication_date": "2022", "journal": "IEEE Transactions on Image Processing (TIP)", "description": "Deep neural networks have achieved remarkable progress in single-image 3D human reconstruction. However, existing methods still fall short in predicting rare poses. The reason is that most of the current models perform regression based on a single human prototype, which is similar to common poses while far from the rare poses. In this work, we 1) identify and analyze this learning obstacle and 2) propose a prototype memory-augmented network, PM-Net, that effectively improves performances of predicting rare poses. The core of our framework is a memory module that learns and stores a set of 3D human prototypes capturing local distributions for either common poses or rare poses. With this formulation, the regression starts from a better initialization, which is relatively easier to converge. Extensive experiments on several widely employed datasets demonstrate the proposed framework\u2019s effectiveness \u2026", "total_citations": 12, "citation_graph": {"2021": 5, "2022": 2, "2023": 5}}, {"title": "ShineOn: Illuminating Design Choices for Practical Video-based Virtual Clothing Try-on", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:b0M2c_1WBrUC", "authors": ["Gaurav Kuppa", "Andrew Jong", "Xin Liu", "Ziwei Liu", "Teng-Sheng Moh"], "publication_date": "2021", "conference": "Winter Conference on Applications of Computer Vision (WACV)", "description": "Virtual try-on has garnered interest as a neural rendering benchmark task to evaluate complex object transfer and scene composition. Recent works in virtual clothing try-on feature a plethora of possible architectural and data representation choices. However, they present little clarity on quantifying the isolated visual effect of each choice, nor do they specify the hyperparameter details that are key to experimental reproduction. Our work, ShineOn, approaches the try-on task from a bottom-up approach and aims to shine light on the visual and quantitative effects of each experiment. We build a series of scientific experiments to isolate effective design choices in video synthesis for virtual clothing try-on. Specifically, we investigate the effect of different pose annotations, self-attention layer placement, and activation functions on the quantitative and qualitative performance of video virtual try-on. We find that DensePose annotations not only enhance face details but also decrease memory usage and training time. Next, we find that attention layers improve face and neck quality. Finally, we show that GELU and ReLU activation functions are the most effective in our experiments despite the appeal of newer activations such as Swish and Sine. We will release a well-organized code base, hyperparameters, and model checkpoints to support the reproducibility of our results. We expect our extensive experiments and code to greatly inform future design choices in video virtual try-on. Our code may be accessed at https://github. com/andrewjong/ShineOn-Virtual-Tryon.", "total_citations": 12, "citation_graph": {"2021": 4, "2022": 3, "2023": 5}}, {"title": "Speech2Talking-Face: Inferring and Driving a Face with Synchronized Audio-Visual Representation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:fPk4N6BV_jEC", "authors": ["Yasheng Sun", "Hang Zhou", "Ziwei Liu", "Hideki Koike"], "publication_date": "2021", "conference": "International Joint Conference on Artificial Intelligence (IJCAI)", "description": "What can we picture solely from a clip of speech? Previous research has shown the possibility of directly inferring the appearance of a person\u2019s face by listening to a voice. However, within human speech lies not only the biometric identity signal but also the identity-irrelevant information such as the speech content. Our goal is to extract such information from a clip of speech. In particular, we aim at not only inferring the face of a person but also animating it. Our key insight is to synchronize audio and visual representations from two perspectives in a style-based generative framework. Specifically, contrastive learning is leveraged to map both the identity and speech content information within audios to visual representation spaces. Furthermore, the identity space is strengthened with class centroids. Through curriculum learning, the style-based generator is capable of automatically balancing the information from the two latent spaces. Extensive experiments show that our approach encourages better speech-identity correlation learning while generating vivid faces whose identities are consistent with given speech samples. Moreover, the same model enables these inferred faces to talk driven by the audios.", "total_citations": 12, "citation_graph": {"2022": 5, "2023": 7}}, {"title": "Full-Range Virtual Try-On With Recurrent Tri-Level Transform", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:5Ul4iDaHHb8C", "authors": ["Han Yang", "Xinrui Yu", "Ziwei Liu"], "publication_date": "2022", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Virtual try-on aims to transfer a target clothing image onto a reference person. Though great progress has been achieved, the functioning zone of existing works is still limited to standard clothes (eg, plain shirt without complex laces or ripped effect), while the vast complexity and variety of non-standard clothes (eg, off-shoulder shirt, word-shoulder dress) are largely ignored. In this work, we propose a principled framework, Recurrent Tri-Level Transform (RT-VTON), that performs full-range virtual try-on on both standard and non-standard clothes. We have two key insights towards the framework design: 1) Semantics transfer requires a gradual feature transform on three different levels of clothing representations, namely clothes code, pose code and parsing code. 2) Geometry transfer requires a regularized image deformation between rigidity and flexibility. Firstly, we predict the semantics of the\" after-try-on\" person by recurrently refining the tri-level feature codes using local gated attention and non-local correspondence learning. Next, we design a semi-rigid deformation to align the clothing image and the predicted semantics, which preserves local warping similarity. Finally, a canonical try-on synthesizer fuses all the processed information to generate the clothed person image. Extensive experiments on conventional benchmarks along with user studies demonstrate that our framework achieves state-of-the-art performance both quantitatively and qualitatively. Notably, RT-VTON shows compelling results on a wide range of non-standard clothes.", "total_citations": 11, "citation_graph": {"2022": 4, "2023": 7}}, {"title": "CelebA-Spoof Challenge 2020 on Face Anti-Spoofing: Methods and Results", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:NaGl4SEjCO4C", "authors": ["Yuanhan Zhang", "Zhenfei Yin", "Jing Shao", "Ziwei Liu", "Shuo Yang", "Yuanjun Xiong", "Wei Xia", "Yan Xu", "Man Luo", "Jian Liu", "Jianshu Li", "Zhijun Chen", "Mingyu Guo", "Hui Li", "Junfu Liu", "Pengfei Gao", "Tianqi Hong", "Hao Han", "Shijie Liu", "Xinhua Chen", "Di Qiu", "Cheng Zhen", "Dashuang Liang", "Yufeng Jin", "Zhanlong Hao"], "publication_date": "2021/2/25", "journal": "arXiv preprint arXiv:2102.12642", "description": "As facial interaction systems are prevalently deployed, security and reliability of these systems become a critical issue, with substantial research efforts devoted. Among them, face anti-spoofing emerges as an important area, whose objective is to identify whether a presented face is live or spoof. Recently, a large-scale face anti-spoofing dataset, CelebA-Spoof which comprised of 625,537 pictures of 10,177 subjects has been released. It is the largest face anti-spoofing dataset in terms of the numbers of the data and the subjects. This paper reports methods and results in the CelebA-Spoof Challenge 2020 on Face AntiSpoofing which employs the CelebA-Spoof dataset. The model evaluation is conducted online on the hidden test set. A total of 134 participants registered for the competition, and 19 teams made valid submissions. We will analyze the top ranked solutions and present some discussion on future work directions.", "total_citations": 11, "citation_graph": {"2021": 2, "2022": 3, "2023": 6}}, {"title": "Benchmarking Omni-Vision Representation through the Lens of Visual Realms", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:kRWSkSYxWN8C", "authors": ["Yuanhan Zhang", "Zhenfei Yin", "Jing Shao", "Ziwei Liu"], "publication_date": "2022", "conference": "European Conference on Computer Vision (ECCV)", "description": "Though impressive performance has been achieved in specific visual realms (e.g. faces, dogs, and places), an omni-vision representation generalizing to many natural visual domains is highly desirable. But, existing benchmarks are biased and inefficient to evaluate the omni-vision representation\u2014these benchmarks either only include several specific realms, or cover most realms at the expense of subsuming numerous datasets that have extensive realm overlapping. In this paper, we propose Omni-Realm Benchmark (OmniBenchmark). It includes 21 realm-wise datasets with 7,372 concepts and 1,074,346 images. Without semantic overlapping, these datasets cover most visual realms comprehensively and meanwhile efficiently. In addition, we propose a new supervised contrastive learning framework, namely Relational Contrastive learning (ReCo), for a better omni-vision representation. Beyond pulling two \u2026", "total_citations": 10, "citation_graph": {"2022": 1, "2023": 9}}, {"title": "Detecting and Recovering Sequential DeepFake Manipulation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:eflP2zaiRacC", "authors": ["Rui Shao", "Tianxing Wu", "Ziwei Liu"], "publication_date": "2022", "conference": "European Conference on Computer Vision (ECCV)", "description": "Since photorealistic faces can be readily generated by facial manipulation technologies nowadays, potential malicious abuse of these technologies has drawn great concerns. Numerous deepfake detection methods are thus proposed. However, existing methods only focus on detecting one-step facial manipulation. As the emergence of easy-accessible facial editing applications, people can easily manipulate facial components using multi-step operations in a sequential manner. This new threat requires us to detect a sequence of facial manipulations, which is vital for both detecting deepfake media and recovering original faces afterwards. Motivated by this observation, we emphasize the need and propose a novel research problem called Detecting Sequential DeepFake Manipulation (Seq-DeepFake). Unlike the existing deepfake detection task only demanding a binary label prediction, detecting Seq-DeepFake \u2026", "total_citations": 10, "citation_graph": {"2023": 10}}, {"title": "Robust Partial-to-Partial Point Cloud Registration in a Full Range", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:1sJd4Hv_s6UC", "authors": ["Liang Pan", "Zhongang Cai", "Ziwei Liu"], "publication_date": "2021/11/30", "journal": "arXiv preprint arXiv:2111.15606", "description": "Point cloud registration for 3D objects is a challenging task due to sparse and noisy measurements, incomplete observations and large transformations. In this work, we propose \\textbf{G}raph \\textbf{M}atching \\textbf{C}onsensus \\textbf{Net}work (\\textbf{GMCNet}), which estimates pose-invariant correspondences for full-range Partial-to-Partial point cloud Registration (PPR) in the object-level registration scenario. To encode robust point descriptors, \\textbf{1)} we first comprehensively investigate transformation-robustness and noise-resilience of various geometric features. \\textbf{2)} Then, we employ a novel {T}ransformation-robust {P}oint {T}ransformer (\\textbf{TPT}) module to adaptively aggregate local features regarding the structural relations, which takes advantage from both handcrafted rotation-invariant ({\\textit{RI}}) features and noise-resilient spatial coordinates. \\textbf{3)} Based on a synergy of hierarchical graph networks and graphical modeling, we propose the {H}ierarchical {G}raphical {M}odeling (\\textbf{HGM}) architecture to encode robust descriptors consisting of i) a unary term learned from {\\textit{RI}} features; and ii) multiple smoothness terms encoded from neighboring point relations at different scales through our TPT modules. Moreover, we construct a challenging PPR dataset (\\textbf{MVP-RG}) based on the recent MVP dataset that features high-quality scans. Extensive experiments show that GMCNet outperforms previous state-of-the-art methods for PPR. Notably, GMCNet encodes point descriptors for each point cloud individually without using cross-contextual information, or ground truth correspondences for training. Our code \u2026", "total_citations": 10, "citation_graph": {"2021": 1, "2022": 5, "2023": 4}}, {"title": "Detecting and Grounding Multi-Modal Media Manipulation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:35r97b3x0nAC", "authors": ["Rui Shao", "Tianxing Wu", "Ziwei Liu"], "publication_date": "2023", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Misinformation has become a pressing issue. Fake media, in both visual and textual forms, is widespread on the web. While various deepfake detection and text fake news detection methods have been proposed, they are only designed for single-modality forgery based on binary classification, let alone analyzing and reasoning subtle forgery traces across different modalities. In this paper, we highlight a new research problem for multi-modal fake media, namely Detecting and Grounding Multi-Modal Media Manipulation (DGM^ 4). DGM^ 4 aims to not only detect the authenticity of multi-modal media, but also ground the manipulated content (ie, image bounding boxes and text tokens), which requires deeper reasoning of multi-modal media manipulation. To support a large-scale investigation, we construct the first DGM^ 4 dataset, where image-text pairs are manipulated by various approaches, with rich annotation of diverse manipulations. Moreover, we propose a novel HierArchical Multi-modal Manipulation rEasoning tRansformer (HAMMER) to fully capture the fine-grained interaction between different modalities. HAMMER performs 1) manipulation-aware contrastive learning between two uni-modal encoders as shallow manipulation reasoning, and 2) modality-aware cross-attention by multi-modal aggregator as deep manipulation reasoning. Dedicated manipulation detection and grounding heads are integrated from shallow to deep levels based on the interacted multi-modal information. Finally, we build an extensive benchmark and set up rigorous evaluation metrics for this new research problem. Comprehensive experiments demonstrate the \u2026", "total_citations": 9, "citation_graph": {"2023": 9}}, {"title": "Masked Lip-Sync Prediction by Audio-Visual Contextual Exploitation in Transformers", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:_Ybze24A_UAC", "authors": ["Yasheng Sun", "Hang Zhou", "Kaisiyuan Wang", "Qianyi Wu", "Zhibin Hong", "Jingtuo Liu", "Errui Ding", "Jingdong Wang", "Ziwei Liu", "Koike Hideki"], "publication_date": "2022", "conference": "SIGGRAPH Asia (Conference Track)", "description": "Previous studies have explored generating accurately lip-synced talking faces for arbitrary targets given audio conditions. However, most of them deform or generate the whole facial area, leading to non-realistic results. In this work, we delve into the formulation of altering only the mouth shapes of the target person. This requires masking a large percentage of the original image and seamlessly inpainting it with the aid of audio and reference frames. To this end, we propose the Audio-Visual Context-Aware Transformer (AV-CAT) framework, which produces accurate lip-sync with photo-realistic quality by predicting the masked mouth shapes. Our key insight is to exploit desired contextual information provided in audio and visual modalities thoroughly with delicately designed Transformers. Specifically, we propose a convolution-Transformer hybrid backbone and design an attention-based fusion strategy for filling the \u2026", "total_citations": 9, "citation_graph": {"2023": 9}}, {"title": "StyleLight: HDR Panorama Generation for Lighting Estimation and Editing", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:J-pR_7NvFogC", "authors": ["Guangcong Wang", "Yinuo Yang", "Chen Change Loy", "Ziwei Liu"], "publication_date": "2022", "conference": "European Conference on Computer Vision (ECCV)", "description": "We present a new lighting estimation and editing framework to generate high-dynamic-range (HDR) indoor panorama lighting from a single limited field-of-view (LFOV) image captured by low-dynamic-range (LDR) cameras. Existing lighting estimation methods either directly regress lighting representation parameters or decompose this problem into LFOV-to-panorama and LDR-to-HDR lighting generation sub-tasks. However, due to the partial observation, the high-dynamic-range lighting, and the intrinsic ambiguity of a scene, lighting estimation remains a challenging task. To tackle this problem, we propose a coupled dual-StyleGAN panorama synthesis network (StyleLight) that integrates LDR and HDR panorama synthesis into a unified framework. The LDR and HDR panorama synthesis share a similar generator but have separate discriminators. During inference, given an LDR LFOV image, we propose a focal \u2026", "total_citations": 9, "citation_graph": {"2022": 2, "2023": 7}}, {"title": "Versatile Multi-Modal Pre-Training for Human-Centric Perception", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:738O_yMBCRsC", "authors": ["Fangzhou Hong", "Liang Pan", "Zhongang Cai", "Ziwei Liu"], "publication_date": "2022", "conference": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Human-centric perception plays a vital role in vision and graphics. But their data annotations are prohibitively expensive. Therefore, it is desirable to have a versatile pre-train model that serves as a foundation for data-efficient downstream tasks transfer. To this end, we propose the Human-Centric Multi-Modal Contrastive Learning framework HCMoCo that leverages the multi-modal nature of human data (eg RGB, depth, 2D keypoints) for effective representation learning. The objective comes with two main challenges: dense pre-train for multi-modality data, efficient usage of sparse human priors. To tackle the challenges, we design the novel Dense Intra-sample Contrastive Learning and Sparse Structure-aware Contrastive Learning targets by hierarchically learning a modal-invariant latent space featured with continuous and ordinal feature distribution and structure-aware semantic consistency. HCMoCo provides pre-train for different modalities by combining heterogeneous datasets, which allows efficient usage of existing task-specific human data. Extensive experiments on four downstream tasks of different modalities demonstrate the effectiveness of HCMoCo, especially under data-efficient settings (7.16% and 12% improvement on DensePose Estimation and Human Parsing). Moreover, we demonstrate the versatility of HCMoCo by exploring cross-modality supervision and missing-modality inference, validating its strong ability in cross-modal association and reasoning.", "total_citations": 9, "citation_graph": {"2022": 3, "2023": 5}}, {"title": "DeeperForensics Challenge 2020 on Real-World Face Forgery Detection: Methods and Results", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:RGFaLdJalmkC", "authors": ["Liming Jiang", "Zhengkui Guo", "Wayne Wu", "Zhaoyang Liu", "Ziwei Liu", "Chen Change Loy", "Shuo Yang", "Yuanjun Xiong", "Wei Xia", "Baoying Chen", "Peiyu Zhuang", "Sili Li", "Shen Chen", "Taiping Yao", "Shouhong Ding", "Jilin Li", "Feiyue Huang", "Liujuan Cao", "Rongrong Ji", "Changlei Lu", "Ganchao Tan"], "publication_date": "2021/2/18", "journal": "arXiv preprint arXiv:2102.09471", "description": "This paper reports methods and results in the DeeperForensics Challenge 2020 on real-world face forgery detection. The challenge employs the DeeperForensics-1.0 dataset, one of the most extensive publicly available real-world face forgery detection datasets, with 60,000 videos constituted by a total of 17.6 million frames. The model evaluation is conducted online on a high-quality hidden test set with multiple sources and diverse distortions. A total of 115 participants registered for the competition, and 25 teams made valid submissions. We will summarize the winning solutions and present some discussions on potential research directions.", "total_citations": 9, "citation_graph": {"2021": 3, "2022": 6}}, {"title": "ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:2KloaMYe4IUC", "authors": ["Mingyuan Zhang", "Xinying Guo", "Liang Pan", "Zhongang Cai", "Fangzhou Hong", "Huirong Li", "Lei Yang", "Ziwei Liu"], "publication_date": "2023", "conference": "IEEE International Conference on Computer Vision (ICCV)", "description": "3D human motion generation is crucial for creative industry. Recent advances rely on generative models with domain knowledge for text-driven motion generation, leading to substantial progress in capturing common motions. However, the performance on more diverse motions remains unsatisfactory. In this work, we propose ReMoDiffuse, a diffusion-model-based motion generation framework that integrates a retrieval mechanism to refine the denoising process. ReMoDiffuse enhances the generalizability and diversity of text-driven motion generation with three key designs: 1) Hybrid Retrieval finds appropriate references from the database in terms of both semantic and kinematic similarities. 2) Semantic-Modulated Transformer selectively absorbs retrieval knowledge, adapting to the difference between retrieved samples and the target motion sequence. 3) Condition Mixture better utilizes the retrieval database during inference, overcoming the scale sensitivity in classifier-free guidance. Extensive experiments demonstrate that ReMoDiffuse outperforms state-of-the-art methods by balancing both text-motion consistency and motion quality, especially for more diverse motion generation.", "total_citations": 8, "citation_graph": {"2023": 8}}, {"title": "BiBench: Benchmarking and Analyzing Network Binarization", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:hkOj_22Ku90C", "authors": ["Haotong Qin", "Mingyuan Zhang", "Yifu Ding", "Aoyu Li", "Zhongang Cai", "Ziwei Liu", "Fisher Yu", "Xianglong Liu"], "publication_date": "2023", "conference": "International Conference on Machine Learning (ICML)", "description": "Network binarization emerges as one of the most promising compression approaches offering extraordinary computation and memory savings by minimizing the bit-width. However, recent research has shown that applying existing binarization algorithms to diverse tasks, architectures, and hardware in realistic scenarios is still not straightforward. Common challenges of binarization, such as accuracy degradation and efficiency limitation, suggest that its attributes are not fully understood. To close this gap, we present BiBench, a rigorously designed benchmark with in-depth analysis for network binarization. We first carefully scrutinize the requirements of binarization in the actual production and define evaluation tracks and metrics for a comprehensive and fair investigation. Then, we evaluate and analyze a series of milestone binarization algorithms that function at the operator level and with extensive influence. Our benchmark reveals that 1) the binarized operator has a crucial impact on the performance and deployability of binarized networks; 2) the accuracy of binarization varies significantly across different learning tasks and neural architectures; 3) binarization has demonstrated promising efficiency potential on edge devices despite the limited hardware support. The results and analysis also lead to a promising paradigm for accurate and efficient binarization. We believe that BiBench will contribute to the broader adoption of binarization and serve as a foundation for future research.", "total_citations": 8, "citation_graph": {"2023": 8}}, {"title": "Free Lunch for Surgical Video Understanding by Distilling Self-Supervisions", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lc45xlcAAAAJ&cstart=100&pagesize=100&citation_for_view=lc45xlcAAAAJ:geHnlv5EZngC", "authors": ["Xinpeng Ding", "Ziwei Liu", "Xiaomeng Li"], "publication_date": "2022", "conference": "Medical Image Computing and Computer Assisted Intervention (MICCAI)", "description": "Self-supervised learning has witnessed great progress in vision and NLP; recently, it also attracted much attention to various medical imaging modalities such as X-ray, CT, and MRI. Existing methods mostly focus on building new pretext self-supervision tasks such as reconstruction, orientation, and masking identification according to the properties of medical images. However, the publicly available self-supervision models are not fully exploited. In this paper, we present a powerful yet efficient self-supervision framework for surgical video understanding. Our key insight is to distill knowledge from publicly available models trained on large generic datasets (For example, the released models trained on ImageNet by MoCo v2: https://github.com/facebookresearch/moco) to facilitate the self-supervised learning of surgical videos. To this end, we first introduce a semantic-preserving training scheme to obtain our teacher \u2026", "total_citations": 8, "citation_graph": {"2022": 2, "2023": 6}}]}