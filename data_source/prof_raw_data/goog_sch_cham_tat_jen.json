{"goog_sch_url": "https://scholar.google.com/citations?hl=en&user=Lx3X7W0AAAAJ", "name": "Cham Tat Jen", "interests": ["Computer Vision"], "co_authors_url": [], "citation_table": {"columns": ["All", "Since 2018"], "Citations": [5616, 2527], "h-index": [40, 23], "i10-index": [76, 42]}, "citation_graph": {"1998": 23, "1999": 34, "2000": 60, "2001": 56, "2002": 91, "2003": 165, "2004": 150, "2005": 190, "2006": 172, "2007": 150, "2008": 143, "2009": 156, "2010": 192, "2011": 166, "2012": 156, "2013": 204, "2014": 184, "2015": 223, "2016": 208, "2017": 253, "2018": 201, "2019": 242, "2020": 350, "2021": 502, "2022": 611, "2023": 616}, "articles": [{"title": "A multiple hypothesis approach to figure tracking", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:u5HHmVD_uO8C", "authors": ["Tat-Jen Cham", "James M Rehg"], "publication_date": "1999", "conference": "Computer Vision and Pattern Recognition, 1999. IEEE Computer Society Conference on.", "description": "This paper describes a probabilistic multiple-hypothesis framework for tracking highly articulated objects. In this framework, the probability density of the tracker state is represented as a set of modes with piecewise Gaussians characterizing the neighborhood around these modes. The temporal evolution of the probability density is achieved through sampling from the prior distribution, followed by local optimization of the sample positions to obtain updated modes. This method of generating hypotheses from state-space search does not require the use of discrete features unlike classical multiple-hypothesis tracking. The parametric form of the model is suited for high dimensional state-spaces which cannot be efficiently modeled using non-parametric approaches. Results are shown for tracking Fred Astaire in a movie dance sequence.", "total_citations": 520, "citation_graph": {"1999": 5, "2000": 25, "2001": 26, "2002": 33, "2003": 54, "2004": 39, "2005": 54, "2006": 42, "2007": 33, "2008": 28, "2009": 29, "2010": 26, "2011": 17, "2012": 19, "2013": 16, "2014": 14, "2015": 11, "2016": 8, "2017": 9, "2018": 3, "2019": 9, "2020": 5, "2021": 2, "2022": 3, "2023": 1}}, {"title": "Pluralistic image completion", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:eJXPG6dFmWUC", "authors": ["Chuanxia Zheng", "Tat-Jen Cham", "Jianfei Cai"], "publication_date": "2019", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "description": "Most image completion methods produce only one result for each masked input, although there may be many reasonable possibilities. In this paper, we present an approach for pluralistic image completion-the task of generating multiple and diverse plausible solutions for image completion. A major challenge faced by learning-based approaches is that usually only one ground truth training instance per label. As such, sampling from conditional VAEs still leads to minimal diversity. To overcome this, we propose a novel and probabilistically principled framework with two parallel paths. One is a reconstructive path that utilizes the only one given ground truth to get prior distribution of missing parts and rebuild the original image from this distribution. The other is a generative path for which the conditional prior is coupled to the distribution obtained in the reconstructive path. Both are supported by GANs. We also introduce a new short+ long term attention layer that exploits distant relations among decoder and encoder features, improving appearance consistency. When tested on datasets with buildings (Paris), faces (CelebA-HQ), and natural images (ImageNet), our method not only generated higherquality completion results, but also with multiple and diverse plausible outputs.", "total_citations": 462, "citation_graph": {"2019": 12, "2020": 67, "2021": 110, "2022": 152, "2023": 118}}, {"title": "Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:kRWSkSYxWN8C", "authors": ["Yujun Cai", "Liuhao Ge", "Jun Liu", "Jianfei Cai", "Tat-Jen Cham", "Junsong Yuan", "Nadia Magnenat Thalmann"], "publication_date": "2019", "conference": "Proceedings of the IEEE/CVF international conference on computer vision", "description": "Despite great progress in 3D pose estimation from single-view images or videos, it remains a challenging task due to the substantial depth ambiguity and severe self-occlusions. Motivated by the effectiveness of incorporating spatial dependencies and temporal consistencies to alleviate these issues, we propose a novel graph-based method to tackle the problem of 3D human body and 3D hand pose estimation from a short sequence of 2D joint detections. Particularly, domain knowledge about the human hand (body) configurations is explicitly incorporated into the graph convolutional operations to meet the specific demand of the 3D pose estimation. Furthermore, we introduce a local-to-global network architecture, which is capable of learning multi-scale features for the graph-based representations. We evaluate the proposed method on challenging benchmark datasets for both 3D hand pose estimation and 3D body pose estimation. Experimental results show that our method achieves state-of-the-art performance on both tasks.", "total_citations": 372, "citation_graph": {"2019": 3, "2020": 49, "2021": 88, "2022": 107, "2023": 124}}, {"title": "A dynamic Bayesian network approach to figure tracking using learned dynamic models", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:u-x6o8ySG0sC", "authors": ["Vladimir Pavlovic", "James M Rehg", "Tat-Jen Cham", "Kevin P Murphy"], "publication_date": "1999", "conference": "Computer Vision, 1999. The Proceedings of the Seventh IEEE International Conference on", "description": "The human figure exhibits complex and rich dynamic behavior that is both nonlinear and time-varying. However most work on tracking and synthesizing figure motion has employed either simple, generic dynamic models or highly specific hand-tailored ones. Recently, a broad class of learning and inference algorithms for time-series models have been successfully cast in the framework of dynamic Bayesian networks (DBNs). This paper describes a novel DBN-based switching linear dynamic system (SLDS) model and presents its application to figure motion analysis. A key feature of our approach is an approximate Viterbi inference technique for overcoming the intractability of exact inference in mixed-state DBNs. We present experimental results for learning figure dynamics from video data and show promising initial results for tracking, interpolation, synthesis, and classification using learned models.", "total_citations": 370, "citation_graph": {"1999": 2, "2000": 10, "2001": 9, "2002": 12, "2003": 37, "2004": 20, "2005": 40, "2006": 39, "2007": 16, "2008": 22, "2009": 21, "2010": 20, "2011": 16, "2012": 7, "2013": 12, "2014": 15, "2015": 13, "2016": 15, "2017": 10, "2018": 7, "2019": 7, "2020": 5, "2021": 4, "2022": 3, "2023": 2}}, {"title": "Large-margin multi-modal deep learning for RGB-D object recognition", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:P5F9QuxV20EC", "authors": ["Anran Wang", "Jiwen Lu", "Jianfei Cai", "Tat-Jen Cham", "Gang Wang"], "publication_date": "2015/9/11", "journal": "IEEE Transactions on Multimedia", "description": "Most existing feature learning-based methods for RGB-D object recognition either combine RGB and depth data in an undifferentiated manner from the outset, or learn features from color and depth separately, which do not adequately exploit different characteristics of the two modalities or utilize the shared relationship between the modalities. In this paper, we propose a general CNN-based multi-modal learning framework for RGB-D object recognition. We first construct deep CNN layers for color and depth separately, which are then connected with a carefully designed multi-modal layer. This layer is designed to not only discover the most discriminative features for each modality, but is also able to harness the complementary relationship between the two modalities. The results of the multi-modal layer are back-propagated to update parameters of the CNN layers, and the multi-modal feature learning and the back \u2026", "total_citations": 185, "citation_graph": {"2015": 5, "2016": 11, "2017": 29, "2018": 37, "2019": 28, "2020": 27, "2021": 18, "2022": 17, "2023": 9}}, {"title": "Fast training and selection of haar features using statistics in boosting-based face detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:IjCSPb-OGe4C", "authors": ["Minh-Tri Pham", "Tat-Jen Cham"], "publication_date": "2007/10/14", "conference": "Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on", "description": "Training a cascade-based face detector using boosting and Haar features is computationally expensive, often requiring weeks on single CPU machines. The bottleneck is at training and selecting Haar features for a single weak classifier, currently in minutes. Traditional techniques for training a weak classifier usually run in 0(NT log N), with N examples (approximately 10,000), and T features (approximately 40,000). We present a method to train a weak classifier in time 0(Nd 2 + T), where d is the number of pixels of the probed image sub-window (usually from 350 to 500), by using only the statistics of the weighted input data. Experimental results revealed a significantly reduced training time of a weak classifier to the order of seconds. In particular, this method suffers very minimal immerse in training time with very large increases in members of Haar features, enjoying a significant gain in accuracy, even with reduced \u2026", "total_citations": 183, "citation_graph": {"2006": 1, "2007": 0, "2008": 10, "2009": 17, "2010": 16, "2011": 10, "2012": 9, "2013": 16, "2014": 16, "2015": 11, "2016": 9, "2017": 9, "2018": 10, "2019": 11, "2020": 13, "2021": 10, "2022": 7, "2023": 6}}, {"title": "T2net: Synthetic-to-realistic translation for solving single-image depth estimation tasks", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:B3FOqHPlNUQC", "authors": ["Chuanxia Zheng", "Tat-Jen Cham", "Jianfei Cai"], "publication_date": "2018", "conference": "Proceedings of the European conference on computer vision (ECCV)", "description": "Current methods for single-image depth estimation use training datasets with real image-depth pairs or stereo pairs, which are not easy to acquire. We propose a framework, trained on synthetic image-depth pairs and unpaired real images, that comprises an image translation network for enhancing realism of input images, followed by a depth prediction network. A key idea is having the first network act as a wide-spectrum input translator, taking in either synthetic or real images, and ideally producing minimally modified realistic images. This is done via a reconstruction loss when the training input is real, and a GAN loss when synthetic, removing the need for heuristic self-regularization. The second network is trained on a task loss for synthetic image-depth pairs, with an extra GAN loss to unify real and synthetic feature distributions. Importantly, the framework can be trained end-to-end, leading to good results, even surpassing early deep-learning methods that use real paired data.", "total_citations": 180, "citation_graph": {"2018": 3, "2019": 24, "2020": 39, "2021": 36, "2022": 40, "2023": 38}}, {"title": "Reconstruction of 3D figure motion from 2D correspondences", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:d1gkVwhDpl0C", "authors": ["David E DiFranco", "Tat-Jen Cham", "James M Rehg"], "publication_date": "2001", "conference": "Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on", "description": "We present a method for computing the 3D motion of articulated models from 2D correspondences. An iterative batch algorithm is proposed which estimates the maximum a posteriori trajectory based on 2D measurements subject to a number of constraints. These include (i) kinematic constraints based on a 3D kinematic model, (ii) joint angle limits, (iii) dynamic smoothing, and (iv) 3D key frames which can be specified by the user. The framework handles any variation in the number of constraints as well as partial or missing data. This method is shown to obtain favorable reconstruction results on a number of complex human motion sequences.", "total_citations": 143, "citation_graph": {"2000": 1, "2001": 3, "2002": 12, "2003": 14, "2004": 18, "2005": 15, "2006": 13, "2007": 11, "2008": 6, "2009": 10, "2010": 9, "2011": 3, "2012": 2, "2013": 2, "2014": 5, "2015": 6, "2016": 0, "2017": 3, "2018": 1, "2019": 0, "2020": 1, "2021": 2, "2022": 2}}, {"title": "Wireless multi-user multi-projector presentation system", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:kNdYIx-mwKoC", "publication_date": "2006/2/28", "description": "Media slides are often employed in conference sessions, meetings, lectures, and other interactive forums. The proliferation of laptops and handheld computers allows a speaker to present directly from the laptop by connecting to the projector at the conference site. Physically connecting and disconnecting each presenter's laptop to the projection apparatus, however, can be a clumsy and disruptive process, particularly since the presenters may be seated at various locations around the room. A wireless interface between a presentation server and a laptop in a multi-user multi-projector presentation system allows a media sequence from each media source to be displayed on a common display via the presentation server and the wireless interface. Presenters need not run or swap cables or other physical connections to switch media sources to the common display. The interface requires no software modification to \u2026", "total_citations": 132, "citation_graph": {"2005": 5, "2006": 1, "2007": 3, "2008": 4, "2009": 5, "2010": 7, "2011": 7, "2012": 13, "2013": 9, "2014": 5, "2015": 8, "2016": 19, "2017": 8, "2018": 7, "2019": 4, "2020": 7, "2021": 11, "2022": 3, "2023": 3}}, {"title": "Method for efficiently tracking object models in video sequences via dynamic ordering of features", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:KlAtU1dfN6UC", "publication_date": "2004/9/21", "description": "An object model has a plurality of features and is described by a model state. An unregistered feature of the object model, and an available frame from a sequence of images are selected to minimize a cost function of a subsequent search for a match of the selected model feature to the image in the selected frame. Upon a match, the feature is registered in that frame. The model state is then updated for each available frame. The steps of selecting, searching and updating are repeated. A video storage module may contain only one frame corresponding to a single time instance, in which case the framework used is based on integrated sequential feature selection. Alternatively, the video store may contain the entire video sequence, in which case feature selection is performed across all video frames for maximum tracking efficiency. Finally, the video store may contain a small number of previous frames plus the current \u2026", "total_citations": 123, "citation_graph": {"2003": 2, "2004": 0, "2005": 1, "2006": 2, "2007": 2, "2008": 3, "2009": 1, "2010": 5, "2011": 4, "2012": 4, "2013": 12, "2014": 13, "2015": 10, "2016": 10, "2017": 13, "2018": 9, "2019": 12, "2020": 8, "2021": 5, "2022": 4, "2023": 3}}, {"title": "Learning progressive joint propagation for human motion prediction", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:5awf1xo2G04C", "authors": ["Yujun Cai", "Lin Huang", "Yiwei Wang", "Tat-Jen Cham", "Jianfei Cai", "Junsong Yuan", "Jun Liu", "Xu Yang", "Yiheng Zhu", "Xiaohui Shen", "Ding Liu", "Jing Liu", "Nadia Magnenat Thalmann"], "publication_date": "2020", "conference": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part VII 16", "description": "Despite the great progress in human motion prediction, it remains a challenging task due to the complicated structural dynamics of human behaviors. In this paper, we address this problem in three aspects. First, to capture the long-range spatial correlations and temporal dependencies, we apply a transformer-based architecture with the global attention mechanism. Specifically, we feed the network with the sequential joints encoded with the temporal information for spatial and temporal explorations. Second, to further exploit the inherent kinematic chains for better 3D structures, we apply a progressive-decoding strategy, which performs in a central-to-peripheral extension according to the structural connectivity. Last, in order to incorporate a general motion space for high-quality prediction, we build a memory-based dictionary, which aims to preserve the global motion patterns in training data to guide the \u2026", "total_citations": 110, "citation_graph": {"2020": 4, "2021": 23, "2022": 35, "2023": 48}}, {"title": "Fast polygonal integration and its application in extending haar-like features to improve object detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:qUcmZB5y_30C", "authors": ["Minh-Tri Pham", "Yang Gao", "Viet-Dung D Hoang", "Tat-Jen Cham"], "publication_date": "2010/6/13", "conference": "2010 IEEE computer society conference on computer vision and pattern recognition", "description": "The integral image is typically used for fast integrating a function over a rectangular region in an image. We propose a method that extends the integral image to do fast integration over the interior of any polygon that is not necessarily rectilinear. The integration time of the method is fast, independent of the image resolution, and only linear to the polygon's number of vertices. We apply the method to Viola and Jones' object detection framework, in which we propose to improve classical Haar-like features with polygonal Haar-like features. We show that the extended feature set improves object detection's performance. The experiments are conducted in three domains: frontal face detection, fixed-pose hand detection, and rock detection for Mars' surface terrain assessment.", "total_citations": 110, "citation_graph": {"2010": 2, "2011": 4, "2012": 4, "2013": 12, "2014": 12, "2015": 2, "2016": 6, "2017": 10, "2018": 6, "2019": 10, "2020": 14, "2021": 10, "2022": 7, "2023": 5}}, {"title": "Mmss: Multi-modal sharable and specific feature learning for rgb-d object recognition", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:1sJd4Hv_s6UC", "authors": ["Anran Wang", "Jianfei Cai", "Jiwen Lu", "Tat-Jen Cham"], "publication_date": "2015", "conference": "Proceedings of the IEEE international conference on computer vision", "description": "Most of the feature-learning methods for RGB-D object recognition either learn features from color and depth modalities separately, or simply treat RGB-D as undifferentiated four-channel data, which cannot adequately exploit the relationship between different modalities. Motivated by the intuition that different modalities should contain not only some modal-specific patterns but also some shared common patterns, we propose a multi-modal feature learning framework for RGB-D object recognition. We first construct deep CNN layers for color and depth separately, and then connect them with our carefully designed multi-modal layers, which fuse color and depth information by enforcing a common part to be shared by features of different modalities. In this way, we obtain features reflecting shared properties as well as modal-specific properties in different modalities. The information of the multi-modal learning frameworks is back-propagated to the early CNN layers. Experimental results show that our proposed multi-modal feature learning method outperforms state-of-the-art approaches on two widely used RGB-D object benchmark datasets.", "total_citations": 109, "citation_graph": {"2015": 3, "2016": 10, "2017": 18, "2018": 10, "2019": 18, "2020": 14, "2021": 18, "2022": 10, "2023": 6}}, {"title": "Dynamic shadow elimination for multi-projector displays", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:9yKSN-GCB0IC", "authors": ["Rahul Sukthankar", "Tat-Jen Cham", "Gita Sukthankar"], "publication_date": "2001/12/8", "conference": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001", "description": "A major problem with interactive displays based on front-projection is that users cast undesirable shadows on the display surface. This situation is only partially addressed by mounting a single projector at an extreme angle and pre-warping the projected image to undo keystoning distortions. This paper demonstrates that shadows can be muted by redundantly illuminating the display surface using multiple projectors, all mounted at different locations. However, this technique alone does not eliminate shadows: multiple projectors create multiple dark regions on the surface (penumbral occlusions). We solve the problem by using cameras to automatically identify occlusions as they occur and dynamically adjust each projector's output so that additional light is projected onto each partially-occluded patch. The system is self-calibrating: relevant homographies relating projectors, cameras and the display surface are \u2026", "total_citations": 107, "citation_graph": {"2001": 1, "2002": 5, "2003": 9, "2004": 5, "2005": 18, "2006": 9, "2007": 6, "2008": 6, "2009": 4, "2010": 6, "2011": 3, "2012": 1, "2013": 0, "2014": 2, "2015": 1, "2016": 5, "2017": 6, "2018": 1, "2019": 5, "2020": 0, "2021": 5, "2022": 2, "2023": 2}}, {"title": "Estimating camera pose from a single urban ground-view omnidirectional image and a 2D building outline map", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:9ZlFYXVOiuMC", "authors": ["Tat-Jen Cham", "Arridhana Ciptadi", "Wei-Chian Tan", "Minh-Tri Pham", "Liang-Tien Chia"], "publication_date": "2010/6/13", "conference": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "description": "A framework is presented for estimating the pose of a camera based on images extracted from a single omnidirectional image of an urban scene, given a 2D map with building outlines with no 3D geometric information nor appearance data. The framework attempts to identify vertical corner edges of buildings in the query image, which we term VCLH, as well as the neighboring plane normals, through vanishing point analysis. A bottom-up process further groups VCLH into elemental planes and subsequently into 3D structural fragments modulo a similarity transformation. A geometric hashing lookup allows us to rapidly establish multiple candidate correspondences between the structural fragments and the 2D map building contours. A voting-based camera pose estimation method is then employed to recover the correspondences admitting a camera pose solution with high consensus. In a dataset that is even \u2026", "total_citations": 99, "citation_graph": {"2011": 8, "2012": 10, "2013": 10, "2014": 8, "2015": 16, "2016": 6, "2017": 13, "2018": 9, "2019": 8, "2020": 2, "2021": 3, "2022": 4, "2023": 1}}, {"title": "Image pre-conditioning for out-of-focus projector blur", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:UebtZRa9Y70C", "authors": ["Michael S Brown", "Peng Song", "Tat-Jen Cham"], "publication_date": "2006/6/17", "conference": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)", "description": "We present a technique to reduce image blur caused by out-of-focus regions in projected imagery. Unlike traditional restoration algorithms that operate on a blurred image to recover the original, the nature of our problem requires that the correction be applied to the original image before blurring. To accomplish this, a camera is used to estimate a series of spatially varying point-spread-functions (PSF) across the projector\u2019s image. These discrete PSFs are then used to guide a pre-processing algorithm based on Wiener filtering to condition the image before projection. Results show that using this technique can help ameliorate the visual effects from out-of-focus projector blur.", "total_citations": 86, "citation_graph": {"2006": 3, "2007": 5, "2008": 5, "2009": 3, "2010": 4, "2011": 5, "2012": 4, "2013": 9, "2014": 8, "2015": 6, "2016": 8, "2017": 3, "2018": 5, "2019": 4, "2020": 5, "2021": 4, "2022": 2, "2023": 3}}, {"title": "Online learning asymmetric boosted classifiers for object detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:Y0pCki6q_DkC", "authors": ["Minh-Tri Pham", "Tat-Jen Cham"], "publication_date": "2007/6/17", "conference": "2007 IEEE Conference on Computer Vision and Pattern Recognition", "description": "We present an integrated framework for learning asymmetric boosted classifiers and online learning to address the problem of online learning asymmetric boosted classifiers, which is applicable to object detection problems. In particular, our method seeks to balance the skewness of the labels presented to the weak classifiers, allowing them to be trained more equally. In online learning, we introduce an extra constraint when propagating the weights of the data points from one weak classifier to another, allowing the algorithm to converge faster. In compared with the Online Boosting algorithm recently applied to object detection problems, we observed about 0-10% increase in accuracy, and about 5-30% gain in learning speed.", "total_citations": 82, "citation_graph": {"2007": 2, "2008": 6, "2009": 12, "2010": 10, "2011": 7, "2012": 8, "2013": 8, "2014": 4, "2015": 6, "2016": 7, "2017": 2, "2018": 1, "2019": 1, "2020": 3, "2021": 2, "2022": 2}}, {"title": "Modality and component aware feature fusion for rgb-d scene classification", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:UxriW0iASnsC", "authors": ["Anran Wang", "Jianfei Cai", "Jiwen Lu", "Tat-Jen Cham"], "publication_date": "2016", "conference": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "description": "While convolutional neural networks (CNN) have been excellent for object recognition, the greater spatial variability in scene images typically meant that the standard full-image CNN features are suboptimal for scene classification. In this paper, we investigate a framework allowing greater spatial flexibility, in which the Fisher vector (FV) encoded distribution of local CNN features, obtained from a multitude of region proposals per image, is considered instead. The CNN features are computed from an augmented pixel-wise representation comprising multiple modalities of RGB, HHA and surface normals, as extracted from RGB-D data. More significantly, we make two postulates:(1) component sparsity---that only a small variety of region proposals and their corresponding FV GMM components contribute to scene discriminability, and (2) modal non-sparsity---within these discriminative components, all modalities have important contribution. In our framework, these are implemented through regularization terms applying group lasso to GMM components and exclusive group lasso across modalities. By learning and combining regressors for both proposal-based FV features and global CNN features, we were able to achieve state-of-the-art scene classification performance on the SUNRGBD Dataset and NYU Depth Dataset V2.", "total_citations": 81, "citation_graph": {"2016": 1, "2017": 10, "2018": 13, "2019": 14, "2020": 11, "2021": 17, "2022": 5, "2023": 9}}, {"title": "Shadow elimination and occluder light suppression for multi-projector displays", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:2osOgNQ5qMEC", "authors": ["Tat-Jen Cham", "James M Rehg", "Rahul Sukthankar", "Gita Sukthankar"], "publication_date": "2003/6/18", "conference": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings.", "description": "Two related problems of front projection displays, which occur when users obscure a projector, are: (i) undesirable shadows cast on the display by the users, and (ii) projected light falling on and distracting the users. This paper provides a computational framework for solving these two problems based on multiple overlapping projectors and cameras. The overlapping projectors are automatically aligned to display the same dekeystoned image. The system detects when and where shadows are cast by occluders and is able to determine the pixels, which are occluded in different projectors. Through a feedback control loop, the contributions of unoccluded pixels from other projectors are boosted in the shadowed regions, thereby eliminating the shadows. In addition, pixels, which are being occluded, are blanked, thereby preventing the projected light from falling on a user when they occlude the display. This can be \u2026", "total_citations": 79, "citation_graph": {"2002": 3, "2003": 3, "2004": 8, "2005": 17, "2006": 9, "2007": 9, "2008": 5, "2009": 1, "2010": 3, "2011": 3, "2012": 1, "2013": 0, "2014": 3, "2015": 1, "2016": 0, "2017": 4, "2018": 0, "2019": 1, "2020": 0, "2021": 1}}, {"title": "The spatially-correlative loss for various image translation tasks", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:N5tVd3kTz84C", "authors": ["Chuanxia Zheng", "Tat-Jen Cham", "Jianfei Cai"], "publication_date": "2021", "conference": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition", "description": "We propose a novel spatially-correlative loss that is simple, efficient, and yet effective for preserving scene structure consistency while supporting large appearance changes during unpaired image-to-image (I2I) translation. Previous methods attempt this by using pixel-level cycle-consistency or feature-level matching losses, but the domain-specific nature of these losses hinder translation across large domain gaps. To address this, we exploit the spatial patterns of self-similarity as a means of defining scene structure. Our spatially-correlative loss is geared towards only capturing spatial relationships within an image rather than domain appearance. We also introduce a new self-supervised learning method to explicitly learn spatially-correlative maps for each specific translation task. We show distinct improvement over baseline models in all three modes of unpaired I2I translation: single-modal, multi-modal, and even single-image translation. This new loss can easily be integrated into existing network architectures and thus allows wide applicability.", "total_citations": 78, "citation_graph": {"2021": 4, "2022": 29, "2023": 45}}, {"title": "Automated B-spline curve representation incorporating MDL and error-minimizing control point insertion strategies", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:qjMakFHDy7sC", "authors": ["T-J Cham", "Roberto Cipolla"], "publication_date": "1999/1", "journal": "Pattern Analysis and Machine Intelligence, IEEE Transactions on", "description": "The main issues of developing an automatic and reliable scheme for spline-fitting are discussed and addressed in this paper, which are not fully covered in previous papers or algorithms. The proposed method incorporates B-spline active contours, the minimum description length (MDL) principle, and a novel control point insertion strategy based on maximizing the potential for energy-reduction maximization (PERM). A comparison of test results shows that it outperforms one of the better existing methods.", "total_citations": 73, "citation_graph": {"1999": 1, "2000": 7, "2001": 5, "2002": 2, "2003": 5, "2004": 7, "2005": 5, "2006": 2, "2007": 7, "2008": 4, "2009": 7, "2010": 3, "2011": 1, "2012": 3, "2013": 1, "2014": 2, "2015": 2, "2016": 2, "2017": 2, "2018": 0, "2019": 0, "2020": 1, "2021": 1}}, {"title": "Face and human gait recognition using image-to-class distance", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:4TOpqqG69KYC", "authors": ["Yi Huang", "Dong Xu", "Tat-Jen Cham"], "publication_date": "2010/3", "journal": "Circuits and Systems for Video Technology, IEEE Transactions on", "description": "We propose a new distance measure for face recognition and human gait recognition. Each probe image (a face image or an average human silhouette image) is represented as a set of local features uniformly sampled over a grid with fixed spacing, and each gallery image is represented as a set of local features sampled at each pixel. We formulate an integer programming problem to compute the distance (referred to as the image-to-class distance) from one probe image to all the gallery images belonging to a certain class, in which any feature of the probe image can be matched to only one feature from one of the gallery images. Considering computational efficiency as well as the fact that face images or average human silhouette images are roughly aligned in the preprocessing step, we also enforce a spatial neighborhood constraint by only allowing neighboring features that are within a given spatial distance to \u2026", "total_citations": 69, "citation_graph": {"2009": 1, "2010": 3, "2011": 6, "2012": 7, "2013": 12, "2014": 8, "2015": 5, "2016": 7, "2017": 8, "2018": 2, "2019": 4, "2020": 0, "2021": 2, "2022": 1, "2023": 1}}, {"title": "Multi-modal unsupervised feature learning for RGB-D scene labeling", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:cFHS6HbyZ2cC", "authors": ["Anran Wang", "Jiwen Lu", "Gang Wang", "Jianfei Cai", "Tat-Jen Cham"], "publication_date": "2014", "conference": "Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13", "description": "Most of the existing approaches for RGB-D indoor scene labeling employ hand-crafted features for each modality independently and combine them in a heuristic manner. There has been some attempt on directly learning features from raw RGB-D data, but the performance is not satisfactory. In this paper, we adapt the unsupervised feature learning technique for RGB-D labeling as a multi-modality learning problem. Our learning framework performs feature learning and feature encoding simultaneously which significantly boosts the performance. By stacking basic learning structure, higher-level features are derived and combined with lower-level features for better representing RGB-D data. Experimental results on the benchmark NYU depth dataset show that our method achieves competitive performance, compared with state-of-the-art.", "total_citations": 68, "citation_graph": {"2014": 2, "2015": 17, "2016": 12, "2017": 15, "2018": 3, "2019": 5, "2020": 3, "2021": 3, "2022": 2, "2023": 1}}, {"title": "Symmetry detection through local skewed symmetries", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:UeHWp8X0CEIC", "authors": ["Tat-Jen Cham", "Roberto Cipolla"], "publication_date": "1995/6/1", "journal": "Image and Vision Computing", "description": "We explore how global symmetry can be detected prior to segmentation and under noise and occlusion. The definition of local symmetries is extended to affine geometries by considering the tangents and curvatures of local structures, and a quantitative measure of local symmetry known as symmetricity is introduced, which is based on Mahalanobis distances from the tangent-curvature states of local structures to the local skewed symmetry state-subspace. These symmetricity values, together with the associated local axes of symmetry, are spatially related in the local skewed symmetry field (LSSF). In the implementation, a fast, local symmetry detection algorithm allows initial hypotheses for the symmetry axis to be generated through the use of a modified Hough transform. This is then improved upon by maximizing a global symmetry measure based on accumulated local support in the LSSF-a straight active contour \u2026", "total_citations": 68, "citation_graph": {"1994": 2, "1995": 0, "1996": 5, "1997": 2, "1998": 7, "1999": 7, "2000": 3, "2001": 3, "2002": 2, "2003": 3, "2004": 6, "2005": 2, "2006": 3, "2007": 2, "2008": 0, "2009": 4, "2010": 1, "2011": 2, "2012": 0, "2013": 5, "2014": 0, "2015": 1, "2016": 4, "2017": 3}}, {"title": "Calibrating scalable multi-projector displays using camera homography trees", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:zYLM7Y9cAGgC", "authors": ["Han Chen", "Rahul Sukthankar", "Grant Wallace", "Tat-Jen Cham"], "publication_date": "2001", "journal": "Computer Vision and Pattern Recognition", "description": "We present a practical vision-based calibration system for large format multi-projector displays. A spanning tree of homographies, automatically constructed from several camera images, accurately registers arbitrarily-mounted projectors to a global reference frame. Experiments on the 18\u2019\u00d7 8\u2019Princeton Display Wall (a 24 projector array with 6000\u00d7 3000 resolution) demonstrate that our algorithm achieves sub-pixel accuracy even on large display surfaces. A direct comparison with the previous best algorithm shows that our technique is significantly more accurate, requires far fewer camera images, and runs faster by an order of magnitude.", "total_citations": 64, "citation_graph": {"2001": 1, "2002": 7, "2003": 7, "2004": 7, "2005": 4, "2006": 8, "2007": 1, "2008": 5, "2009": 4, "2010": 2, "2011": 3, "2012": 2, "2013": 3, "2014": 4, "2015": 3, "2016": 1, "2017": 0, "2018": 0, "2019": 1, "2020": 0, "2021": 1}}, {"title": "Method for object registration via selection of models with dynamically ordered features", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:Zph67rFs4hoC", "publication_date": "2003/7/22", "description": "A plurality of object models, where each object model comprises a plurality of features and is described by a model state, are registered in at least one image a subset of the object models is selected. Different object models have different sets of features, which may or may not overlap. A feature of each selected object model is registered in one of the images, and the model state for each selected object model is updated accordingly. The model states of some or all of the object models are then updated according to a set of constraints. These steps are repeated until one or more object models are registered. At the beginning of each registration cycle, a cost function of a subsequent search is determined for each unregistered feature of each object model. An unregistered feature of each object model is then selected such that the cost function is minimized. Object models to which the selected features belong are then \u2026", "total_citations": 61, "citation_graph": {"2004": 1, "2005": 2, "2006": 1, "2007": 1, "2008": 3, "2009": 3, "2010": 0, "2011": 4, "2012": 2, "2013": 3, "2014": 7, "2015": 9, "2016": 1, "2017": 5, "2018": 4, "2019": 3, "2020": 3, "2021": 4, "2022": 4, "2023": 1}}, {"title": "Agilegan: stylizing portraits by inversion-consistent transfer learning", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:hkOj_22Ku90C", "authors": ["Guoxian Song", "Linjie Luo", "Jing Liu", "Wan-Chun Ma", "Chunpong Lai", "Chuanxia Zheng", "Tat-Jen Cham"], "publication_date": "2021/7/19", "journal": "ACM Transactions on Graphics (TOG)", "description": "Portraiture as an art form has evolved from realistic depiction into a plethora of creative styles. While substantial progress has been made in automated stylization, generating high quality stylistic portraits is still a challenge, and even the recent popular Toonify suffers from several artifacts when used on real input images. Such StyleGAN-based methods have focused on finding the best latent inversion mapping for reconstructing input images; however, our key insight is that this does not lead to good generalization to different portrait styles. Hence we propose AgileGAN, a framework that can generate high quality stylistic portraits via inversion-consistent transfer learning. We introduce a novel hierarchical variational autoencoder to ensure the inverse mapped distribution conforms to the original latent Gaussian distribution, while augmenting the original space to a multi-resolution latent space so as to better encode \u2026", "total_citations": 60, "citation_graph": {"2021": 5, "2022": 28, "2023": 26}}, {"title": "A statistical framework for long-range feature matching in uncalibrated image mosaicing", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:Tyk-4Ss8FVUC", "authors": ["Tat-Jen Cham", "Roberto Cipolla"], "publication_date": "1998/6/25", "conference": "Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 98CB36231)", "description": "The problem considered is that of estimating the projective transformation between two images in situations where the image motion is large and feature matching is not aided by a proximity heuristic. The overall algorithm designed is based on a multiresolution, multihypothesis scheme, and similarities between tracking and matching through multiple resolution levels are exploited. Two major tools are developed in this paper: (i) a Bayesian framework for incorporating similarity measures of feature correspondences in regression to specify the different levels of confidence in the correspondences; and (ii) a Bayesian version of RANSAC, which is able to utilise prior estimates and matching probabilities. The algorithm is tested on a number of real images with large image motion and promising results were obtained.", "total_citations": 59, "citation_graph": {"1998": 2, "1999": 7, "2000": 3, "2001": 1, "2002": 2, "2003": 5, "2004": 4, "2005": 1, "2006": 2, "2007": 4, "2008": 0, "2009": 1, "2010": 7, "2011": 5, "2012": 2, "2013": 3, "2014": 1, "2015": 2, "2016": 2, "2017": 0, "2018": 0, "2019": 2, "2020": 0, "2021": 0, "2022": 2}}, {"title": "Detection with multi-exit asymmetric boosting", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:0EnyYjriUFMC", "authors": ["Minh-Tri Pham", "Viet-Dung D Hoang", "Tat-Jen Cham"], "publication_date": "2008/6/23", "conference": "2008 IEEE Conference on Computer Vision and Pattern Recognition", "description": "We introduce a generalized representation for a boosted classifier with multiple exit nodes, and propose a method to training which combines the idea of propagating scores across boosted classifiers [14, 17] and the use of asymmetric goals [13]. A means for determining the ideal constant asymmetric goal is provided, which is theoretically justified under a conservative bound on the ROC operating point target and empirically near-optimal under the exact bound. Moreover, our method automatically minimizes the number of weak classifiers, avoiding the need to retrain a boosted classifier multiple times for empirical best performance as in conventional methods. Experimental results shows significant reduction in training time and number of weak classifiers, as well as better accuracy, compared to conventional cascades and multi-exit boosted classifiers.", "total_citations": 58, "citation_graph": {"2009": 3, "2010": 9, "2011": 7, "2012": 6, "2013": 8, "2014": 8, "2015": 5, "2016": 6, "2017": 2, "2018": 1, "2019": 0, "2020": 1, "2021": 1}}, {"title": "Advances in Multimedia Modeling (Paperback): 13th International Multimedia Modeling Conference, MMM 2007, Singapore, January 9-12, 2007, Proceedings, Part I Book.(Series \u2026", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:ns9cj8rnVeAC", "authors": ["Tat-Jen CHAM"], "publication_date": "2006/12", "total_citations": 55, "citation_graph": {"2007": 1, "2008": 2, "2009": 1, "2010": 5, "2011": 3, "2012": 3, "2013": 3, "2014": 3, "2015": 6, "2016": 3, "2017": 4, "2018": 4, "2019": 1, "2020": 2, "2021": 7, "2022": 5, "2023": 2}}, {"title": "Bridging global context interactions for high-fidelity image completion", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:NJ774b8OgUMC", "authors": ["Chuanxia Zheng", "Tat-Jen Cham", "Jianfei Cai", "Dinh Phung"], "publication_date": "2022", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "description": "Bridging global context interactions correctly is important for high-fidelity image completion with large masks. Previous methods attempting this via deep or large receptive field (RF) convolutions cannot escape from the dominance of nearby interactions, which may be inferior. In this paper, we propose to treat image completion as a directionless sequence-to-sequence prediction task, and deploy a transformer to directly capture long-range dependence. Crucially, we employ a restrictive CNN with small and non-overlapping RF for weighted token representation, which allows the transformer to explicitly model the long-range visible context relations with equal importance in all layers, without implicitly confounding neighboring tokens when larger RFs are used. To improve appearance consistency between visible and generated regions, a novel attention-aware layer (AAL) is introduced to better exploit distantly related high-frequency features. Overall, extensive experiments demonstrate superior performance compared to state-of-the-art methods on several datasets. Code is available at https://github. com/lyndonzheng/TFill.", "total_citations": 53, "citation_graph": {"2021": 4, "2022": 19, "2023": 29}}, {"title": "Method for efficiently registering object models in images via dynamic ordering of features", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:MXK_kJrjxJIC", "publication_date": "2003/9/9", "description": "An object model, having a plurality of features and described by a model state, is registered in an image. Unregistered features of the object model are dynamically selected such that the cost function of each feature search is minimized. A search is performed for a match of the selected model feature to the image, or to features within the image, to register the feature, and the model state is updated accordingly. These steps are repeated until all features have been registered. The search is performed in a region of high probability of a match. The cost function for a feature is based on the feature's basin of attraction, and in particular can be based on the complexity of the search process at each basin of attraction. A search region is based on a projected state probability distribution. In particular, the cost function is based on the \u201cmatching ambiguity,\u201d or the number of search operations required to find a true match with \u2026", "total_citations": 52, "citation_graph": {"2004": 1, "2005": 0, "2006": 1, "2007": 10, "2008": 0, "2009": 1, "2010": 7, "2011": 7, "2012": 4, "2013": 3, "2014": 0, "2015": 2, "2016": 1, "2017": 2, "2018": 2, "2019": 4, "2020": 4, "2021": 2, "2022": 1}}, {"title": "Near duplicate image identification with patially aligned pyramid matching", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:LkGwnXOMwfcC", "authors": ["Dong Xu", "Tat-Jen Cham", "Shuicheng Yan", "Shih-Fu Chang"], "publication_date": "2008/6/23", "conference": "2008 IEEE Conference on Computer Vision and Pattern Recognition", "description": "A new framework, termed spatially aligned pyramid matching, is proposed for near duplicate image identification. The proposed method robustly handles spatial shifts as well as scale changes. Images are divided into both overlapped and non-overlapped blocks over multiple levels. In the first matching stage, pairwise distances between blocks from the examined image pair are computed using SIFT features and Earth Moverpsilas distance (EMD). In the second stage, multiple alignment hypotheses that consider piecewise spatial shifts and scale variation are postulated and resolved using integer-flow EMD. Two application scenarios are addressed - retrieval ranking and binary classification. For retrieval ranking, a pyramid-based scheme is constructed to fuse matching results from different partition levels. For binary classification, a novel generalized neighborhood component analysis method is formulated that \u2026", "total_citations": 51, "citation_graph": {"2009": 6, "2010": 7, "2011": 11, "2012": 5, "2013": 9, "2014": 3, "2015": 3, "2016": 1, "2017": 1, "2018": 1, "2019": 1, "2020": 0, "2021": 2}}, {"title": "Multiple mode probability density estimation with application to sequential markovian decision processes", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:_FxGoFyzp5QC", "publication_date": "2001/5/1", "description": "The invention recognizes that a probability density function for fitting a model to a complex set of data often has multiple modes, each mode representing a reasonably probable state of the model when compared with the data. Particularly, an image may require a complex sequence of analyses in order for a pattern embedded in the image to be ascertained. Computation of the probability density function of the model state involves two main stages:(1) state prediction, in which the prior probability distribution is generated from information known prior to the availability of the data, and (2) state update, in which the posterior probability distribution is formed by updating the prior distribution with information obtained from observing the data. In particular this information obtained purely from data observations can also be expressed as a probability density function, known as the likelihood function. The likelihood function is \u2026", "total_citations": 50, "citation_graph": {"2000": 1, "2001": 0, "2002": 2, "2003": 3, "2004": 7, "2005": 3, "2006": 4, "2007": 4, "2008": 1, "2009": 1, "2010": 4, "2011": 0, "2012": 4, "2013": 1, "2014": 3, "2015": 4, "2016": 1, "2017": 3, "2018": 3, "2019": 0, "2020": 0, "2021": 1}}, {"title": "Stereo coupled active contours", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:YsMSGLbcyi4C", "authors": ["Tat-Jen Cham", "Roberto Cipolla"], "publication_date": "1997/6/17", "conference": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "description": "We consider how tracking in stereo may be enhanced by coupling pairs of active contours in different views via affine epipolar geometry and various subsets of planar affine transformations, as well as by implementing temporal constraints imposed by curve rigidity. 3D curve tracking is achieved using a submanifold model, where it is shown how the coupling mechanisms can be decomposed to cater for fired and variable epipolar geometries. In the case of tracking planar curves, the canonical frame model is developed such that the various geometrical constraints needed in different situations may be efficiently selected. The results show that coupled active contours add consistency and robustness to tracking in stereo.", "total_citations": 49, "citation_graph": {"1997": 1, "1998": 0, "1999": 2, "2000": 2, "2001": 1, "2002": 2, "2003": 6, "2004": 3, "2005": 3, "2006": 2, "2007": 6, "2008": 4, "2009": 2, "2010": 0, "2011": 1, "2012": 2, "2013": 2, "2014": 0, "2015": 2, "2016": 3, "2017": 2, "2018": 0, "2019": 0, "2020": 1, "2021": 1, "2022": 0, "2023": 1}}, {"title": "Near duplicate identification with spatially aligned pyramid matching", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:QIV2ME_5wuYC", "authors": ["Dong Xu", "Tat Jen Cham", "Shuicheng Yan", "Lixin Duan", "Shih-Fu Chang"], "publication_date": "2010/5/27", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "description": "A new framework, termed spatially aligned pyramid matching, is proposed for near duplicate image identification. The proposed method robustly handles spatial shifts as well as scale changes, and is extensible for video data. Images are divided into both overlapped and non-overlapped blocks over multiple levels. In the first matching stage, pairwise distances between blocks from the examined image pair are computed using earth mover's distance (EMD) or the visual word with distance based method with scale-invariant feature transform (SIFT) features. In the second stage, multiple alignment hypotheses that consider piecewise spatial shifts and scale variation are postulated and resolved using integer-flow EMD. Moreover, to compute the distances between two videos, we conduct the third step matching (i.e., temporal matching) after spatial matching. Two application scenarios are addressed\u2014near \u2026", "total_citations": 48, "citation_graph": {"2010": 1, "2011": 4, "2012": 5, "2013": 11, "2014": 6, "2015": 5, "2016": 3, "2017": 1, "2018": 1, "2019": 3, "2020": 0, "2021": 4, "2022": 2, "2023": 1}}, {"title": "Sample refinement method of multiple mode probability density estimation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:roLk4NBRz8UC", "publication_date": "2002/3/5", "description": "The invention recognizes that a probability density function for fitting a model to a complex set of data often has multiple modes, each mode representing a reasonably probable state of the model when compared with the data. Particularly, sequential data such as are collected from detection of moving objects in three dimensional space are placed into data frames. Also, a single frame of data may require analysis by a sequence of analysis operations. Computation of the probability density function of the model state involves two main stages:(1) state prediction, in which the prior probability distribution is generated from information known prior to the availability of the data, and (2) state update, in which the posterior probability distribution is formed by updating the prior distribution with information obtained from observing the data. In particular this information obtained purely from data observations can also be \u2026", "total_citations": 44, "citation_graph": {"2002": 1, "2003": 2, "2004": 4, "2005": 3, "2006": 3, "2007": 2, "2008": 1, "2009": 0, "2010": 5, "2011": 0, "2012": 0, "2013": 5, "2014": 3, "2015": 3, "2016": 1, "2017": 4, "2018": 2, "2019": 0, "2020": 1, "2021": 2, "2022": 1}}, {"title": "Multiple mode probability density estimation with application to multiple hypothesis tracking", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:eQOLeE2rZwMC", "publication_date": "2001/11/6", "description": "The invention recognizes that a probability density function for fitting a model to a complex set of data often has multiple modes, each mode representing a reasonably probable state of the model when compared with the data. Particularly, sequential data such as are collected from detection of moving objects in three dimensional space are placed into data frames. Computation of the probability density function of the model state involves two main stages:(1) state prediction, in which the prior probability distribution is generated from information known prior to the availability of the data, and (2) state update, in which the posterior probability distribution is formed by updating the prior distribution with information obtained from observing the data. In particular this information obtained purely from data observations can also be expressed as a probability density function, known as the likelihood function. The likelihood \u2026", "total_citations": 44, "citation_graph": {"2000": 1, "2001": 0, "2002": 2, "2003": 1, "2004": 5, "2005": 2, "2006": 9, "2007": 4, "2008": 1, "2009": 1, "2010": 6, "2011": 0, "2012": 3, "2013": 2, "2014": 3, "2015": 3, "2016": 0, "2017": 0, "2018": 0, "2019": 0, "2020": 1}}, {"title": "Geometric saliency of curve correspondences and grouping of symmetric contours", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:W7OEmFMy1HYC", "authors": ["Tat-Jen Cham", "Roberto Cipolla"], "publication_date": "1996", "conference": "Computer Vision\u2014ECCV'96: 4th European Conference on Computer Vision Cambridge, UK, April 15\u201318, 1996 Proceedings, Volume I 4", "description": "Dependence on landmark points or high-order derivatives when establishing correspondences between geometrical image curves under various subclasses of projective transformation remains a shortcoming of present methods. In the proposed framework, geometric transformations are treated as smooth functions involving the parameters of the curves on which the transformation basis points lie. By allowing the basis points to vary along the curves, hypothesised correspondences are freed from the restriction to fixed point sets. An optimisation approach to localising neighbourhood-validated transformation bases is described which uses the deviation between projected and actual curve neighbourhood to iteratively improve correspondence estimates along the curves. However as transformation bases are inherently localisable to different degrees, the concept of geometric saliency is proposed in order to \u2026", "total_citations": 44, "citation_graph": {"1996": 5, "1997": 4, "1998": 8, "1999": 3, "2000": 1, "2001": 4, "2002": 0, "2003": 2, "2004": 1, "2005": 0, "2006": 0, "2007": 2, "2008": 3, "2009": 1, "2010": 1, "2011": 0, "2012": 1, "2013": 2, "2014": 0, "2015": 1, "2016": 1, "2017": 2}}, {"title": "A unified 3d human motion synthesis model via conditional variational auto-encoder", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:JQOojiI6XY0C", "authors": ["Yujun Cai", "Yiwei Wang", "Yiheng Zhu", "Tat-Jen Cham", "Jianfei Cai", "Junsong Yuan", "Jun Liu", "Chuanxia Zheng", "Sijie Yan", "Henghui Ding", "Xiaohui Shen", "Ding Liu", "Nadia Magnenat Thalmann"], "publication_date": "2021", "conference": "Proceedings of the IEEE/CVF International Conference on Computer Vision", "description": "We present a unified and flexible framework to address the generalized problem of 3D motion synthesis that covers the tasks of motion prediction, completion, interpolation, and spatial-temporal recovery. Since these tasks have different input constraints and various fidelity and diversity requirements, most existing approaches only cater to a specific task or use different architectures to address various tasks. Here we propose a unified framework based on Conditional Variational Auto-Encoder (CVAE), where we treat any arbitrary input as a masked motion series. Notably, by considering this problem as a conditional generation process, we estimate a parametric distribution of the missing regions based on the input conditions, from which to sample and synthesize the full motion series. To further allow the flexibility of manipulating the motion style of the generated series, we design an Action-Adaptive Modulation (AAM) to propagate the given semantic guidance through the whole sequence. We also introduce a cross-attention mechanism to exploit distant relations among decoder and encoder features for better realism and global consistency. We conducted extensive experiments on Human 3.6 M and CMU-Mocap. The results show that our method produces coherent and realistic results for various motion synthesis tasks, with the synthesized motions distinctly adapted by the given action labels.", "total_citations": 40, "citation_graph": {"2021": 3, "2022": 13, "2023": 24}}, {"title": "Projected light displays using visual feedback", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:WF5omc3nYNoC", "authors": ["James M Rehg", "Matthew Flagg", "Tat-Jen Cham", "Rahul Sukthankar", "Gita Sukthankar"], "publication_date": "2002/12/2", "conference": "7th International Conference on Control, Automation, Robotics and Vision, 2002. ICARCV 2002.", "description": "A system of coordinated projectors and cameras enables the creation of projected light displays that are robust to environmental disturbances. This paper describes approaches for tackling both geometric and photometric aspects of the problem: (1) the projected image remains stable even when the system components (projector, camera or screen) are moved; (2) the display automatically removes shadows caused by users moving between a projector and the screen, while simultaneously suppressing projected light on the user. The former can be accomplished without knowing the positions of the system components. The latter can be achieved without direct observation of the occluder. We demonstrate that the system responds quickly to environmental disturbances and achieves low steady-state errors.", "total_citations": 40, "citation_graph": {"2003": 2, "2004": 6, "2005": 2, "2006": 4, "2007": 3, "2008": 0, "2009": 4, "2010": 3, "2011": 7, "2012": 1, "2013": 1, "2014": 0, "2015": 0, "2016": 0, "2017": 0, "2018": 1, "2019": 3, "2020": 0, "2021": 2}}, {"title": "Unsupervised joint feature learning and encoding for RGB-D scene labeling", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:xtRiw3GOFMkC", "authors": ["Anran Wang", "Jiwen Lu", "Jianfei Cai", "Gang Wang", "Tat-Jen Cham"], "publication_date": "2015/8/11", "journal": "IEEE Transactions on Image Processing", "description": "Most existing approaches for RGB-D indoor scene labeling employ hand-crafted features for each modality independently and combine them in a heuristic manner. There has been some attempt on directly learning features from raw RGB-D data, but the performance is not satisfactory. In this paper, we propose an unsupervised joint feature learning and encoding (JFLE) framework for RGB-D scene labeling. The main novelty of our learning framework lies in the joint optimization of feature learning and feature encoding in a coherent way, which significantly boosts the performance. By stacking basic learning structure, higher level features are derived and combined with lower level features for better representing RGB-D data. Moreover, to explore the nonlinear intrinsic characteristic of data, we further propose a more general joint deep feature learning and encoding (JDFLE) framework that introduces the nonlinear \u2026", "total_citations": 39, "citation_graph": {"2015": 4, "2016": 6, "2017": 8, "2018": 6, "2019": 5, "2020": 2, "2021": 6}}, {"title": "A dynamic bayesian network approach to tracking using learned switching dynamic models", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:hqOjcs7Dif8C", "authors": ["Vladimir Pavlovi\u0107", "James Rehg", "Tat-Jen Cham"], "publication_date": "2000", "journal": "Hybrid Systems: Computation and Control", "description": "Switching linear dynamic systems (SLDS) attempt to describe a complex nonlinear dynamic system with a succession of linear models indexed by a switching variable. Unfortunately, despite SLDS\u2019s simplicity exact state and parameter estimation are still intractable. Recently, a broad class of learning and inference algorithms for time-series models have been successfully cast in the framework of dynamic Bayesian networks (DBNs). This paper describes a novel DBN-based SLDS model. A key feature of our approach are two approximate inference techniques for overcoming the intractability of exact inference in SLDS. As an example, we apply our model to the human figure motion analysis. We present experimental results for learning figure dynamics from video data and show promising results for tracking, interpolation, synthesis, and classification using learned models.", "total_citations": 35, "citation_graph": {"2002": 3, "2003": 0, "2004": 3, "2005": 2, "2006": 3, "2007": 4, "2008": 1, "2009": 3, "2010": 3, "2011": 1, "2012": 1, "2013": 1, "2014": 1, "2015": 1, "2016": 0, "2017": 2, "2018": 3, "2019": 0, "2020": 1, "2021": 0, "2022": 1}}, {"title": "Conditional adversarial synthesis of 3D facial action units", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:08ZZubdj9fEC", "authors": ["Zhilei Liu", "Guoxian Song", "Jianfei Cai", "Tat-Jen Cham", "Juyong Zhang"], "publication_date": "2019/8/25", "journal": "Neurocomputing", "description": "Employing deep learning-based approaches for fine-grained facial expression analysis, such as those involving the estimation of Action Unit (AU) intensities, is difficult due to the lack of a large-scale dataset of real faces with sufficiently diverse AU labels for training. In this paper, we consider how AU-level facial image synthesis can be used to substantially augment such a dataset. We propose an AU synthesis framework that combines the well-known 3D Morphable Model (3DMM), which intrinsically disentangles expression parameters from other face attributes, with models that adversarially generate 3DMM expression parameters conditioned on given target AU labels, in contrast to the more conventional approach of generating facial images directly. In this way, we are able to synthesize new combinations of expression parameters and facial images from desired AU labels. Extensive quantitative and qualitative \u2026", "total_citations": 33, "citation_graph": {"2018": 2, "2019": 4, "2020": 9, "2021": 9, "2022": 6, "2023": 3}}, {"title": "A unified feature selection framework for graph embedding on high dimensional data", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:pyW8ca7W8N0C", "authors": ["Marcus Chen", "Ivor W Tsang", "Mingkui Tan", "Tat Jen Cham"], "publication_date": "2014/12/18", "journal": "IEEE Transactions on Knowledge and Data Engineering", "description": "Although graph embedding has been a powerful tool for modeling data intrinsic structures, simply employing all features for data structure discovery may result in noise amplification. This is particularly severe for high dimensional data with small samples. To meet this challenge, this paper proposes a novel efficient framework to perform feature selection for graph embedding, in which a category of graph embedding methods is cast as a least squares regression problem. In this framework, a binary feature selector is introduced to naturally handle the feature cardinality in the least squares formulation. The resultant integral programming problem is then relaxed into a convex Quadratically Constrained Quadratic Program (QCQP) learning problem, which can be efficiently solved via a sequence of accelerated proximal gradient (APG) methods. Since each APG optimization is w.r.t. only a subset of features, the \u2026", "total_citations": 29, "citation_graph": {"2015": 1, "2016": 2, "2017": 2, "2018": 2, "2019": 7, "2020": 2, "2021": 6, "2022": 4, "2023": 3}}, {"title": "Sem2NeRF: Converting Single-View Semantic Masks to Neural Radiance Fields", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:z_wVstp3MssC", "authors": ["Yuedong Chen", "Qianyi Wu", "Chuanxia Zheng", "Tat-Jen Cham", "Jianfei Cai"], "publication_date": "2022/10/28", "conference": "European Conference on Computer Vision (ECCV), Tel Aviv, Israel", "description": "Image translation and manipulation have gain increasing attention along with the rapid development of deep generative models. Although existing approaches have brought impressive results, they mainly operated in 2D space. In light of recent advances in NeRF-based 3D-aware generative models, we introduce a new task, Semantic-to-NeRF translation, that aims to reconstruct a 3D scene modelled by NeRF, conditioned on one single-view semantic mask as input. To kick-off this novel task, we propose the Sem2NeRF framework. In particular, Sem2NeRF addresses the highly challenging task by encoding the semantic mask into the latent code that controls the 3D scene representation of a pre-trained decoder. To further improve the accuracy of the mapping, we integrate a new region-aware learning strategy into the design of both the encoder and the decoder. We verify the efficacy of the proposed Sem2NeRF \u2026", "total_citations": 27, "citation_graph": {"2021": 1, "2022": 11, "2023": 15}}, {"title": "Towards a switchable AR/VR near-eye display with accommodation-vergence and eyeglass prescription support", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:mvPsJ3kp5DgC", "authors": ["Xinxing Xia", "Yunqing Guan", "Andrei State", "Praneeth Chakravarthula", "Kishore Rathinavel", "Tat-Jen Cham", "Henry Fuchs"], "publication_date": "2019/8/12", "journal": "IEEE transactions on visualization and computer graphics", "description": "In this paper, we present our novel design for switchable AR/VR near-eye displays which can help solve the vergence-accommodation-conflict issue. The principal idea is to time-multiplex virtual imagery and real-world imagery and use a tunable lens to adjust focus for the virtual display and the see-through scene separately. With this novel design, prescription eyeglasses for near- and far-sighted users become unnecessary. This is achieved by integrating the wearer's corrective optical prescription into the tunable lens for both virtual display and see-through environment. We built a prototype based on the design, comprised of micro-display, optical systems, a tunable lens, and active shutters. The experimental results confirm that the proposed near-eye display design can switch between AR and VR and can provide correct accommodation for both.", "total_citations": 27, "citation_graph": {"2020": 3, "2021": 15, "2022": 3, "2023": 6}}, {"title": "A theory for photometric self-calibration of multiple overlapping projectors and cameras", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:Se3iqnhoufwC", "authors": ["Peng Song", "Tat-Jen Cham"], "publication_date": "2005/6/25", "conference": "Computer Vision and Pattern Recognition-Workshops, 2005. CVPR Workshops. IEEE Computer Society Conference on", "description": "A theory for photometric calibration of cameras and multiple projectors with overlapping displays is presented. The theory is predominantly based on the analysis of isointensity curves in projector input state space curves which define different projector input intensity combinations that result in the same camera-observed pixel intensities. Three methods, which have different speed and accuracy tradeoffs, are proposed for recovering the projector-to-screen and screento- camera intensity transfer functions. The methods do not require a specific parametric model for the shapes of these functions, nor impose any smoothness constraints. Additional methods are described for calibrating projector offsets and binary light sources, and also extending the perpixel analysis to other pixels in the display. The methods do not require the use of expensive equipment, and may be carried out with a low dynamic range camera with \u2026", "total_citations": 27, "citation_graph": {"2005": 1, "2006": 5, "2007": 3, "2008": 5, "2009": 2, "2010": 5, "2011": 0, "2012": 3, "2013": 1, "2014": 0, "2015": 1}}, {"title": "Detection caching for faster object detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:ULOm3_A8WrAC", "authors": ["Minh-Tri Pham", "Tat-Jen Cham"], "publication_date": "2005", "journal": "Proc. of CVPR", "description": "This paper investigates how the speed of an object detector can be rapidly increased through a caching framework when input sequences are quasi-repetitive. In the proposed framework, observed output states are discretized into a large number of classes. Each class induces its own discriminant subspace in the feature space, and is associated with a feature exemplar and a local metric. The feature exemplars and the local metrics are learned online using a novel piecewise linear discriminant analysis. The execution of the original object detector is skipped when the current image feature vector is similar to previously observed feature exemplars, and previously detected object states may simply be recalled. The exemplar recognition is carried via a 1-pass approximate nearest neighbor search in an index tree based on k-means clustering. Preliminary results show up to a 5-fold improvement when applied to the Viola & Jones [23] face detector, improving speeds from 10fps to 50fps. Experiments were done on the standard MPEG-4 testing sequences and real data.", "total_citations": 26, "citation_graph": {"2009": 1, "2010": 1, "2011": 5, "2012": 4, "2013": 7, "2014": 2, "2015": 1, "2016": 1, "2017": 1, "2018": 1, "2019": 0, "2020": 0, "2021": 1, "2022": 1}}, {"title": "A color-guided, region-adaptive and depth-selective unified framework for Kinect depth recovery", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:pqnbT2bcN3wC", "authors": ["Chongyu Chen", "Jianfei Cai", "Jianmin Zheng", "Tat-Jen Cham", "Guangming Shi"], "publication_date": "2013/9/30", "conference": "2013 IEEE 15th International Workshop on Multimedia Signal Processing (MMSP)", "description": "Considering the existing depth recovery approaches that have different limitations when applying to Kinect depth data, in this paper, we propose to integrate their effective features including adaptive support region selection, reliable depth selection and color guidance together under a unified framework for Kinect depth recovery. In particular, we formulate our depth recovery as an energy minimization problem, which solves the depth hole-filling and denoising simultaneously. The energy function consists of a fidelity term and a regularization term. The fidelity term takes into account the characteristics of Kinect data. The regularization term is designed to incorporate the joint bilateral filtering (JBF) kernel and the joint trilateral filtering (JTF) kernel so as to facilitate both depth hole-filling and denoising. Moreover, the JBF kernel is modified to incorporate the structure information. Both simulations on the benchmark \u2026", "total_citations": 24, "citation_graph": {"2014": 3, "2015": 5, "2016": 5, "2017": 5, "2018": 2, "2019": 1, "2020": 0, "2021": 2, "2022": 1}}, {"title": "Shadow elimination and blinding light suppression for interactive projected displays", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:YOwf2qJgpHMC", "authors": ["Jay Summet", "Matthew Flagg", "Tat-Jen Cham", "James M Rehg", "Rahul Sukthankar"], "publication_date": "2007/5/1", "journal": "IEEE Transactions on Visualization and Computer Graphics", "description": "A major problem with interactive displays based on front-projection is that users cast undesirable shadows on the display surface. This situation is only partially-addressed by mounting a single projector at an extreme angle and warping the projected image to undo keystoning distortions. This paper demonstrates that shadows can be muted by redundantly-illuminating the display surface using multiple projectors, all mounted at different locations. However, this technique alone does not eliminate shadows: multiple projectors create multiple dark regions on the surface (penumbral occlusions) and cast undesirable light onto the users. These problems can be solved by eliminating shadows and suppressing the light that falls on occluding users by actively modifying the projected output. This paper categorizes various methods that can be used to achieve redundant illumination, shadow elimination, and blinding light suppression, and evaluates their performance.", "total_citations": 23, "citation_graph": {"2006": 1, "2007": 2, "2008": 3, "2009": 1, "2010": 2, "2011": 1, "2012": 1, "2013": 0, "2014": 4, "2015": 2, "2016": 2, "2017": 2, "2018": 0, "2019": 1}}, {"title": "Minimizing image blur in an image projected onto a display surface by a projector", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:R3hNpaxXUhUC", "publication_date": "2007/12/13", "description": "A method for minimizing image blur in an image projected onto a display surface by a projector, the image blur being caused by out-of-focus regions, the method comprising: estimating (10) a spatially varying point-spread-functions (PSF) profile for a test image projected by the projector; and pre-conditioning (11) the image using a predetermined pre-processing algorithm based on the estimated PSF profile; wherein the pre-conditioned image is projected (17) by the projector to minimise image blur.", "total_citations": 22, "citation_graph": {"2010": 1, "2011": 0, "2012": 4, "2013": 2, "2014": 2, "2015": 2, "2016": 3, "2017": 2, "2018": 1, "2019": 0, "2020": 1, "2021": 2, "2022": 1, "2023": 1}}, {"title": "Kinect depth recovery using a color-guided, region-adaptive, and depth-selective framework", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:bFI3QPDXJZMC", "authors": ["Chongyu Chen", "Jianfei Cai", "Jianmin Zheng", "Tat Jen Cham", "Guangming Shi"], "publication_date": "2015/3/31", "journal": "ACM Transactions on Intelligent Systems and Technology (TIST)", "description": "Considering that the existing depth recovery approaches have different limitations when applied to Kinect depth data, in this article, we propose to integrate their effective features including adaptive support region selection, reliable depth selection, and color guidance together under an optimization framework for Kinect depth recovery. In particular, we formulate our depth recovery as an energy minimization problem, which solves the depth hole filling and denoising simultaneously. The energy function consists of a fidelity term and a regularization term, which are designed according to the Kinect characteristics. Our framework inherits and improves the idea of guided filtering by incorporating structure information and prior knowledge of the Kinect noise model. Through analyzing the solution to the optimization framework, we also derive a local filtering version that provides an efficient and effective way of improving \u2026", "total_citations": 21, "citation_graph": {"2015": 2, "2016": 1, "2017": 3, "2018": 5, "2019": 3, "2020": 2, "2021": 2, "2022": 1, "2023": 2}}, {"title": "Visual tracking with generative template model based on riemannian manifold of covariances", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:IWHjjKOFINEC", "authors": ["Marcus Chen", "Sze Kim Pang", "Tat Jen Cham", "Alvina Goh"], "publication_date": "2011/7/5", "conference": "14th International Conference on Information Fusion", "description": "Robust visual tracking is a research area that has many important applications. The main challenges include how the target image can be modeled and how this model can be updated. In this paper, we model the target using a covariance descriptor. This descriptor is robust to problems that commonly occur in visual tracking such as pixel-pixel misalignment, pose and illumination changes. We model the changes in the template using a generative process. We introduce a new dynamical model for the template update using a random walk on the Riemannian manifold where the covariance descriptors lie in. This enables us to jointly quantify the uncertainties relating to the kinematic states and the template in a principled way. The sequential inference of the posterior distribution of the kinematic states and the template is done using a particle filter. Our results show that this principled approach is robust to changes in \u2026", "total_citations": 21, "citation_graph": {"2011": 1, "2012": 3, "2013": 5, "2014": 0, "2015": 4, "2016": 1, "2017": 2, "2018": 0, "2019": 1, "2020": 0, "2021": 0, "2022": 1, "2023": 1}}, {"title": "Global context with discrete diffusion in vector quantised modelling for image generation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:kzcrU_BdoSEC", "authors": ["Minghui Hu", "Yujie Wang", "Tat-Jen Cham", "Jianfei Yang", "Ponnuthurai N Suganthan"], "publication_date": "2022", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "description": "The integration of Vector Quantised Variational AutoEncoder (VQ-VAE) with autoregressive models as generation part has yielded high-quality results on image generation. However, the autoregressive models will strictly follow the progressive scanning order during the sampling phase. This leads the existing VQ series models to hardly escape the trap of lacking global information. Denoising Diffusion Probabilistic Models (DDPM) in the continuous domain have shown a capability to capture the global context, while generating high-quality images. In the discrete state space, some works have demonstrated the potential to perform text generation and low resolution image generation. We show that with the help of a content-rich discrete visual codebook from VQ-VAE, the discrete diffusion model can also generate high fidelity images with global context, which compensates for the deficiency of the classical autoregressive model along pixel space. Meanwhile, the integration of the discrete VAE with the diffusion model resolves the drawback of conventional autoregressive models being oversized, and the diffusion model which demands excessive time in the sampling process when generating images. It is found that the quality of the generated images is heavily dependent on the discrete visual codebook. Extensive experiments demonstrate that the proposed Vector Quantised Discrete Diffusion Model (VQ-DDM) is able to achieve comparable performance to top-tier methods with low complexity. It also demonstrates outstanding advantages over other vectors quantised with autoregressive models in terms of image inpainting tasks without additional \u2026", "total_citations": 20, "citation_graph": {"2022": 8, "2023": 12}}, {"title": "Kinect shadow detection and classification", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:HoB7MX3m0LUC", "authors": ["Teng Deng", "Hui Li", "Jianfei Cai", "Tat-Jen Cham", "Henry Fuchs"], "publication_date": "2013", "conference": "Proceedings of the IEEE International Conference on Computer Vision Workshops", "description": "Kinect depth maps often contain missing data, or\" holes\", for various reasons. Most existing Kinect-related research treat these holes as artifacts and try to minimize them as much as possible. In this paper, we advocate a totally different idea turning Kinect holes into useful information. In particular, we are interested in the unique type of holes that are caused by occlusion of the Kinect's structured light, resulting in shadows and loss of depth acquisition. We propose a robust detection scheme to detect and classify different types of shadows based on their distinct local shadow patterns as determined from geometric analysis, without assumption on object geometry. Experimental results demonstrate that the proposed scheme can achieve very accurate shadow detection. We also demonstrate the usefulness of the extracted shadow information by successfully applying it for automatic foreground segmentation.", "total_citations": 20, "citation_graph": {"2014": 4, "2015": 3, "2016": 4, "2017": 2, "2018": 3, "2019": 2, "2020": 0, "2021": 1, "2022": 0, "2023": 1}}, {"title": "Recovering surface details under general unknown illumination using shading and coarse multi-view stereo", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:zA6iFVUQeVQC", "authors": ["Di Xu", "Qi Duan", "Jianming Zheng", "Juyong Zhang", "Jianfei Cai", "Tat-Jen Cham"], "publication_date": "2014", "conference": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "description": "Reconstructing the shape of a 3D object from multi-view images under unknown, general illumination is a fundamental problem in computer vision and high quality reconstruction is usually challenging especially when high detail is needed. This paper presents a total variation (TV) based approach for recovering surface details using shading and multi-view stereo (MVS). Behind the approach are our two important observations:(1) the illumination over the surface of an object tends to be piecewise smooth and (2) the recovery of surface orientation is not sufficient for reconstructing geometry, which were previously overlooked. Thus we introduce TV to regularize the lighting and use visual hull to constrain partial vertices. The reconstruction is formulated as a constrained TVminimization problem that treats the shape and lighting as unknowns simultaneously. An augmented Lagrangian method is proposed to quickly solve the TV-minimization problem. As a result, our approach is robust, stable and is able to efficiently recover high quality of surface details even starting with a coarse MVS. These advantages are demonstrated by the experiments with synthetic and real world examples.", "total_citations": 19, "citation_graph": {"2015": 1, "2016": 4, "2017": 3, "2018": 1, "2019": 1, "2020": 4, "2021": 4, "2022": 1}}, {"title": "Video-based human action classi. cation with ambiguous correspondences", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:M3ejUd6NZC8C", "authors": ["Zhou Feng", "Tat-Jen Cham"], "publication_date": "2005/9/21", "conference": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)-Workshops", "description": "This paper describes a combined tracking-classification framework for the unsupervised classification of human action. While most existing approaches assume that featurewise correspondences on people are either available or not at all, this method explicitly formalizes how the probability of correspondences can be used in computation when the correspondences are ambiguous. It is also able to exploit in a probabilistic manner any foreground-background preprocessed segmentation, even if the segmentation is of low confidence. A principled analysis of the problem leads to a novel probabilistic action representation called the correspondence-ambiguous feature histogram array (CAFHA) that is robust to variations across similar actions. Our results show that the new framework outperforms the recent Zelnik-Manor and Irani method [19] for unsupervised event classi?cation. Additionally, the framework is extended \u2026", "total_citations": 19, "citation_graph": {"2006": 1, "2007": 2, "2008": 1, "2009": 1, "2010": 0, "2011": 1, "2012": 1, "2013": 3, "2014": 2, "2015": 3, "2016": 3, "2017": 0, "2018": 0, "2019": 0, "2020": 0, "2021": 0, "2022": 0, "2023": 1}}, {"title": "Analogous view transfer for gaze correction in video sequences", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:dhFuZR0502QC", "authors": ["Tat-Jen Cham", "Shyam Krishnamoorthy", "Michael Jones"], "publication_date": "2002/12/2", "conference": "7th International Conference on Control, Automation, Robotics and Vision, 2002. ICARCV 2002.", "description": "This paper provides a framework for doing facial gaze correction in video sequences. The proposed framework involves stages of face registration, face parameter mapping, and face synthesis. We introduce the concept of analogous views, and derive a novel formulation which extends view transfers based on epipolar geometry to cope with non-rigid motion. Additionally, a disparity mapping function is derived which is learned from training data and handles both spatial disparities as well as pixel-value changes. The disparity mapping function generalizes to facial expressions, illumination conditions and individuals not in the training set, as shown by the results obtained.", "total_citations": 19, "citation_graph": {"2001": 1, "2002": 1, "2003": 0, "2004": 1, "2005": 1, "2006": 0, "2007": 0, "2008": 0, "2009": 0, "2010": 0, "2011": 1, "2012": 2, "2013": 2, "2014": 1, "2015": 5, "2016": 1, "2017": 3}}, {"title": "Geoconv: Geodesic guided convolution for facial action unit recognition", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:eMMeJKvmdy0C", "authors": ["Yuedong Chen", "Guoxian Song", "Zhiwen Shao", "Jianfei Cai", "Tat-Jen Cham", "Jianmin Zheng"], "publication_date": "2022/2/1", "journal": "Pattern Recognition", "description": "Automatic facial action unit (AU) recognition has attracted great attention but still remains a challenging task, as subtle changes of local facial muscles are difficult to thoroughly capture. Most existing AU recognition approaches leverage geometry information in a straightforward 2D or 3D manner, which either ignore 3D manifold information or suffer from high computational costs. In this paper, we propose a novel geodesic guided convolution (GeoConv) for AU recognition by embedding 3D manifold information into 2D convolutions. Specifically, the kernel of GeoConv is weighted by our introduced geodesic weights, which are negatively correlated to geodesic distances on a coarsely reconstructed 3D morphable face model. Moreover, based on GeoConv, we further develop an end-to-end trainable framework named GeoCNN for AU recognition. Extensive experiments on BP4D and DISFA benchmarks show that \u2026", "total_citations": 17, "citation_graph": {"2021": 2, "2022": 10, "2023": 5}}, {"title": "Shading-based surface detail recovery under general unknown illumination", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:Tiz5es2fbqcC", "authors": ["Di Xu", "Qi Duan", "Jianmin Zheng", "Juyong Zhang", "Jianfei Cai", "Tat-Jen Cham"], "publication_date": "2017/2/17", "journal": "IEEE transactions on pattern analysis and machine intelligence", "description": "Reconstructing the shape of a 3D object from multi-view images under unknown, general illumination is a fundamental problem in computer vision. High quality reconstruction is usually challenging especially when fine detail is needed and the albedo of the object is non-uniform. This paper introduces vertex overall illumination vectors to model the illumination effect and presents a total variation (TV) based approach for recovering surface details using shading and multi-view stereo (MVS). Behind the approach are the two important observations: (1) the illumination over the surface of an object often appears to be piecewise smooth and (2) the recovery of surface orientation is not sufficient for reconstructing the surface, which was often overlooked previously. Thus we propose to use TV to regularize the overall illumination vectors and use visual hull to constrain partial vertices. The reconstruction is formulated as a \u2026", "total_citations": 17, "citation_graph": {"2018": 1, "2019": 3, "2020": 4, "2021": 4, "2022": 5}}, {"title": "Real-time and temporal-coherent foreground extraction with commodity RGBD camera", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:f2IySw72cVMC", "authors": ["Mengyao Zhao", "Chi-Wing Fu", "Jianfei Cai", "Tat-Jen Cham"], "publication_date": "2014/12/18", "journal": "IEEE Journal of selected topics in signal processing", "description": "Foreground extraction from video stream is an important component in many multimedia applications. By exploiting commodity RGBD cameras, we could further extract dynamic foreground objects with 3D information in real-time, thereby enabling new forms of multimedia applications such as 3D telepresence. However, one critical problem with existing methods for real-time foreground extraction is temporal coherency. They could exhibit severe flickering results for foreground objects such as human motion, thus affecting the visual quality as well as the image object analysis in the multimedia applications. This paper presents a new GPU-based real-time foreground extraction method with several novel techniques. First, we detect shadow and fill missing depth data accordingly in RGBD video, and then adaptively combine color and depth masks to form a trimap. After that, we formulate a novel closed-form matting \u2026", "total_citations": 17, "citation_graph": {"2015": 1, "2016": 3, "2017": 6, "2018": 4, "2019": 1, "2020": 0, "2021": 1, "2022": 0, "2023": 1}}, {"title": "Dynamic feature ordering for efficient registration", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:8k81kl-MbHgC", "authors": ["Tat-Jen Cham", "James M Rehg"], "publication_date": "1999/9/20", "conference": "Proceedings of the Seventh IEEE International Conference on Computer Vision", "description": "Existing sequential feature based registration algorithms involving search typically either select features randomly (e.g. the RANSAC approach (M. Fischler and R. Bolles, 1981)) or assume a predefined, intuitive ordering for the features (e.g. based on size or resolution). The paper presents a formal framework for computing an ordering for features which maximizes search efficiency. Features are ranked according to matching ambiguity measure, and an algorithm is proposed which couples the feature selection with the parameter estimation, resulting in a dynamic feature ordering. The analysis is extended to template features where the matching is non discrete and a sample refinement process is proposed. The framework is demonstrated effectively on the localization of a person in an image, using a kinematic model with template features. Different priors are used on the model parameters and the results \u2026", "total_citations": 17, "citation_graph": {"2000": 4, "2001": 1, "2002": 1, "2003": 1, "2004": 0, "2005": 4, "2006": 0, "2007": 1, "2008": 1, "2009": 0, "2010": 1, "2011": 3}}, {"title": "Skewed Symmetry Detection Through Local Skewed Symmetries.", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:3fE2CSJIrl8C", "authors": ["Tat-Jen Cham", "Roberto Cipolla"], "publication_date": "1994/9", "journal": "BMVC", "description": "We explore how global symmetry can be detected prior to segmentation and under noise and occlusion. The definition of local symmetries is extended to affine geometries by considering the tangents and curvatures of local structures, and a quantitative measure of local symmetry known as symmetricity is introduced, which is based on Mahalanobis distances from the tangent-curvature states of local structures to the local skewed symmetry state-subspace. These symmetricity values, together with the associated local axes of symmetry, are spatially related in the local skewed symmetry field (LSSF). In the implementation, a fast, local symmetry detection algorithm allows initial hypotheses for the symmetry axis to be generated through the use of a modified Hough transform. This is then improved upon by maximising a global symmetry measure based on accumulated local support in the LSSF\u2014a straight active contour \u2026", "total_citations": 17, "citation_graph": {"1995": 1, "1996": 3, "1997": 0, "1998": 1, "1999": 0, "2000": 0, "2001": 0, "2002": 0, "2003": 2, "2004": 0, "2005": 0, "2006": 0, "2007": 0, "2008": 6, "2009": 0, "2010": 1, "2011": 0, "2012": 0, "2013": 1, "2014": 0, "2015": 0, "2016": 0, "2017": 0, "2018": 0, "2019": 0, "2020": 0, "2021": 1, "2022": 1}}, {"title": "3D faces are recognized more accurately and faster than 2D faces, but with similar inversion effects", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:K3LRdlH-MEoC", "authors": ["ZHD Eng", "Yee Ying Yick", "Yulan Guo", "Hong Xu", "Miriam Reiner", "Tat-Jen Cham", "SHA Chen"], "publication_date": "2017/9/1", "journal": "Vision research", "description": "Recognition of faces typically occurs via holistic processing where individual features are combined to provide an overall facial representation. However, when faces are inverted, there is greater reliance on featural processing where faces are recognized based on their individual features. These findings are based on a substantial number of studies using 2-dimensional (2D) faces and it is unknown whether these results can be extended to 3-dimensional (3D) faces, which have more depth information that is absent in the typical 2D stimuli used in face recognition literature. The current study used the face inversion paradigm as a means to investigate how holistic and featural processing are differentially influenced by 2D and 3D faces. Twenty-five participants completed a delayed face-matching task consisting of upright and inverted faces that were presented as both 2D and 3D stereoscopic images. Recognition \u2026", "total_citations": 16, "citation_graph": {"2018": 4, "2019": 2, "2020": 3, "2021": 4, "2022": 1, "2023": 1}}, {"title": "A generative model for depth-based robust 3D facial pose tracking", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:l7t_Zn2s7bgC", "authors": ["Lu Sheng", "Jianfei Cai", "Tat-Jen Cham", "Vladimir Pavlovic", "King Ngi Ngan"], "publication_date": "2017", "conference": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "description": "We consider the problem of depth-based robust 3D facial pose tracking under unconstrained scenarios with heavy occlusions and arbitrary facial expression variations. Unlike the previous depth-based discriminative or data-driven methods that require sophisticated training or manual intervention, we propose a generative framework that unifies pose tracking and face model adaptation on-the-fly. Particularly, we propose a statistical 3D face model that owns the flexibility to generate and predict the distribution and uncertainty underlying the face model. Moreover, unlike prior arts employing the ICP-based facial pose estimation, we propose a ray visibility constraint that regularizes the pose based on the face model's visibility against the input point cloud, which augments the robustness against the occlusions. The experimental results on Biwi and ICT-3DHP datasets reveal that the proposed framework is effective and outperforms the state-of-the-art depth-based methods.", "total_citations": 16, "citation_graph": {"2017": 2, "2018": 3, "2019": 0, "2020": 3, "2021": 4, "2022": 2, "2023": 1}}, {"title": "Objects co-segmentation: Propagated from simpler images", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:NhqRSupF_l8C", "authors": ["Marcus Chen", "Santiago Velasco-Forero", "Ivor Tsang", "Tat-Jen Cham"], "publication_date": "2015/4/19", "conference": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "description": "Recent works on image co-segmentation aim to segment common objects among image sets. These methods can co-segment simple images well, but their performance may degrade significantly on more cluttered images. In order to co-segment both simple and complex images well, this paper proposes a novel paradigm to rank images and to propagate the segmentation results from the simple images to more and more complex ones. In the experiments, the proposed paradigm demonstrates its effectiveness in segmenting large image sets with a wide variety in object appearance, sizes, orientations, poses, and multiple objects in one image. It outperformed the current state-of-the-art algorithms significantly, especially in difficult images.", "total_citations": 14, "citation_graph": {"2016": 2, "2017": 1, "2018": 4, "2019": 2, "2020": 4, "2021": 0, "2022": 0, "2023": 1}}, {"title": "Kinect-based easy 3d object reconstruction", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:M05iB0D1s5AC", "authors": ["Di Xu", "Jianfei Cai", "Tat Jen Cham", "Philip Fu", "Juyong Zhang"], "publication_date": "2012", "conference": "Advances in Multimedia Information Processing\u2013PCM 2012: 13th Pacific-Rim Conference on Multimedia, Singapore, December 4-6, 2012. Proceedings 13", "description": "Inspired by the recently developed KinectFusion technique, which is able to reconstruct a 3D scene in real time through moving Kinect, we consider improving KinectFusion for 3D reconstruction of a real object. We make some adaptations to KinectFusion so as to identify the object-of-interest and separate the 3D object model from the entire 3D scene. Moreover, considering that the 3D object model generated by KinectFusion often contains some clearly visible outliers due to the noisy Kinect data, we propose a refinement scheme to remove the outliers. Our basic idea is to make use of the existing powerful 2D segmentation tool to refine the silhouette in each color image and then form visual hull via the refined dense silhouettes to improve the 3D object model. Experimental results show improved performance.", "total_citations": 14, "citation_graph": {"2013": 1, "2014": 0, "2015": 3, "2016": 2, "2017": 3, "2018": 2, "2019": 1, "2020": 0, "2021": 2}}, {"title": "Automated B-Spline Curve Representation with MDL-based Active Contours.", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:5nxA0vEk-isC", "authors": ["Tat-Jen Cham", "Roberto Cipolla"], "publication_date": "1996/9", "conference": "BMVC", "description": "Present spline-tting methods used in computer vision do not fully address the main issues of developing an automatic and reliable algorithm, which are discussed in this paper. A paradigm for spline tting is proposed, with features of the algorithm selected such that the main issues are resolved. This is achieved through the use of B-spline active contours, the minimum description length principle, and in conjunction with a control point insertion strategy based on the Potential for Energy-Reduction Maximisation (PERM). This strategy selects control points such that the formation of compatible collapse mechanisms for the splines is encouraged. An implementation of the algorithm is carried and tested on various images. A comparison with one of the better existing methods for spline tting demonstrates that there is considerable potential for the algorithm to outperform current algorithms.", "total_citations": 14, "citation_graph": {"1996": 1, "1997": 2, "1998": 2, "1999": 4, "2000": 1, "2001": 0, "2002": 0, "2003": 0, "2004": 1, "2005": 1, "2006": 0, "2007": 0, "2008": 1}}, {"title": "Robust real-time performance-driven 3D face tracking", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:738O_yMBCRsC", "authors": ["Hai X Pham", "Vladimir Pavlovic", "Jianfei Cai", "Tat-jen Cham"], "publication_date": "2016/12/4", "conference": "2016 23rd International Conference on Pattern Recognition (ICPR)", "description": "We introduce a novel robust hybrid 3D face tracking framework from RGBD video streams, which is capable of tracking head pose and facial actions without pre-calibration or intervention from a user. In particular, we emphasize on improving the tracking performance in instances where the tracked subject is at a large distance from the cameras, and the quality of point cloud deteriorates severely. This is accomplished by the combination of a flexible 3D shape regressor and the joint 2D+3D optimization on shape parameters. Our approach fits facial blendshapes to the point cloud of the human head, while being driven by an efficient and rapid 3D shape regressor trained on generic RGB datasets. As an on-line tracking system, the identity of the unknown user is adapted on-the-fly resulting in improved 3D model reconstruction and consequently better tracking performance. The result is a robust RGBD face tracker \u2026", "total_citations": 13, "citation_graph": {"2017": 3, "2018": 3, "2019": 2, "2020": 2, "2021": 1, "2022": 1, "2023": 1}}, {"title": "Discriminative distance measures for image matching", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:Wp0gIr-vW9MC", "authors": ["Xi Chen", "T-J Cham"], "publication_date": "2004/8/26", "conference": "Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.", "description": "We propose a framework incorporating aspects of image classification to aid the matching of a reference image to target images. The framework involves an image representation based on a set of feature vectors, and a parametric distance measure on any two such vector sets. The distance measure may be optimized to provide maximum discrimination between the matching target images and the background images, when compared to the reference image. Preliminary results indicate that the new distance measure performs substantially better than the traditional SSD and the Bhattacharyya histogram measures in classification and tracking tasks.", "total_citations": 13, "citation_graph": {"2005": 1, "2006": 0, "2007": 0, "2008": 0, "2009": 1, "2010": 3, "2011": 1, "2012": 1, "2013": 0, "2014": 3, "2015": 2, "2016": 1}}, {"title": "Real-time 3D face-eye performance capture of a person wearing VR headset", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:sSrBHYA8nusC", "authors": ["Guoxian Song", "Jianfei Cai", "Tat-Jen Cham", "Jianmin Zheng", "Juyong Zhang", "Henry Fuchs"], "publication_date": "2018/10/15", "book": "Proceedings of the 26th ACM international conference on Multimedia", "description": "Teleconference or telepresence based on virtual reality (VR) head-mount display (HMD) device is a very interesting and promising application since HMD can provide immersive feelings for users. However, in order to facilitate face-to-face communications for HMD users, real-time 3D facial performance capture of a person wearing HMD is needed, which is a very challenging task due to the large occlusion caused by HMD. The existing limited solutions are very complex either in setting or in approach as well as lacking the performance capture of 3D eye gaze movement. In this paper, we propose a convolutional neural network (CNN) based solution for real-time 3D face-eye performance capture of HMD users without complex modification to devices. To address the issue of lacking training data, we generate massive pairs of HMD face-label dataset by data synthesis as well as collecting VR-IR eye dataset from \u2026", "total_citations": 12, "citation_graph": {"2019": 1, "2020": 4, "2021": 0, "2022": 4, "2023": 3}}, {"title": "Pluralistic free-form image completion", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:UHK10RUVsp4C", "authors": ["Chuanxia Zheng", "Tat-Jen Cham", "Jianfei Cai"], "publication_date": "2021/10", "journal": "International Journal of Computer Vision", "description": "Image completion involves filling plausible contents to missing regions in images. Current image completion methods produce only one result for a given masked image, although there may be many reasonable possibilities. In this paper, we present an approach for pluralistic image completion\u2014the task of generating multiple and diverse plausible solutions for free-form image completion. A major challenge faced by learning-based approaches is that usually only one ground truth training instance per label for this multi-output problem. To overcome this, we propose a novel and probabilistically principled framework with two parallel paths. One is a reconstructive path that utilizes the only one ground truth to get prior distribution of missing patches and rebuild the original image from this distribution. The other is a generative path for which the conditional prior is coupled to the distribution obtained in the \u2026", "total_citations": 11, "citation_graph": {"2022": 6, "2023": 5}}, {"title": "High-quality Kinect depth filtering for real-time 3D telepresence", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:g5m5HwL7SMYC", "authors": ["Mengyao Zhao", "Fuwen Tan", "Chi-Wing Fu", "Chi-Keung Tang", "Jianfei Cai", "Tat Jen Cham"], "publication_date": "2013/7/15", "conference": "2013 IEEE International Conference on Multimedia and Expo (ICME)", "description": "3D telepresence is a next-generation multimedia application, offering remote users an immersive and natural video-conferencing environment with real-time 3D graphics. Kinect sensor, a consumer-grade range camera, facilitates the implementation of some recent 3D telepresence systems. However, conventional data filtering methods are insufficient to handle Kinect depth error because such error is quantized rather than just randomly-distributed. Hence, one could often observe large irregularly-shaped patches of pixels that receive the same depth values from Kinect. To enhance visual quality in 3D telepresence, we propose a novel depth data filtering method for Kinect by means of multi-scale and direction-aware support windows. In addition, we develop a GPU-based CUDA implementation that can perform real-time depth filtering. Results from the experiments show that our method can reconstruct hole-free \u2026", "total_citations": 11, "citation_graph": {"2014": 1, "2015": 5, "2016": 1, "2017": 0, "2018": 2, "2019": 1, "2020": 0, "2021": 0, "2022": 1}}, {"title": "Gaze correction for video conferencing", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:M3NEmzRMIkIC", "authors": ["TJ Cham", "M Jones"], "publication_date": "2003", "journal": "Compaq Cambridge Research Laboratory. http://www. crl. research. digital. com/vision/interfaces/corga", "total_citations": 11, "citation_graph": {"2002": 1, "2003": 3, "2004": 2, "2005": 0, "2006": 0, "2007": 1, "2008": 0, "2009": 1, "2010": 0, "2011": 0, "2012": 1}}, {"title": "Learning feature distance measures for image correspondences", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:4DMP91E08xMC", "authors": ["Xi Chen", "T-J Cham"], "publication_date": "2005/6/20", "conference": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)", "description": "Standard but ad hoc measures such as sum-of-squared pixel differences (SSD) are often used when comparing and registering two images that have not been previously observed before. In this paper, we propose a framework to address the problem of learning a parametric feature distance measure to measure the dissimilarity between pairs of images. The method is based on optimizing the parameters of the distance measure in order to minimize correspondence classification errors on training data. Because the learning process involves relative (rather than absolute) visual content between image pairs, the learned distance measure may also be applied to other images with very different visual content. Results on matching classification with a wide variety of image content show that the learned feature distance measure clearly outperforms the standard measures of SSD, chamfer and Bhattacharyya histogram \u2026", "total_citations": 10, "citation_graph": {"2007": 2, "2008": 3, "2009": 1, "2010": 1, "2011": 0, "2012": 1, "2013": 0, "2014": 0, "2015": 1, "2016": 0, "2017": 0, "2018": 0, "2019": 1}}, {"title": "Unified discrete diffusion for simultaneous vision-language generation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:fEOibwPWpKIC", "authors": ["Minghui Hu", "Chuanxia Zheng", "Heliang Zheng", "Tat-Jen Cham", "Chaoyue Wang", "Zuopeng Yang", "Dacheng Tao", "Ponnuthurai N Suganthan"], "publication_date": "2022/11/27", "journal": "arXiv preprint arXiv:2211.14842", "description": "The recently developed discrete diffusion models perform extraordinarily well in the text-to-image task, showing significant promise for handling the multi-modality signals. In this work, we harness these traits and present a unified multimodal generation model that can conduct both the \"modality translation\" and \"multi-modality generation\" tasks using a single model, performing text-based, image-based, and even vision-language simultaneous generation. Specifically, we unify the discrete diffusion process for multimodal signals by proposing a unified transition matrix. Moreover, we design a mutual attention module with fused embedding layer and a unified objective function to emphasise the inter-modal linkages, which are vital for multi-modality generation. Extensive experiments indicate that our proposed method can perform comparably to the state-of-the-art solutions in various generation tasks.", "total_citations": 9, "citation_graph": {"2023": 9}}, {"title": "Towards unbiased visual emotion recognition via causal intervention", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:dQ2og3OwTAUC", "authors": ["Yuedong Chen", "Xu Yang", "Tat-Jen Cham", "Jianfei Cai"], "publication_date": "2022/10/10", "book": "Proceedings of the 30th ACM International Conference on Multimedia", "description": "Although much progress has been made in visual emotion recognition, researchers have realized that modern deep networks tend to exploit dataset characteristics to learn spurious statistical associations between the input and the target. Such dataset characteristics are usually treated as dataset bias, which damages the robustness and generalization performance of these recognition systems. In this work, we scrutinize this problem from the perspective of causal inference, where such dataset characteristic is termed as a confounder which misleads the system to learn the spurious correlation. To alleviate the negative effects brought by the dataset bias, we propose a novel Interventional Emotion Recognition Network (IERN) to achieve the backdoor adjustment, which is one fundamental deconfounding technique in causal inference. Specifically, IERN starts by disentangling the dataset-related context feature from \u2026", "total_citations": 9, "citation_graph": {"2022": 4, "2023": 5}}, {"title": "Unconstrained facial action unit detection via latent feature domain", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:1yQoGdGgb4wC", "authors": ["Zhiwen Shao", "Jianfei Cai", "Tat-Jen Cham", "Xuequan Lu", "Lizhuang Ma"], "publication_date": "2021/6/22", "journal": "IEEE Transactions on Affective Computing", "description": "Facial action unit (AU) detection in the wild is a challenging problem, due to the unconstrained variability in facial appearances and the lack of accurate annotations. Most existing methods depend on either impractical labor-intensive labeling or inaccurate pseudo labels. In this paper, we propose an end-to-end unconstrained facial AU detection framework based on domain adaptation, which transfers accurate AU labels from a constrained source domain to an unconstrained target domain by exploiting labels of AU-related facial landmarks. Specifically, we map a source image with label and a target image without label into a latent feature domain by combining source landmark-related feature with target landmark-free feature. Due to the combination of source AU-related information and target AU-free information, the latent feature domain with transferred source label can be learned by maximizing the target \u2026", "total_citations": 8, "citation_graph": {"2022": 5, "2023": 3}}, {"title": "Semantic and spatial content fusion for scene recognition", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:yD5IFk8b50cC", "authors": ["Elahe Farahzadeh", "Tat-Jen Cham", "Wanqing Li"], "publication_date": "2015", "journal": "New Development in Robot Vision", "description": "In the field of scene recognition, it is usually insufficient to use only one visual feature regardless of how discriminative the feature is. Therefore, the spatial location and semantic relationships of local features need to be captured together with the scene contextual information. In this paper we proposed a novel framework to project image contextual feature space with semantic space of local features into a map function. This embedding is performed based on a subset of training images denoted as an exemplar-set. This exemplar-set is composed of images that describe better the scene category\u2019s attributes than the other images. The proposed framework learns a weighted combination of local semantic topics as well as global and spatial information, where the weights represent the features\u2019 contributions in each scene category. An empirical study was performed on two of the most challenging scene \u2026", "total_citations": 8, "citation_graph": {"2017": 1, "2018": 1, "2019": 1, "2020": 2, "2021": 0, "2022": 2, "2023": 1}}, {"title": "A local approach to recovering global skewed symmetry", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:qxL8FJ1GzNcC", "authors": ["Tat-Jen Cham", "Roberto Cipolla"], "publication_date": "1994/10/9", "conference": "Proceedings of 12th International Conference on Pattern Recognition", "description": "A local approach is adopted to allow recovery of global skewed symmetry in the presence of occlusion. Local skewed symmetries are established by extending the definition of local symmetries to affine geometries through the use of local derivatives. Symmetricity, a quantitative gauge of local symmetry based on Mahalanobis distances from the tangent-curvature states of local structures to the local skewed symmetry state-subspace, is also introduced to cope with noise. The symmetricity values and local symmetry axes for each pair of points are then spatially related in the local skewed symmetry field. The global symmetry detection algorithm implemented involves obtaining fast, initial estimates of the symmetry axis from a separate Hough transform technique, followed by maximising a global symmetry measure via a straight active contour model which is driven by effective symmetricity values. This produces useful \u2026", "total_citations": 8, "citation_graph": {"1995": 1, "1996": 1, "1997": 0, "1998": 1, "1999": 1, "2000": 0, "2001": 0, "2002": 0, "2003": 1, "2004": 1, "2005": 0, "2006": 1, "2007": 0, "2008": 0, "2009": 0, "2010": 0, "2011": 0, "2012": 0, "2013": 0, "2014": 0, "2015": 1}}, {"title": "Towards eyeglass-style holographic near-eye displays with statically", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:LjlpjdlvIbIC", "authors": ["Xinxing Xia", "Yunqing Guan", "Andrei State", "Praneeth Chakravarthula", "Tat-Jen Cham", "Henry Fuchs"], "publication_date": "2020/11/9", "conference": "2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)", "description": "Holography is perhaps the only method demonstrated so far that can achieve a wide field of view (FOV) and a compact eyeglass-style form factor for augmented reality (AR) near-eye displays (NEDs). Unfortunately, the eyebox of such NEDs is impractically small (~ <; 1mm). In this paper, we introduce and demonstrate a design for holographic NEDs with a practical, wide eyebox of ~ 10mm and without any moving parts, based on holographic lenslets. In our design, a holographic optical element (HOE) based on a lenslet array was fabricated as the image combiner with expanded eyebox. A phase spatial light modulator (SLM) alters the phase of the incident laser light projected onto the HOE combiner such that the virtual image can be perceived at different focus distances, which can reduce the vergence-accommodation conflict (VAC). We have successfully implemented a bench-top prototype following the proposed \u2026", "total_citations": 7, "citation_graph": {"2021": 2, "2022": 3, "2023": 2}}, {"title": "Visibility constrained generative model for depth-based 3D facial pose tracking", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:8AbLer7MMksC", "authors": ["Lu Sheng", "Jianfei Cai", "Tat-Jen Cham", "Vladimir Pavlovic", "King Ngi Ngan"], "publication_date": "2018/10/23", "journal": "IEEE transactions on pattern analysis and machine intelligence", "description": "In this paper, we propose a generative framework that unifies depth-based 3D facial pose tracking and face model adaptation on-the-fly, in the unconstrained scenarios with heavy occlusions and arbitrary facial expression variations. Specifically, we introduce a statistical 3D morphable model that flexibly describes the distribution of points on the surface of the face model, with an efficient switchable online adaptation that gradually captures the identity of the tracked subject and rapidly constructs a suitable face model when the subject changes. Moreover, unlike prior art that employed ICP-based facial pose estimation, to improve robustness to occlusions, we propose a ray visibility constraint that regularizes the pose based on the face model's visibility with respect to the input point cloud. Ablation studies and experimental results on Biwi and ICT-3DHP datasets demonstrate that the proposed framework is effective \u2026", "total_citations": 7, "citation_graph": {"2020": 3, "2021": 1, "2022": 0, "2023": 2}}, {"title": "Structure-aware multimodal feature fusion for RGB-D scene classification and beyond", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:vRqMK49ujn8C", "authors": ["Anran Wang", "Jianfei Cai", "Jiwen Lu", "Tat-Jen Cham"], "publication_date": "2018/5/22", "journal": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)", "description": "While convolutional neural networks (CNNs) have been excellent for object recognition, the greater spatial variability in scene images typically means that the standard full-image CNN features are suboptimal for scene classification. In this article, we investigate a framework allowing greater spatial flexibility, in which the Fisher vector (FV)-encoded distribution of local CNN features, obtained from a multitude of region proposals per image, is considered instead. The CNN features are computed from an augmented pixel-wise representation consisting of multiple modalities of RGB, HHA, and surface normals, as extracted from RGB-D data. More significantly, we make two postulates: (1) component sparsity\u2014that only a small variety of region proposals and their corresponding FV GMM components contribute to scene discriminability, and (2) modal nonsparsity\u2014that features from all modalities are encouraged to \u2026", "total_citations": 7, "citation_graph": {"2019": 3, "2020": 2, "2021": 2}}, {"title": "Click4BuildingID@ NTU: Click for Building Identification with GPS-enabled Camera Cell Phone", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:hC7cP41nSMkC", "authors": ["Chai Kiat Yeo", "Liang-Tien Chia", "Tat-Jen Cham", "Deepu Rajan"], "publication_date": "2007/7/2", "conference": "2007 IEEE International Conference on Multimedia and Expo", "description": "A working prototype of a building identification service which can be used on any camera cell phones equipped with GPS capability has been developed. Users can simply snap photos of architectures and send them, together with the corresponding GPS coordinates, via MMS to a remote server. The server will match the photos with the stored, GPS-tagged images using a combination of scale saliency algorithm for feature matching and earth movers distance measure for scene matching. The estimated location and other information are then sent back to the users via MMS. This prototype will have better accuracy than systems which rely solely on photo recognition given the exploitation of GPS information. Moreover, it is computationally lighter since the recognition engine only needs to compare stored images which lie within the GPS coordinates error range. It is relatively inexpensive as no special phones or \u2026", "total_citations": 7, "citation_graph": {"2008": 1, "2009": 0, "2010": 0, "2011": 0, "2012": 1, "2013": 1, "2014": 1, "2015": 0, "2016": 0, "2017": 0, "2018": 0, "2019": 0, "2020": 0, "2021": 1, "2022": 1, "2023": 1}}, {"title": "Video editing using figure tracking and image-based rendering", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:_kc_bZDykSQC", "authors": ["James M Rehg", "Sing Bing Kang", "Tat-Jen Cham"], "publication_date": "2000/9/10", "conference": "Image Processing, 2000. Proceedings. 2000 International Conference on", "description": "We describe a new approach to video editing based on the semi-automatic segmentation of video into multiple layers and the composition of layers using image-based rendering. Using figure tracking and background motion estimation, we can segment a moving figure and reconstruct the background. Using geometrically-correct pixel reprojection, layers can be composited on the basis of the geometry of the underlying scene and the position of a virtual camera. We have implemented a prototype editing system called SpliceWorld.", "total_citations": 7, "citation_graph": {"2000": 1, "2001": 0, "2002": 0, "2003": 2, "2004": 0, "2005": 1, "2006": 1, "2007": 0, "2008": 1, "2009": 1}}, {"title": "Entry-flipped transformer for inference and prediction of participant behavior", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:7T2F9Uy0os0C", "authors": ["Bo Hu", "Tat-Jen Cham"], "publication_date": "2022/10/23", "book": "European Conference on Computer Vision", "description": "Some group activities, such as team sports and choreographed dances, involve closely coupled interaction between participants. Here we investigate the tasks of inferring and predicting participant behavior, in terms of motion paths and actions, under such conditions. We narrow the problem to that of estimating how a set target participants react to the behavior of other observed participants. Our key idea is to model the spatio-temporal relations among participants in a manner that is robust to error accumulation during frame-wise inference and prediction. We propose a novel Entry-Flipped Transformer (EF-Transformer), which models the relations of participants by attention mechanisms on both spatial and temporal domains. Unlike typical transformers, we tackle the problem of error accumulation by flipping the order of query, key, and value entries, to increase the importance and fidelity of observed features in the \u2026", "total_citations": 5, "citation_graph": {"2023": 5}}, {"title": "High-quality pluralistic image completion via code shared VQGAN", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:Fu2w8maKXqMC", "authors": ["Chuanxia Zheng", "Guoxian Song", "Tat-Jen Cham", "Jianfei Cai", "Dinh Phung", "Linjie Luo"], "publication_date": "2022/4/5", "journal": "arXiv preprint arXiv:2204.01931", "description": "PICNet pioneered the generation of multiple and diverse results for image completion task, but it required a careful balance between loss (diversity) and reconstruction loss (quality), resulting in a limited diversity and quality . Separately, iGPT-based architecture has been employed to infer distributions in a discrete space derived from a pixel-level pre-clustered palette, which however cannot generate high-quality results directly. In this work, we present a novel framework for pluralistic image completion that can achieve both high quality and diversity at much faster inference speed. The core of our design lies in a simple yet effective code sharing mechanism that leads to a very compact yet expressive image representation in a discrete latent domain. The compactness and the richness of the representation further facilitate the subsequent deployment of a transformer to effectively learn how to composite and complete a masked image at the discrete code domain. Based on the global context well-captured by the transformer and the available visual regions, we are able to sample all tokens simultaneously, which is completely different from the prevailing autoregressive approach of iGPT-based works, and leads to more than 100 faster inference speed. Experiments show that our framework is able to learn semantically-rich discrete codes efficiently and robustly, resulting in much better image reconstruction quality. Our diverse image completion framework significantly outperforms the state-of-the-art both quantitatively and qualitatively on multiple benchmark datasets.", "total_citations": 5, "citation_graph": {"2022": 3, "2023": 2}}, {"title": "Visiting the invisible: Layer-by-layer completed scene decomposition", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:ZuybSZzF8UAC", "authors": ["Chuanxia Zheng", "Duy-Son Dao", "Guoxian Song", "Tat-Jen Cham", "Jianfei Cai"], "publication_date": "2021/12", "journal": "International Journal of Computer Vision", "description": "Existing scene understanding systems mainly focus on recognizing the visible parts of a scene, ignoring the intact appearance of physical objects in the real-world. Concurrently, image completion has aimed to create plausible appearance for the invisible regions, but requires a manual mask as input. In this work, we propose a higher-level scene understanding system to tackle both visible and invisible parts of objects and backgrounds in a given scene. Particularly, we built a system to decompose a scene into individual objects, infer their underlying occlusion relationships, and even automatically learn which parts of the objects are occluded that need to be completed. In order to disentangle the occluded relationships of all objects in a complex scene, we use the fact that the front object without being occluded is easy to be identified, detected, and segmented. Our system interleaves the two tasks of \u2026", "total_citations": 5, "citation_graph": {"2022": 1, "2023": 4}}, {"title": "Recovering facial reflectance and geometry from multi-view images", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:J-pR_7NvFogC", "authors": ["Guoxian Song", "Jianmin Zheng", "Jianfei Cai", "Tat-Jen Cham"], "publication_date": "2020/4/1", "journal": "Image and Vision Computing", "description": "While the problem of estimating shapes and diffuse reflectances of human faces from images has been extensively studied, there is relatively less work done on recovering the specular albedo. This paper presents a lightweight solution for inferring photorealistic facial reflectance and geometry. Our system processes video streams from two views of a subject, and outputs two reflectance maps for diffuse and specular albedos, as well as a vector map of surface normals. A model-based optimization approach is used, consisting of the three stages of multi-view face model fitting, facial reflectance inference and facial geometry refinement. Our approach is based on a novel formulation built upon the 3D morphable model (3DMM) for representing 3D textured faces in conjunction with the Blinn-Phong reflection model. It has the advantage of requiring only a simple setup with two video streams, and is able to exploit the \u2026", "total_citations": 5, "citation_graph": {"2021": 3, "2022": 2}}, {"title": "FaceCollage: A rapidly deployable system for real-time head reconstruction for on-the-go 3D telepresence", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:tOudhMTPpwUC", "authors": ["Fuwen Tan", "Chi-Wing Fu", "Teng Deng", "Jianfei Cai", "Tat-Jen Cham"], "publication_date": "2017/10/19", "book": "Proceedings of the 25th ACM international conference on Multimedia", "description": "This paper presents FaceCollage, a robust and real-time system for head reconstruction that can be used to create easy-to-deploy telepresence systems, using a pair of consumer-grade RGBD cameras that provide a wide range of views of the reconstructed user. A key feature is that the system is very simple to rapidly deploy, with autonomous calibration and requiring minimal intervention from the user, other than casually placing the cameras. This system is realized through three technical contributions: (1) a fully automatic calibration method, which analyzes and correlates the left and right RGBD faces just by the face features; (2) an implementation that exploits the parallel computation capability of GPU throughout most of the system pipeline, in order to attain real-time performance; and (3) a complete integrated system on which we conducted various experiments to demonstrate its capability, robustness, and \u2026", "total_citations": 5, "citation_graph": {"2018": 2, "2019": 0, "2020": 1, "2021": 1, "2022": 1}}, {"title": "Multiple consumer-grade depth camera registration using everyday objects", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:WbkHhVStYXYC", "authors": ["Teng Deng", "Jianfei Cai", "Tat-Jen Cham", "Jianmin Zheng"], "publication_date": "2017/6/1", "journal": "Image and Vision Computing", "description": "The registration of multiple consumer-grade depth sensors is a challenging task due to noisy and systematic distortions in depth measurements. Most of the existing works heavily rely on large number of checkerboard observations for calibration and registration of multiple depth cameras, which is tedious and not flexible. In this paper, we propose a more practical method for conducting and maintaining registration of multi-depth sensors, via replacing checkerboards with everyday objects found in the scene, such as regular furniture. Particularly, high quality pre-scanned 3D shapes of standard furniture are used as calibration targets. We propose a unified framework that jointly computes the optimal extrinsic calibration and depth correction parameters. Experimental results show that our proposed method significantly outperforms state-of-the-art depth camera registration methods.", "total_citations": 5, "citation_graph": {"2017": 1, "2018": 1, "2019": 0, "2020": 0, "2021": 1, "2022": 2}}, {"title": "Alignment of 3D models to images using region-based mutual information and neighborhood extended gaussian images", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:mVmsd5A6BfQC", "authors": ["Hon-Keat Pong", "Tat-Jen Cham"], "publication_date": "2006/1/13", "book": "Asian Conference on Computer Vision", "description": "Mutual information has been used for matching and registering 3D models to 2D images. However, in Viola\u2019s original framework [1], surface albedo variance is assumed to be minimal when measuring similarity between 3D models and 2D image data using mutual information. In reality, most objects have textured surfaces with different albedo values across their surfaces, and direct application of this method in such circumstances will fail. To solve this problem, we propose to include spatial information into the original formulation by using histogram-based features of local regions that are robust to local but significant albedo variation. Neighborhood Extended Gaussian Images (NEGI) are used as descriptors to represent local surface regions on the 3D model, while pixel intensity data are considered within corresponding region windows on the image. Experiments on aligning 3D car models in cluttered \u2026", "total_citations": 5, "citation_graph": {"2007": 1, "2008": 0, "2009": 1, "2010": 1, "2011": 1, "2012": 1}}, {"title": "Shading\u2010Based Surface Recovery Using Subdivision\u2010Based Representation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:geHnlv5EZngC", "authors": ["Teng Deng", "Jianmin Zheng", "Jianfei Cai", "Tat\u2010Jen Cham"], "publication_date": "2019/2", "journal": "Computer Graphics Forum", "description": "This paper presents subdivision\u2010based representations for both lighting and geometry in shape\u2010from\u2010shading. A very recent shading\u2010based method introduced a per\u2010vertex overall illumination model for surface reconstruction, which has advantage of conveniently handling complicated lighting condition and avoiding explicit estimation of visibility and varied albedo. However, due to its discrete nature, the per\u2010vertex overall illumination requires a large amount of memory and lacks intrinsic coherence. To overcome these problems, in this paper we propose to use classic subdivision to define the basic smooth lighting function and surface, and introduce additional independent variables into the subdivision to adaptively model sharp changes of illumination and geometry. Compared to previous works, the new model not only preserves the merits of the per\u2010vertex illumination model, but also greatly reduces the \u2026", "total_citations": 4, "citation_graph": {"2018": 1, "2019": 0, "2020": 2, "2021": 0, "2022": 1}}, {"title": "Incorporating local and global information using a novel distance function for scene recognition", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:3s1wT3WcHBgC", "authors": ["Elahe Farahzadeh", "Tat-jen Cham", "Wanqing Li"], "publication_date": "2013/1/15", "conference": "2013 IEEE Workshop on Robot Vision (WORV)", "description": "In the field of scene recognition using only one type of visual feature is not powerful enough to discriminate scene categories. In this paper we propose an innovative method to integrate global and local feature space into a map function based on a novel distance function. A subset of train images denoted as exemplar-set are selected. The local and global distances are defined according to the images in the exemplar-set. Distances are defined such that they indicate the contribution of different semantic aspects and global information in each scene category. An empirical study has been performed on the 15-Scene dataset in order to demonstrate the impact of appropriately incorporating both local and global information for the purpose of scene recognition. The experiments show, our model achieved state-of-the-art accuracy of 87.47.", "total_citations": 4, "citation_graph": {"2014": 2, "2015": 1, "2016": 1}}, {"title": "Object detection using a cascade of 3d models", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:L8Ckcad2t8MC", "authors": ["Hon-Keat Pong", "Tat-Jen Cham"], "publication_date": "2006/1/13", "book": "Asian Conference on Computer Vision", "description": "We present an alignment framework for object detection using a hierarchy of 3D polygonal models. One difficulty with alignment methods is that the high-dimensional transformation space makes finding potential candidate states a time-consuming task. This is an important consideration in our approach, as an exhaustive search is applied on a densely-sampled state space in order to avoid local minima and to extract all possible candidates. In our framework, a level-of-detail (LOD) 3D geometric model hierarchy is generated for the target object. Each of this model acts as a classifier to determine which of the discrete states are potential candidates. The classification is done through the estimation of pixel and edge-based mutual information between the 3D model and the image, where the classification speed significantly depends on the LOD and resolution of the image. By combining these models of \u2026", "total_citations": 4, "citation_graph": {"2006": 2, "2007": 0, "2008": 0, "2009": 0, "2010": 0, "2011": 1}}, {"title": "Geometric representation and grouping of image curves", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:aqlVkmm33-oC", "authors": ["Tat-Jen Cham"], "publication_date": "1996/8", "description": "Perceptual organisation, while acknowledged to be important to computer vision, is often based on heuristics rather than geometrical methods. This thesis highlights the role which geometry plays in perceptual organisation including the geometric representation of image curves and the grouping of geometrically-related curves.\nFirst, the representation of image curves via B-splines is considered. Current methods are de cient because they do not fully take into account the main issues of spline tting which include selecting the number and distribution of control points, and the parameterisation and sampling of the data points. By addressing these issues with a combination of active contours, the minimum description length principle and a structural-mechanics-inspired control point insertion strategy, superior results are obtained. Having provided a geometric representation for image curves, the grouping of curves related by a ne symmetry transformations is investigated. Means for quantifying the local symmetry of contour points based on uncertainty analysis of contour di erential properties is provided, together with a spatial representation for the local symmetries and a method for recovering the global symmetry con guration. It is also shown how point correspondence hypotheses between geometrically-related curves can be localised by using the curve neighbourhood around the correspondences, and quantitative measures for both the localisability and accuracy of these correspondences are proposed. Salient correspondences which are accurate and highly localisable are thus used as theseeds' for grouping the remainder of the curves. Finally \u2026", "total_citations": 4, "citation_graph": {"1997": 1, "1998": 1, "1999": 2}}, {"title": "Ipo-ldm: Depth-aided 360-degree indoor rgb panorama outpainting via latent diffusion model", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:yB1At4FlUx8C", "authors": ["Tianhao Wu", "Chuanxia Zheng", "Tat-Jen Cham"], "publication_date": "2023/7/6", "journal": "arXiv preprint arXiv:2307.03177", "description": "Generating complete 360-degree panoramas from narrow field of view images is ongoing research as omnidirectional RGB data is not readily available. Existing GAN-based approaches face some barriers to achieving higher quality output, and have poor generalization performance over different mask types. In this paper, we present our 360-degree indoor RGB panorama outpainting model using latent diffusion models (LDM), called IPO-LDM. We introduce a new bi-modal latent diffusion structure that utilizes both RGB and depth panoramic data during training, but works surprisingly well to outpaint normal depth-free RGB images during inference. We further propose a novel technique of introducing progressive camera rotations during each diffusion denoising step, which leads to substantial improvement in achieving panorama wraparound consistency. Results show that our IPO-LDM not only significantly outperforms state-of-the-art methods on RGB panorama outpainting, but can also produce multiple and diverse well-structured results for different types of masks.", "total_citations": 3, "citation_graph": {"2023": 3}}, {"title": "Explicit Correspondence Matching for Generalizable Neural Radiance Fields", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:XD-gHx7UXLsC", "authors": ["Yuedong Chen", "Haofei Xu", "Qianyi Wu", "Chuanxia Zheng", "Tat-Jen Cham", "Jianfei Cai"], "publication_date": "2023/4/24", "journal": "arXiv preprint arXiv:2304.12294", "description": "We present a new generalizable NeRF method that is able to directly generalize to new unseen scenarios and perform novel view synthesis with as few as two source views. The key to our approach lies in the explicitly modeled correspondence matching information, so as to provide the geometry prior to the prediction of NeRF color and density for volume rendering. The explicit correspondence matching is quantified with the cosine similarity between image features sampled at the 2D projections of a 3D point on different views, which is able to provide reliable cues about the surface geometry. Unlike previous methods where image features are extracted independently for each view, we consider modeling the cross-view interactions via Transformer cross-attention, which greatly improves the feature matching quality. Our method achieves state-of-the-art results on different evaluation settings, with the experiments showing a strong correlation between our learned cosine feature similarity and volume density, demonstrating the effectiveness and superiority of our proposed method. Code is at https://github.com/donydchen/matchnerf", "total_citations": 3, "citation_graph": {"2023": 3}}, {"title": "Towards Efficient 3D Calibration for Different Types of Multi-view Autostereoscopic 3D Displays", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:4fKUyHm3Qg0C", "authors": ["Xinxing Xia", "Yunqing Guan", "Andrei State", "Tat-Jen Cham", "Henry Fuchs"], "publication_date": "2018/6/11", "book": "Proceedings of Computer Graphics International 2018", "description": "A novel and efficient 3D calibration method for different types of autostereoscopic multi-view 3D displays is presented in this paper. In our method, a camera is placed at different locations within the viewing volume of a 3D display to capture a series of images that relate to the subset of light rays emitted by the 3D display and arriving at each of the camera positions. Gray code patterns modulate the images shown on the 3D display, helping to significantly reduce the number of images captured by the camera and thereby accelerate the process of calculating the correspondence relationship between the pixels on the 3D display and the locations of the capturing camera. The proposed calibration method has been successfully tested on two different types of multi-view 3D displays and can be easily generalized for calibrating other types of such displays. The experimental results show that this novel 3D calibration \u2026", "total_citations": 3, "citation_graph": {"2020": 1, "2021": 1, "2022": 1}}, {"title": "Scene recognition by semantic visual words", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:a0OBvERweLwC", "authors": ["Elahe Farahzadeh", "Tat-Jen Cham", "Andrzej Sluzek"], "publication_date": "2015/11", "journal": "Signal, Image and Video Processing", "description": "In this paper, we propose a novel approach to introduce semantic relations into the bag-of-words framework. We use the latent semantic models, such as latent semantic analysis (LSA) and probabilistic latent semantic analysis (pLSA), in order to define semantically rich features and embed the visual features into a semantic space. The semantic features used in LSA technique are derived from the low-rank approximation of word\u2013image occurrence matrix by singular value decomposition. Similarly, by using the pLSA approach, the topic-specific distributions of words can be considered dimensions of a concept space. In the proposed space, the distances between words represent the semantic distances which are used for constructing a discriminative and semantically meaningful vocabulary. Position information significantly improves scene recognition accuracy. Inspired by this, in this paper, we bring \u2026", "total_citations": 3, "citation_graph": {"2015": 1, "2016": 0, "2017": 0, "2018": 1, "2019": 0, "2020": 0, "2021": 0, "2022": 0, "2023": 1}}, {"title": "Shadow elimination and occluder light suppression for switched multi-projector displays", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:ZeXyd9-uunAC", "authors": ["Matthew Flagg", "Jay Summet", "Ramswaroop Somani", "James M Rehg", "Rahul Sukthankar", "Tat-Jen Cham"], "publication_date": "2003", "journal": "Ninth Internationl Conference on Computer Vision (ICCV\u201903)", "description": "Two related problems of front projection displays which occur when users obscure a projector are:(i) undesirable shadows cast on the display by the users, and (ii) projected light falling on and distracting the users. Our system uses multiple, conventional projectors which are positioned so that their projections overlap on the selected display surface to produce shadow-free displays even in the presence of multiple, moving occluders. Furthermore, projector light cast on the occluders is suppressed without affecting the quality of the display.\nThis demonstration is a two projector binary switching system where each pixel on the screen is illuminated by only one projector at any one time. If there is any observed deviation in pixel value from the reference pixel value, captured during initial calibration, it may be deduced that the illuminating projector is occluded without active probing. The system immediately blanks the pixel \u2026", "total_citations": 3, "citation_graph": {"2005": 2, "2006": 0, "2007": 0, "2008": 1}}, {"title": "Self-Calibrating Camera Projector Systems for Interactive Displays and Presentations.", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:7PzlFSSx8tAC", "authors": ["Rahul Sukthankar", "Tat-Jen Cham", "Gita Sukthankar", "James M Rehg", "David Hsu", "Thomas Leung"], "publication_date": "2001/7/1", "conference": "ICCV", "description": "We demonstrate a self-calibrating system that employs uncalibrated cameras and microportable projectors to create novel interactive displays and presentations. 1 Three benefits of our system are detailed in the following sections.", "total_citations": 3, "citation_graph": {"2003": 1, "2004": 0, "2005": 0, "2006": 0, "2007": 0, "2008": 0, "2009": 0, "2010": 1, "2011": 1}}, {"title": "ABLE-NeRF: Attention-Based Rendering with Learnable Embeddings for Neural Radiance Field", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:t7zJ5fGR-2UC", "authors": ["Zhe Jun Tang", "Tat-Jen Cham", "Haiyu Zhao"], "publication_date": "2023", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "description": "Neural Radiance Field (NeRF) is a popular method in representing 3D scenes by optimising a continuous volumetric scene function. Its large success which lies in applying volumetric rendering (VR) is also its Achilles' heel in producing view-dependent effects. As a consequence, glossy and transparent surfaces often appear murky. A remedy to reduce these artefacts is to constrain this VR equation by excluding volumes with back-facing normal. While this approach has some success in rendering glossy surfaces, translucent objects are still poorly represented. In this paper, we present an alternative to the physics-based VR approach by introducing a self-attention-based framework on volumes along a ray. In addition, inspired by modern game engines which utilise Light Probes to store local lighting passing through the scene, we incorporate Learnable Embeddings to capture view dependent effects within the scene. Our method, which we call ABLE-NeRF, significantly reduces' blurry'glossy surfaces in rendering and produces realistic translucent surfaces which lack in prior art. In the Blender dataset, ABLE-NeRF achieves SOTA results and surpasses Ref-NeRF in all 3 image quality metrics PSNR, SSIM, LPIPS.", "total_citations": 2, "citation_graph": {"2023": 2}}, {"title": "Half\u2010body Portrait Relighting with Overcomplete Lighting Representation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:PR6Y55bgFSsC", "authors": ["Guoxian Song", "Tat\u2010Jen Cham", "Jianfei Cai", "Jianmin Zheng"], "publication_date": "2021/9", "journal": "Computer Graphics Forum", "description": "We present a neural\u2010based model for relighting a half\u2010body portrait image by simply referring to another portrait image with the desired lighting condition. Rather than following classical inverse rendering methodology that involves estimating normals, albedo and environment maps, we implicitly encode the subject and lighting in a latent space, and use these latent codes to generate relighted images by neural rendering. A key technical innovation is the use of a novel overcomplete lighting representation, which facilitates lighting interpolation in the latent space, as well as helping regularize the self\u2010organization of the lighting latent space during training. In addition, we propose a novel multiplicative neural render that more effectively combines the subject and lighting latent codes for rendering. We also created a large\u2010scale photorealistic rendered relighting dataset for training, which allows our model to generalize \u2026", "total_citations": 2, "citation_graph": {"2021": 1, "2022": 1}}, {"title": "Weakly-supervised unconstrained action unit detection via feature disentanglement", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:D_sINldO8mEC", "authors": ["Zhiwen Shao", "Jianfei Cai", "Tat-Jen Cham", "Xuequan Lu", "Lizhuang Ma"], "publication_date": "2019", "journal": "arXiv preprint arXiv:1903.10143", "total_citations": 2, "citation_graph": {"2020": 1, "2021": 1}}, {"title": "SubdSH: Subdivision-based Spherical Harmonics Field for Real-time Shading-based Refinement under Challenging Unknown Illumination", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:eflP2zaiRacC", "authors": ["Teng Deng", "Jianmin Zheng", "Jianfei Cai", "Tat-Jen Cham"], "publication_date": "2018/12/9", "conference": "2018 IEEE Visual Communications and Image Processing (VCIP)", "description": "This paper presents a spatial-varying illumination model for shading-based depth refinement that based on a smooth Spherical Harmonics (SH) lighting field. The proposed lighting model is able to recover shading under challenging unknown lighting conditions, thus improving the quality of recovered surface detail. To avoid over-parameterization, local lighting coefficients are treated as a vector-valued function which is represented by subdivided surfaces using Catmull-Clark subdivision. We solve our lighting model utilizing a highly parallelized scheme that recovers lighting in a few milliseconds. A real-time shading-based depth recovery system is implemented with the integration of our proposed lighting model. We conduct quantitative and qualitative evaluations on both synthetic and real world datasets under challenging illumination. The experimental results show our method outperforms the state-of-the-art real \u2026", "total_citations": 2, "citation_graph": {"2020": 1, "2021": 1}}, {"title": "Steerable second order intensity features for pedestrian detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:OU6Ihb5iCvQC", "authors": ["Sing Kuang Tan", "Tat-Jen Cham", "Jianxin Wu"], "publication_date": "2015/11/3", "conference": "2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)", "description": "In this paper we consider how second order intensity features can be used to boost the accuracies of SVM-based pedestrian detectors, which remain much faster than recently popular deep learning approaches. In particular, we demonstrate that combining second order information features, corresponding to Hessians of patch-wise image intensities, with HOG and LBP features leads to more than 10% improvement in accuracy for frequently used and difficult datasets. In addition, we present a framework to visualize the responses of the linear SVM classifier at different locations and for different feature types, which enables a comprehensive and detailed analysis of failure modes of the current HOG-LBP detector. An interesting observation made is that the weight patterns of the Hessian-based features change when combined with HOG and LBP features as compared to using Hessians only, and that these changes \u2026", "total_citations": 2, "citation_graph": {"2019": 1, "2020": 1}}, {"title": "Robust performance-driven 3d face tracking in long range depth scenes", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:abG-DnoFyZgC", "authors": ["Hai X Pham", "Chongyu Chen", "Luc N Dao", "Vladimir Pavlovic", "Jianfei Cai", "Tat-jen Cham"], "publication_date": "2015/7/10", "journal": "arXiv preprint arXiv:1507.02779", "description": "We introduce a novel robust hybrid 3D face tracking framework from RGBD video streams, which is capable of tracking head pose and facial actions without pre-calibration or intervention from a user. In particular, we emphasize on improving the tracking performance in instances where the tracked subject is at a large distance from the cameras, and the quality of point cloud deteriorates severely. This is accomplished by the combination of a flexible 3D shape regressor and the joint 2D+3D optimization on shape parameters. Our approach fits facial blendshapes to the point cloud of the human head, while being driven by an efficient and rapid 3D shape regressor trained on generic RGB datasets. As an on-line tracking system, the identity of the unknown user is adapted on-the-fly resulting in improved 3D model reconstruction and consequently better tracking performance. The result is a robust RGBD face tracker, capable of handling a wide range of target scene depths, beyond those that can be afforded by traditional depth or RGB face trackers. Lastly, since the blendshape is not able to accurately recover the real facial shape, we use the tracked 3D face model as a prior in a novel filtering process to further refine the depth map for use in other tasks, such as 3D reconstruction.", "total_citations": 2, "citation_graph": {"2016": 1, "2017": 0, "2018": 0, "2019": 0, "2020": 1}}, {"title": "MPT-Net: Mask Point Transformer Network for Large Scale Point Cloud Semantic Segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:j8SEvjWlNXcC", "authors": ["Zhe Jun Tang", "Tat-Jen Cham"], "publication_date": "2022/10/23", "conference": "2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "description": "Point cloud semantic segmentation is important for road scene perception, a task for driverless vehicles to achieve full fledged autonomy. In this work, we introduce Mask Point Transformer Network (MPT-Net), a novel architecture for point cloud segmentation that is simple to implement. MPT-Net consists of a local and global feature encoder and a transformer based decoder; a 3D Point-Voxel Convolution encoder backbone with voxel self attention to encode features and a Mask Point Transformer module to decode point features and segment the point cloud. Firstly, we introduce the novel MPT designed to specifically handle point cloud segmentation. MPT offers two benefits. It attends to every point in the point cloud using mask tokens to extract class specific features globally with cross attention, and provide inter-class feature information exchange using self attention on the learned mask tokens. Secondly, we \u2026", "total_citations": 1, "citation_graph": {"2023": 1}}, {"title": "Real-time shadow-aware portrait relighting in virtual backgrounds for realistic telepresence", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:uJ-U7cs_P_0C", "authors": ["Guoxian Song", "Tat-Jen Cham", "Jianfei Cai", "Jianmin Zheng"], "publication_date": "2022/10/17", "conference": "2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)", "description": "While using virtual backgrounds has recently become a very popular feature in videoconferencing, there often exists a jarring mismatch between the lighting of the user and the illumination condition of the virtual background. Existing portrait relighting methods can alleviate the problem, but do not have the capacity to deal with difficult shadow effects. In this paper, we present a new shadow-aware portrait relighting system that can relight an input portrait to be consistent with a given desired background image with shadow effects. Our system consists of four major components: portrait neutralization, illumination estimation, shadow generation and hierarchical neural rendering, which are all based on deep neural networks, and the whole system is end-to-end trainable. In addition, we created a large-scale photorealistic synthetic dataset with shadow, illumination and depth annotations for training, which allows our \u2026", "total_citations": 1, "citation_graph": {"2021": 1}}, {"title": "Determination of nuclear position by the arrangement of actin filaments using deep generative networks", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:uLbwQdceFCQC", "authors": ["Jyothsna Vasudevan", "Chuanxia Zheng", "James G Wan", "Tat-Jen Cham", "Lim Chwee Teck", "Javier G Fernandez"], "publication_date": "2021/11/15", "journal": "bioRxiv", "description": "The cell nucleus is a dynamic structure that changes locales during cellular processes such as proliferation, differentiation, or migration, and its mispositioning is a hallmark of several disorders,. As with most mechanobiological activities of adherent cells, the repositioning and anchoring of the nucleus are presumed to be associated with the organization of the cytoskeleton, the network of protein filaments providing structural integrity to the cells. However, this correlation between cytoskeleton organization and nuclear position has not, to date, been demonstrated, as it would require the parameterization of the extraordinarily intricate cytoskeletal fiber arrangements. Here, we show that this parameterization and demonstration can be achieved outside the limits of human conceptualization, using generative network and raw microscope images, relying on machine-driven interpretation and selection of parameterizable features. The developed transformer-based architecture was able to generate high-quality, completed images of more than 8,000 cells, using only information on actin filaments, predicting the presence of a nucleus and its exact localization in more than 70 per cent of instances. Our results demonstrate one of the most basic principles of mechanobiology with a remarkable level of significance. They also highlight the role of deep learning as a powerful tool in biology beyond data augmentation and analysis, capable of interpreting\u2014unconstrained by the principles of human reasoning\u2014complex biological systems from qualitative data.", "total_citations": 1, "citation_graph": {"2022": 1}}, {"title": "Progress regression RNN for online spatial-temporal action localization in unconstrained videos", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:VOx2b1Wkg3QC", "authors": ["Bo Hu", "Jianfei Cai", "Tat-Jen Cham", "Junsong Yuan"], "publication_date": "2019/3/1", "journal": "arXiv preprint arXiv:1903.00304", "description": "Previous spatial-temporal action localization methods commonly follow the pipeline of object detection to estimate bounding boxes and labels of actions. However, the temporal relation of an action has not been fully explored. In this paper, we propose an end-to-end Progress Regression Recurrent Neural Network (PR-RNN) for online spatial-temporal action localization, which learns to infer the action by temporal progress regression. Two new action attributes, called progression and progress rate, are introduced to describe the temporal engagement and relative temporal position of an action. In our method, frame-level features are first extracted by a Fully Convolutional Network (FCN). Subsequently, detection results and action progress attributes are regressed by the Convolutional Gated Recurrent Unit (ConvGRU) based on all the observed frames instead of a single frame or a short clip. Finally, a novel online linking method is designed to connect single-frame results to spatial-temporal tubes with the help of the estimated action progress attributes. Extensive experiments demonstrate that the progress attributes improve the localization accuracy by providing more precise temporal position of an action in unconstrained videos. Our proposed PR-RNN achieves the stateof-the-art performance for most of the IoU thresholds on two benchmark datasets.", "total_citations": 1, "citation_graph": {"2022": 1}}, {"title": "Merging live and pre-captured data to support full 3d head reconstruction for telepresence", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:u_35RYKgDlwC", "authors": ["C\u00e9dric Fleury", "Tiberiu Popa", "Tat Jen Cham", "Henry Fuchs"], "publication_date": "2014/4/7", "conference": "EG'14", "description": "This paper proposes a 3D head reconstruction method for low cost 3D telepresence systems that uses only a single consumer level hybrid sensor (color+depth) located in front of the users. Our method fuses the real-time, noisy and incomplete output of a hybrid sensor with a set of static, high-resolution textured models acquired in a calibration phase. A complete and fully textured 3D model of the users' head can thus be reconstructed in real-time, accurately preserving the facial expression of the user. The main features of our method are a mesh interpolation and a fusion of a static and a dynamic textures to combine respectively a better resolution and the dynamic features of the face.", "total_citations": 1, "citation_graph": {"2015": 1}}, {"title": "Robust Projected Displays for Ubiquitous Computing", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:bEWYMUwI8FkC", "authors": ["Jay W Summet", "Matthew Flagg", "Mark Ashdown", "Rahul Sukthankar", "James M Rehg", "Gregory D Abowd", "Tat Jen Cham"], "publication_date": "2004/8", "journal": "Ubicomp Workshop on Ubiquitous Display Environments", "description": "We believe projectors are currently the best technology for creating very large displays that are flexible, affordable, and easy to configure and deploy. Such displays have numerous applications in ubiquitous computing: generating a shared display for a spontaneous hallway meeting, placing a hidden display on a breakfast table behind a box of cereal for the private notification of an important message, or covering the front of a classroom with an interactive whiteboard. Recently there has been an explosion of interest in projector-based displays, and in particular on the use of vision technology for automated calibration and compensation [7]. Some representative examples are the Office of the Future project [9], the Everywhere Displays Projector [8], and using a projector to make one object look like another [3].", "total_citations": 1, "citation_graph": {"2011": 1}}, {"title": "Cocktail: Mixing Multi-Modality Controls for Text-Conditional Image Generation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:9Nmd_mFXekcC", "authors": ["Minghui Hu", "Jianbin Zheng", "Daqing Liu", "Chuanxia Zheng", "Chaoyue Wang", "Dacheng Tao", "Tat-Jen Cham"], "publication_date": "2023/6/1", "journal": "arXiv preprint arXiv:2306.00964", "description": "Text-conditional diffusion models are able to generate high-fidelity images with diverse contents. However, linguistic representations frequently exhibit ambiguous descriptions of the envisioned objective imagery, requiring the incorporation of additional control signals to bolster the efficacy of text-guided diffusion models. In this work, we propose Cocktail, a pipeline to mix various modalities into one embedding, amalgamated with a generalized ControlNet (gControlNet), a controllable normalisation (ControlNorm), and a spatial guidance sampling method, to actualize multi-modal and spatially-refined control for text-conditional diffusion models. Specifically, we introduce a hyper-network gControlNet, dedicated to the alignment and infusion of the control signals from disparate modalities into the pre-trained diffusion model. gControlNet is capable of accepting flexible modality signals, encompassing the simultaneous reception of any combination of modality signals, or the supplementary fusion of multiple modality signals. The control signals are then fused and injected into the backbone model according to our proposed ControlNorm. Furthermore, our advanced spatial guidance sampling methodology proficiently incorporates the control signal into the designated region, thereby circumventing the manifestation of undesired objects within the generated image. We demonstrate the results of our method in controlling various modalities, proving high-quality synthesis and fidelity to multiple external signals."}, {"title": "Artificial intelligence for gastroenterology: Singapore artificial intelligence for Gastroenterology Working Group Position Statement", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:nrtMV_XWKgEC", "authors": ["Joseph JY Sung", "Julian Savulescu", "KY Ngiam", "Bo An", "Tiing Leong Ang", "KG Yeoh", "Tat\u2010Jen Cham", "Stephen Tsao", "TS Chua"], "publication_date": "2023", "journal": "Journal of Gastroenterology and Hepatology", "description": "Background\nSuccessful implementation of artificial intelligence in gastroenterology and hepatology practice requires more than technology. There are ethical, legal, and social issues that need to be settled.\nAim\nA group consisting of AI developers (engineer), AI users (gastroenterologist, hepatologist, and surgeon) and AI regulators (ethicist and administrator) formed a Working Group to draft these Positions Statements with the objective of arousing public and professional interest and dialogue, to promote ethical considerations when implementing AI technology, to suggest to policy makers and health authorities relevant factors to take into account when approving and regulating the use of AI tools, and to engage the profession in preparing for change in clinical practice.\nStatements\nThese series of Position Statements point out the salient issues to maintain the trust between care provider and care receivers, and to \u2026"}, {"title": "From qualitative data to correlation using deep generative networks: Demonstrating the relation of nuclear position with the arrangement of actin filaments", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:-_dYPAW6P2MC", "authors": ["Jyothsna Vasudevan", "Chuanxia Zheng", "James G Wan", "Tat-Jen Cham", "Lim Chwee Teck", "Javier G Fernandez"], "publication_date": "2022/7/29", "journal": "Plos one", "description": "The cell nucleus is a dynamic structure that changes locales during cellular processes such as proliferation, differentiation, or migration, and its mispositioning is a hallmark of several disorders. As with most mechanobiological activities of adherent cells, the repositioning and anchoring of the nucleus are presumed to be associated with the organization of the cytoskeleton, the network of protein filaments providing structural integrity to the cells. However, demonstrating this correlation between cytoskeleton organization and nuclear position requires the parameterization of the extraordinarily intricate cytoskeletal fiber arrangements. Here, we show that this parameterization and demonstration can be achieved outside the limits of human conceptualization, using generative network and raw microscope images, relying on machine-driven interpretation and selection of parameterizable features. The developed transformer-based architecture was able to generate high-quality, completed images of more than 8,000 cells, using only information on actin filaments, predicting the presence of a nucleus and its exact localization in more than 70 per cent of instances. Our results demonstrate one of the most basic principles of mechanobiology with a remarkable level of significance. They also highlight the role of deep learning as a powerful tool in biology beyond data augmentation and analysis, capable of interpreting\u2014unconstrained by the principles of human reasoning\u2014complex biological systems from qualitative data."}, {"title": "Estimating spatial layout of rooms from RGB-D videos", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:D03iK_w7-QYC", "authors": ["Anran Wang", "Jiwen Lu", "Jianfei Cai", "Gang Wang", "Tat-Jen Cham"], "publication_date": "2014/9/22", "conference": "2014 IEEE 16th International Workshop on Multimedia Signal Processing (MMSP)", "description": "Spatial layout estimation of indoor rooms plays an important role in many visual analysis applications such as robotics and human-computer interaction. While many methods have been proposed for recovering spatial layout of rooms in recent years, their performance is still far from satisfactory due to high occlusion caused by the presence of objects that clutter the scene. In this paper, we propose a new approach to estimate the spatial layout of rooms from RGB-D videos. Unlike most existing methods which estimate the layout from still images, RGB-D videos provide more spatial-temporal and depth information, which are helpful to improve the estimation performance because more contextual information can be exploited in RGB-D videos. Given a RGB-D video, we first estimate the spatial layout of the scene in each single frame and compute the camera trajectory using the simultaneous localization and mapping \u2026"}, {"title": "Bounding the asymmetric error of a convex combination of classifiers.", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:ldfaerwXgEUC", "authors": ["Minh-Tri Pham", "Tat-Jen Cham"], "publication_date": "2012/12/18", "journal": "Journal of Computer Science and Cybernetics", "description": "Asymmetric error is an error that trades off between the false positive rate and the false negative rate of a binary classifier. It has been recently used in solving the imbalanced classification problem eg, in asymmetric boosting. However, to date, the relationship between an empirical asymmetric error and its generalization counterpart has not been addressed. Bounds on the classical generalization error are not directly applicable since different penalties are associated with the false positive rate and the false negative rate respectively, and the class probability is typically ignored in the training set. In this paper, we present a bound on the expected asymmetric error of any convex combination of classifiers based on its empirical asymmetric error. We also show that the bound is a generalization of one of the latest (and tightest) bounds on the classification error of the combined classifier."}, {"title": "Image relighting by analogy", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:NaGl4SEjCO4C", "authors": ["Xiao Teng", "Tat-Jen Cham"], "publication_date": "2011", "journal": "Advances in Visual Computing", "description": "We propose and analyze an example-based framework for relighting images. In this framework, there are a number of images of reference objects captured under different illumination conditions. Given an input image of a new object captured under one of the previously observed illumination conditions, new images can be synthesized for the input object under all the other illumination conditions that are present in the reference images. It does not require any other prior knowledge on the reference and target objects, except that they share the same albedo. Though it is appreciated if the reference objects have similar shape as the target object, sphere or ellipsoid which has plenty of local geometry samples are sufficient to build up a look-up table, as this method solves the problem locally. Gradient domain methods are introduced to finally generate visual-pleasing results. We demonstrate this framework \u2026"}, {"title": "High Distortion and Non-Structural Image Matching via Feature Co-occurrence", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:4JMBOYKVnBMC", "authors": ["Xi Chen", "Tat-Jen Cham"], "publication_date": "2007/6/17", "conference": "2007 IEEE Conference on Computer Vision and Pattern Recognition", "description": "We propose a novel approach for determining if a pair of images match each other under the effect of a high-distortion transformation or non-structural relation. The co-occurrence statistics between features across a pair of images are learned from a training set comprising matched and mismatched image pairs - these are expressed in the form of a cross-feature ratio table. The proposed method does not require feature-to-feature correspondences, but instead identifies and exploits feature co-occurrences that are able to provide discriminative result from the transformation. The method not only allows for the matching of test image pairs that have substantially different visual content as compared to those present in the training set, but also caters for transformations and relations that do not preserve image structure."}, {"title": "Proceedings of the 13th international conference on Multimedia Modeling-Volume Part I", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:_Ybze24A_UAC", "authors": ["Tat-Jen Cham", "Jianfei Cai", "Chitra Dorai", "Deepu Rajan", "Tat-Seng Chua"], "publication_date": "2007/1/9", "description": "Proceedings of the 13th international conference on Multimedia Modeling - Volume Part I | Guide Proceedings ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsMMM'07 ABSTRACT No abstract available. Comments Login options Check if you have access through your login credentials or your institution to get full access on this article. Sign in Full Access Get this Publication Information Contributors Published in Guide Proceedings cover image MMM'07: Proceedings of the 13th international conference on Multimedia Modeling - Volume Part I January 2007 796 pages ISBN:3540694218 Editors: Tat-Jen Cham , Jianfei Cai , \u2026"}, {"title": "Optimal Cascade Construction for Detection using 3D Models", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:iH-uZ7U-co4C", "authors": ["Hon-Keat Pong", "Tat-Jen Cham"], "publication_date": "2006/8/20", "conference": "18th International Conference on Pattern Recognition (ICPR'06)", "description": "We describe a method for optimal construction of a detection cascade comprising 3D models of increasing levelof- detail (LOD). An LOD 3D model hierarchy of the target object is first generated. By analyzing detection performance of each individual model in the LOD hierarchy, an optimization framework that allows trade-off between speed and accuracy is formulated. The formulation allows models to be explicitly selected for inclusion in the final detection cascade while achieving optimal running time with respect to a target detection performance."}, {"title": "Proactive Detection and Recovery of Lost Mobile Phones", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:-f6ydRqryjwC", "authors": ["Chen Hui Ong", "Nelly Kasim", "Sajindra Kolitha Bandara Jayasena", "Larry Rudolph", "Tat Jen Cham"], "publication_date": "2005", "description": "This paper describes the successful implementation of a prototype software application that independently and proactively detects whether a mobile phone is lost or misused. When the mobile phone is detected as being lost or misused, the application takes steps to mitigate the impact of loss and to gather evidence. The goal is to aid in the recovery of the mobile phone. The prototype works regardless of the cellular infrastructure the mobile phone is operating in and makes minimum demands on the owner of the mobile phone. The prototype was developed on Nokia 6600 mobile phones that run Symbian Operating System 7.0s. Development was done using Nokia\u2019s Series 60 Developer\u2019s Platform 2.0."}, {"title": "Interactive display walls based on camera-projector system", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:VLnqNzywnoUC", "authors": ["Tat Jen Cham"], "publication_date": "2003", "description": "Wall-sized displays are extremely valuable in many fields for visualizing complex processes and large quantities of data, as well as facilitating active collaboration among users. This research project is aimed at developing technologies for cre- ating display walls easily with inexpensive portable equipment, and dramatically enhancing these display walls by allowing users to naturally interact with the dis- plays as well as having the displays proactively adapt to users. These interactive display walls are implemented using multiple cost-effective video cameras and ultra-portable computer projectors, circumventing the need for special, expen- sive and cumbersome equipment. In this report, we document the achievements made during the course of the research project, and describe some of the ma- jor research and development work carried out. These include (i) a display wall based on casually-placed projectors, (ii) virtual persistent displays that are able to dynamically eliminate shadows cast by users blocking the projectors, and (iii) a framework for detecting and recognizing symbolic hand gestures. These research efforts are backed by interesting demonstrable research systems. Future research directions and subsequent deliverables are discussed at the end of the report."}, {"title": "A Local Approach to Recovering Global Skewed Symmetry in Images", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:RGFaLdJalmkC", "authors": ["Tat-Jen Cham", "Roberto Cipolla"], "publication_date": "1994"}, {"title": "Exploiting Spatial-temporal Relationships for 3D Pose Estimation via Graph Convolutional Networks Supplementary Document", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:9vf0nzSNQJEC", "authors": ["Yujun Cai", "Liuhao Ge", "Jun Liu", "Jianfei Cai", "Tat-Jen Cham", "Junsong Yuan", "Nadia Magnenat Thalmann"], "description": "In this supplementary document, we provide materials not included in the main paper due to space constraints. Firstly, Section 1 provides more details of our proposed network structures. Next, Section 2 elaborates on our quantitative results on Human3. 6M using the MPJPE metric under protocol# 1. Finally, Section 3 presents additional qualitative results for comparison."}, {"title": "CCI//or PC/DerS", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:HE397vMXCloC", "authors": ["ThelnstituteOf ElectronicSInformationand CommunicationEngineerSJapan", "Tal-Seng Chua", "Mohan Kankanhali", "Tat-Jen Cham", "Shin KOSugi", "HyOung-JOOng Kim", "Liang-Tien Chia", "InOridIf DieS"], "description": "\uae30\ud55c\ub2e4. 16. \uadf8\ub9bc\uc740 \ud2b8\ub808\uc774\uc2f1 \ud398\uc774\ud37c\ub098 \ubc31\uc9c0\uc5d0 \uba39\uc774\ub098 \ub808 E \ub808\ub9c1\uc73c\ub85c \uc120\uba85\ud558\uac8c \uadf8\ub9ac\ub3c4, \ud06c\uae30\ub294 \uac00\uae09\uc801 \ubc18\ub2e8 (\uac00\ub85c 7cm) \uc73c\ub85c \ud558\uace0 \uc6d0\uace0\uac00 \uc774 \ud06c\uae30\ubcf4\ub2e4 \ud074 \ub54c\ub294 \ucd95\uc18c\ub420 \uac83\uc744\uace0\ub824\ud558\uc5ec \uae00\uc790\uc758 \ud06c\uae30\ub97c \uc54c\ub9de\uac8c \uc120\uc815\ud55c\ub2e4. \uc804\ub2e8 (\uac00"}, {"title": "Associate Editor-in-Chief Hongbin Zha (Peking University) In So Kweon (KAIST) Sing Bing Kang (Microsoft Research Redmond)", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:q3oQSFYPqjQC", "authors": ["Yasushi Yagi", "Cha Zhang", "Chil-Woo Lee", "David Suter", "Guangyou Xu", "Guy Godin", "Hideo Saito", "Ian Reid", "Katsushi Ikeuchi", "Kazuhiko Sumi", "Koichiro Deguchi", "Kyoung Mu Lee", "Long Quan", "Marc Pollefeys", "PJ Narayanan", "Peter Sturm", "Reinhard Klette", "Rin-ichiro Taniguchi", "Ryusuke Sagawa", "Sang Wook Lee", "Shin\u2019ichi Satoh", "Tat-Jen Cham", "Tomas Pajdla", "Vincent Lepetit", "Yasuyuki Matsushita", "Yi-ping Hung", "Yoichi Sato"], "description": "\u60c5\u5831\u5b66\u5e83\u5834:\u60c5\u5831\u51e6\u7406\u5b66\u4f1a\u96fb\u5b50\u56f3\u66f8\u9928 \u97f3\u58f0\u30d6\u30e9\u30a6\u30b6\u5bfe\u5fdc\u30da\u30fc\u30b8\u3078 \u30b3\u30f3\u30c6\u30f3\u30c4\u30a8\u30ea\u30a2\u3078 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 \u304a\u77e5\u3089\u305b \u203b\u30e6\u30fc\u30b6\u767b\u9332\u306f\u7121\u6599\u3067\u3059. \u60c5\u5831\u5b66\u5e83\u5834\u306b\u63b2\u8f09\u3055\u308c\u3066\u3044\u308b\u30b3\u30f3\u30c6\u30f3\u30c4\u306b\u306f\u6709\u6599\u306e\u3082\u306e\u3082 \u542b\u307e\u308c\u3066\u3044\u307e\u3059. \u6709\u6599\u30b3\u30f3\u30c6\u30f3\u30c4\u3092\u3054\u8cfc\u5165\u3044\u305f\u3060\u3044\u305f\u5834\u5408\u3067\u3082,\u9818\u53ce\u66f8\u306e\u767a\u884c\u306f\u3044\u305f\u3057\u3066\u304a\u308a\u307e\u305b\u3093. \u30af\u30ec\u30b8\u30c3\u30c8\u30ab\u30fc\u30c9\u4f1a\u793e\u69d8\u304b\u3089\u306e\u9818\u53ce\u66f8/\u8acb\u6c42\u66f8\u3092\u3082\u3063\u3066\u304b\u3048\u3055\u305b\u3066\u3044\u305f\u3060\u3044\u3066\u304a\u308a\u307e\u3059. \u672c\u96fb\u5b50\u56f3\u66f8\u9928\u306e \u3054\u5229\u7528\u306b\u3042\u305f\u3063\u3066\u306f\u300c\u60c5\u5831\u51e6\u7406\u5b66\u4f1a\u96fb\u5b50\u56f3\u66f8\u9928\u5229\u7528\u898f\u7d04\u300d\u3092\u3054\u9075\u5b88\u4e0b\u3055\u3044. \u8907\u5199\u304a\u3088\u3073\u8ee2\u8f09\u3092 \u3055\u308c\u308b\u65b9\u3078 \u4e00\u822c\u793e\u56e3\u6cd5\u4eba\u60c5\u5831\u51e6\u7406\u5b66\u4f1a\u3067\u306f\u8907\u5199\u8907\u88fd\u304a\u3088\u3073\u8ee2\u8f09\u8907\u88fd\u306b\u4fc2\u308b\u8457\u4f5c\u6a29\u3092\u5b66\u8853\u8457\u4f5c\u6a29\u5354\u4f1a \u306b\u59d4\u8a17\u3057\u3066\u3044\u307e\u3059.\u5f53\u8a72\u5229\u7528\u3092\u3054\u5e0c\u671b\u306e\u65b9\u306f,\u5b66\u8853\u8457\u4f5c\u6a29\u5354\u4f1a\u304c\u63d0\u4f9b\u3057\u3066\u3044\u308b\u8907\u88fd\u5229\u7528\u8a31\u8afe\u30b7\u30b9\u30c6\u30e0 \u3082\u3057\u304f\u306f\u8ee2\u8f09\u8a31\u8afe\u30b7\u30b9\u30c6\u30e0\u3092\u901a\u3058\u3066\u7533\u8acb\u304f\u3060\u3055\u3044. \u5c1a,\u672c\u4f1a\u4f1a\u54e1(\u8cdb\u52a9\u4f1a\u54e1\u542b\u3080)\u304a\u3088\u3073\u8457\u8005\u304c\u8ee2\u8f09\u5229\u7528 \u306e\u7533\u8acb\u3092\u3055\u308c\u308b\u5834\u5408\u306b\u3064\u3044\u3066\u306f,\u5b66\u8853\u76ee\u7684\u5229\u7528\u306b\u9650\u308a,\u7121\u511f\u3067\u8ee2\u8f09\u5229\u7528\u3044\u305f\u3060\u304f\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059.\u305f\u3060\u3057 ,\u5229\u7528\u306e\u969b\u306b\u306f\u4e88\u3081\u7533\u8acb\u3044\u305f\u3060\u304f\u3088\u3046\u304a\u9858\u3044\u81f4\u3057\u307e\u3059. \u203b\u60c5\u5831\u51e6\u7406\u5b66\u4f1a\u767a\u884c\u306e\u520a\u884c\u7269\u306b\u63b2\u8f09\u3055\u308c\u3066\u3044\u308b\u2026"}, {"title": "Bai, Hongliang 20, 32 Blasi, Saverio 20 Bovik, Alan. C. 23", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:BrmTIyaxlBUC", "authors": ["Julian Cabreray", "Jianfei Cai", "Yutong Cai", "Patrick Le Callet", "Thuong Nguyen Canh", "Guiyan Cao", "Shuang Cao", "Tat-Jen Cham", "Chin-Wei Chang", "Li-Jen Chang", "Yen-Cheng Chang", "Chien-Hua Chen", "Chien-Yu Chen", "Feiyu Chen", "Jianhui Chen", "Jianle Chen", "Yung-Yao Chen", "Zhenzhong Chen", "Zhibo Chen", "Zhineng Chen", "Zhu Chen", "Jianjun Chen", "Keyang Cheng", "Ming-Ming Cheng", "Wen-Huang Cheng", "Zhaohui Chey", "Jui-Chiu Chiang", "Hsuan-Ting Chou", "Wei-Ta Chu", "Jen-Hui Chuang", "Tae-Sun Chung", "Luis A Cruz", "Chang Cui"], "description": "VCIP 2018 Author Index Page 1 35 Author Index [A] Aizawa, Tomoyoshi 25 Aminlou, Alireza 32 An, Ping 26 Antioquia, Arren Matthew C. 30 Asai, Tetsuya 29 Azcarraga, Arnulfo 30, 33 [B] Bai, Hongliang 20, 32 Blasi, Saverio 20 Bovik, Alan. C. 23 [C] Cabreray, Julian 20 Cai, Jianfei 23 Cai, Yutong 27 Callet, Patrick Le 18 Canh, Thuong Nguyen 24 Cao, Guiyan 32 Cao, Shuang 22 Cham, Tat-Jen 23 Chang, Chin-Wei 25 Chang, Li-Jen 26 Chang, Yen-Cheng 34 Chen, Chien-Hua 34 Chen, Chien-Yu 21 Chen, Feiyu 29 Chen, Jianhui 22 Chen, Jianle 19 Chen, Jianwen 23 Chen, Jinling 33 Chen, Jun 19 Chen, Kexin 20 Chen, Kuo-Wei 34 Chen, Li 27 Chen, Lin 20 Chen, Xin 31 Chen, Xuejin 24 Chen, Yan-Jhu 25 Chen, Yie-Tarng 27 Chen, Yu 21 Chen, Yung-Yao 25 Chen, Zhenzhong 20, 32 Chen, Zhibo 33 Chen, Zhineng 31 Chen, Zhu 30 Chen,Jianjun 26 Cheng, Keyang 22 Cheng, Ming-Ming 27 Cheng, Wen-Huang 25, \u2026"}, {"title": "SPECIAL ISSUE ON DEEP LEARNING", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:nb7KW1ujOQ8C", "authors": ["A Wang", "J Lu", "J Cai", "TJ Cham", "G Wang", "J Tang", "L Jin", "Z Li", "S Gao", "M Hasan", "AK Roy-Chowdhury"], "description": "Presents the table of contents for this issue of the publication."}, {"title": "Margin-based Bounds on an Asymmetric Error of a Combined Classifier", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&cstart=100&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:mB3voiENLucC", "authors": ["Minh-Tri Pham", "Tat-Jen Cham"], "description": "Asymmetric error is a kind of error that trades off between the false positive rate and the false negative rate of a binary classifier. It has been recently used in the imbalanced binary classification problem, where the probability of one class far outweighs that of the other. Classical bounds on the generalization error are not directly applicable to explain the ability to generalize of a binary classifier learned with an asymmetric error. Because in this context, there are different penalties associated with the false positive rate and the false negative rate respectively. In this paper, we present some margin-based bounds on an asymmetric error of a binary classifier based on its empirical asymmetric error. We focus our study on convex combinations of classifiers like classifiers learned from boosting and neural networks. Based on our bounds, it is suggested that machine learning methods that use an asymmetric error to learn should produce a classifier that has different margins for examples coming from different classes, rather than treating the margins equally like current methods do."}]}