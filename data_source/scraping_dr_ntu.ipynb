{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import requests\n",
    "from thefuzz import fuzz\n",
    "import re\n",
    "import time\n",
    "import itertools\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract DR-NTU\n",
    "\n",
    "1. DR-NTU\n",
    "    - Name,Email,Designations,Patents,Grants,Biography,External Websites,Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract SCSE Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_scse_profiles():    \n",
    "    # rpp query parameter specifies number of rows to display\n",
    "    # start query parameter specifies which row to start displaying from.\n",
    "    start = 0\n",
    "    rpp = 50\n",
    "    dr_ntu = {\"email\":[],'name':[], \"dr_ntu_url\":[]}\n",
    "    while True:\n",
    "        SCSE_list_url = f\"https://dr.ntu.edu.sg/simple-search?query=&location=researcherprofiles&filter_field_1=school&filter_type_1=authority&filter_value_1=ou00030&crisID=&relationName=&sort_by=bi_sort_4_sort&order=asc&rpp={rpp}&etal=0&start={start}\"\n",
    "\n",
    "        soup_source = requests.get(SCSE_list_url).text\n",
    "        soup = BeautifulSoup(soup_source,'lxml')\n",
    "\n",
    "        table = soup.find('table')\n",
    "        if table==None:\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            table_rows = table.find_all('tr')\n",
    "            # Skip Header Column\n",
    "            for row in table_rows[1:]:\n",
    "                name = row.find(name='td', headers='t1').text\n",
    "                dr_ntu_url = row.find(name='td', headers='t1').find(name='a')['href']\n",
    "                email = row.find(name='td', headers='t3').text\n",
    "\n",
    "                dr_ntu['name'].append(name)\n",
    "                dr_ntu['dr_ntu_url'].append(\"https://dr.ntu.edu.sg\"+dr_ntu_url)\n",
    "                dr_ntu['email'].append(email)\n",
    "                \n",
    "        start+=rpp\n",
    "    return pd.DataFrame(dr_ntu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scse_profiles = extract_scse_profiles()\n",
    "# Save to csv file in prof_raw_data dir\n",
    "raw_dir = './prof_raw_data/'\n",
    "filename = 'scse_profiles'\n",
    "filepath = f\"{raw_dir}{filename}.csv\" \n",
    "\n",
    "scse_profiles.to_csv(filepath)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Individual's Information from DR-NTU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dr_ntu(row):\n",
    "\n",
    "    name = row['name']\n",
    "    dr_ntu_url = row['dr_ntu_url']\n",
    "    ntu_email = row['email']\n",
    "\n",
    "    profile = {}\n",
    "    profile['full_name'] = name\n",
    "    profile['email'] = ntu_email\n",
    "\n",
    "\n",
    "    response = requests.get(dr_ntu_url).text\n",
    "    html = BeautifulSoup(response,'lxml')\n",
    "    \n",
    "    \n",
    "    # scrape the text on the name card class\n",
    "    span = html.find(name='span',attrs={'class':'namecard-fullname'})\n",
    "    if span is None:\n",
    "        profile['name_card'] = None\n",
    "    else:\n",
    "        profile['name_card'] = span.text.strip()\n",
    "\n",
    "    img = html.find(name='img',attrs={'id':'picture'})\n",
    "    img_link = f\"https://dr.ntu.edu.sg{img.get('src')}\"\n",
    "    response = requests.get(img_link)\n",
    "    \n",
    "    image_path = f\"./profile_image/{name.lower().replace(' ','_')}.jpg\"\n",
    "    with open(image_path,'wb')as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    profile['image_path'] = f\"./data_source/profile_image/{name.lower().replace(' ','_')}.jpg\"\n",
    "\n",
    "    \n",
    "    # scraping designations can have multiple designation\n",
    "    div = span.parent\n",
    "    div = div.find_next_sibling('div')\n",
    "    designations = []\n",
    "    while div.get('id') is None:\n",
    "        designations.append(div.text.strip())\n",
    "        div = div.find_next_sibling('div')\n",
    "\n",
    "    profile['designations'] = designations\n",
    "    \n",
    "    urls = {'dr_ntu':dr_ntu_url,'orcid':None,'personal':None}\n",
    "    # scrape all personal websites from personalsiteDiv\n",
    "    div = html.find(name='div',attrs={'id':'personalsiteDiv'})\n",
    "    if div != None:\n",
    "        a_tags = div.find_all(name='a')\n",
    "        for i,a_tag in enumerate(a_tags):\n",
    "            # first one is always personal website\n",
    "            if i==0:\n",
    "                url = a_tag.get('href')\n",
    "                # fix the key to personal instead of what is typed as text in html\n",
    "                text = 'personal'\n",
    "            else:\n",
    "                url = a_tag.get('href')\n",
    "                text = a_tag.text.lower().replace(' ','_').strip()\n",
    "\n",
    "            if url is None or url =='#':\n",
    "                continue\n",
    "            else:\n",
    "                urls[text] = url\n",
    "    \n",
    "    # scrape biography\n",
    "    biography_text = html.find(name='div',attrs={'id':'biographyDiv'}).text.strip()\n",
    "    profile['biography'] = biography_text\n",
    "\n",
    "    # scrape keywords\n",
    "    keywords = []\n",
    "    div = html.find(name='div',attrs={'id':'researchkeywords','class':'panel'})\n",
    "    if div != None:\n",
    "        spans = html.find_all(name='span',attrs={'class':'rkeyword'})\n",
    "        for span in spans:\n",
    "            keywords.append(span.text.strip())\n",
    "\n",
    "    profile['keywords'] = keywords\n",
    "\n",
    "    # scrape research grants\n",
    "    grants = []\n",
    "    div = html.find(name='div',attrs={'id':'currentgrantsDiv'})\n",
    "    if div != None:\n",
    "        # first case is encapsulated by li tag\n",
    "        ul = div.find(name='ul')\n",
    "        if ul != None:\n",
    "            li_tags = ul.find_all('li')\n",
    "            for li_tag in li_tags:\n",
    "                grants.append(li_tag.text.strip())\n",
    "\n",
    "        elif div.find('br'):\n",
    "            for child in div.children:\n",
    "                if child.name == 'br':\n",
    "                    grants.append(child.next_sibling.strip())\n",
    "        else:\n",
    "            pass\n",
    "    profile['grants'] = grants\n",
    "\n",
    "    #scrape patents\n",
    "    patents = []\n",
    "    div = html.find(name='div',attrs={'id':'centralpatentsDiv'})\n",
    "    if div:\n",
    "        a_tags = div.find_all('a')\n",
    "        links = [a_tag['href'] for a_tag in a_tags]\n",
    "        titles = [a_tag.find('b').text for a_tag in a_tags]\n",
    "        u_tags = div.find_all('u')\n",
    "        abstracts = [u_tag.next_sibling.strip() for u_tag in u_tags]\n",
    "\n",
    "        for link,title,abstract in zip(links,titles,abstracts):\n",
    "            patents.append({'link':link,'title':title,'abstract':abstract})\n",
    "    \n",
    "    profile['patents'] = patents\n",
    "\n",
    "    # scrape all personal websites from bibliometric\n",
    "    publication_url = dr_ntu_url + '/selectedPublications.html'\n",
    "    resp = requests.get(publication_url).text\n",
    "    html = BeautifulSoup(resp, \"html.parser\")\n",
    "    div = html.find(name='div', attrs={'id':'custombiblio'})\n",
    "    if div:\n",
    "        div_list = div.find_all(name='div', attrs={'class':'dynaField'})\n",
    "        for div in div_list:\n",
    "            a_tag = div.find(name='a')\n",
    "            url = a_tag.get('href')\n",
    "            text = a_tag.find(name='span').text.lower().strip().replace(' ','_')\n",
    "            # if key already exist in urls dict or if url is empty --> skip\n",
    "            if url and text not in urls:\n",
    "                urls[text] = url\n",
    "    \n",
    "    profile['urls'] = urls\n",
    "\n",
    "\n",
    "    dir = './prof_raw_data/'\n",
    "    prefix = 'dr_ntu_'\n",
    "    filename = name.lower().replace(' ','_')\n",
    "    filepath = f\"{dir}{prefix}{filename}.json\"\n",
    "    with open(filepath,'w') as f:\n",
    "        json.dump(profile,f)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scse_profile_df = pd.read_csv('./prof_raw_data/scse_profiles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "      ... \n",
       "81    None\n",
       "82    None\n",
       "83    None\n",
       "84    None\n",
       "85    None\n",
       "Length: 86, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scse_profile_df.apply(extract_dr_ntu,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract About Us\n",
    "\n",
    "- Introduction Text from NTU Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.ntu.edu.sg/scse/about-us\"\n",
    "response = requests.get(url).text\n",
    "soup = BeautifulSoup(response)\n",
    "div = soup.find(attrs={'class':'rte'})\n",
    "p_tags = soup.find_all('p')\n",
    "text = []\n",
    "for p in p_tags:\n",
    "    p_text = p.get_text(strip=True)\n",
    "    if p_text != \"\":\n",
    "        # Split the text at the `<br>` tags and strip any leading/trailing whitespace\n",
    "        text.extend(br.strip() for br in p_text.split('<br>'))\n",
    "\n",
    "# br_tags = div.find_all('br')\n",
    "# text = [br.parent.get_text(strip=True) for br in br_tags if br.parent and br.parent.get_text(strip=True) != \"\"]\n",
    "\n",
    "with open('./prof_raw_data/scse_intro.json','w') as f:\n",
    "    scse_intro = {'intro':text}\n",
    "    json.dump(scse_intro,f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CZ4125",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
