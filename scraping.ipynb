{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to Scrape\n",
    "\n",
    "1. Professor's Experience/Affiliation (DBLP) & Google Scholar URL --> Background info of researcher\n",
    "2. Description, Year, Venues of all Publications by invidivual (Google Scholar) --> Quantitative Metric & Track Shifts in Research Interest\n",
    "3. Collaboration counts and their citation counts for each co-authors(DBLP & Google Scholar) --> Create Coauthor Network Graph\n",
    "4. Scrape Conference/Journals Ratings (portal conference website)\n",
    "5. Scrape the research interest for each professor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import requests\n",
    "from thefuzz import fuzz\n",
    "import re\n",
    "import itertools\n",
    "from selenium import webdriver\n",
    "from selenium_stealth import stealth\n",
    "from selenium.webdriver.common.by import By\n",
    "import os,json\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Google Scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n    {\\n        \\'name\\': \"Li Boyang\",\\n        \\'interest\\': [\\'ML\\',\\'AI\\'],\\n        \\'articles\\':[\\n            {\\n                \\'Title\\': \"Story Generation\",\\n                \\'CO-Authors\\' : [\\'Mark Riedl\\',\\'LOL\\'],\\n                \\'date\\': str,\\n                \\'venue\\': str,\\n                \\'Description: str,\\n                \\'Cited by\\': int,\\n                \\'citation_graph\\':dict\\n            },\\n        ],\\n        \\'co_authors_profile_url\\': [\\n            \\'https://scholar.google.com/citations?user=Yg_QjxcAAAAJ&hl=en\\',\\n        ]\\n        \\'all_citation_count\\': int\\n        \\'recent_citation_count: int\\n        }\\n\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "    {\n",
    "        'name': \"Li Boyang\",\n",
    "        'interest': ['ML','AI'],\n",
    "        'articles':[\n",
    "            {\n",
    "                'Title': \"Story Generation\",\n",
    "                'CO-Authors' : ['Mark Riedl','LOL'],\n",
    "                'date': str,\n",
    "                'venue': str,\n",
    "                'Description: str,\n",
    "                'Cited by': int,\n",
    "                'citation_graph':dict\n",
    "            },\n",
    "        ],\n",
    "        'co_authors_profile_url': [\n",
    "            'https://scholar.google.com/citations?user=Yg_QjxcAAAAJ&hl=en',\n",
    "        ]\n",
    "        'all_citation_count': int\n",
    "        'recent_citation_count: int\n",
    "        }\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scholarly Name Search\n",
    "\n",
    "- Add Nanyang Technological University as keyword during the search to narrow down the results\n",
    "- Disadvantage is that those faculty member which did not update their affiliation will be lost in this search\n",
    "- Hence we need to search by citation as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prof_details(details, filename, dir='./prof_raw_data'):\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    filename = dir + '/' + filename\n",
    "        \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(details, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name_combinations(name):\n",
    "    words = name.split()\n",
    "    combinations = []\n",
    "    for r in range(2, len(words) + 1):\n",
    "        combinations.extend(list(itertools.combinations(words, r)))\n",
    "    combinations = [' '.join(combination).strip(',') for combination in combinations]\n",
    "    return combinations[::-1]\n",
    "\n",
    "def create_driver(debug=False):\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    if debug==False:\n",
    "        options.add_argument(\"--headless\")\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    options.add_experimental_option('useAutomationExtension', False)\n",
    "    driver = webdriver.Chrome(\n",
    "        options=options\n",
    "    )\n",
    "    stealth(driver,\n",
    "            # user_agent=agent,\n",
    "            languages=[\"en-US\", \"en\"],\n",
    "            vendor=\"Google Inc.\",\n",
    "            platform=\"Win32\",\n",
    "            webgl_vendor=\"Intel Inc.\",\n",
    "            renderer=\"Intel Iris OpenGL Engine\",\n",
    "            fix_hairline=True,\n",
    "            )\n",
    "    return driver\n",
    "\n",
    "def extract_goog_sch_profile(author_full_name):\n",
    "\n",
    "    BASE_URL = 'https://scholar.google.com'\n",
    "    if len(author_full_name.split())>2:\n",
    "        name_perm_list = generate_name_combinations(author_full_name)\n",
    "    else:\n",
    "        name_perm_list = [author_full_name]\n",
    "\n",
    "    prof_result = {}\n",
    "    for name in name_perm_list:\n",
    "        query_param = name+\" Nanyang Technological University\"\n",
    "        search_url = f\"https://scholar.google.com/citations?hl=en&view_op=search_authors&mauthors={query_param}&btnG=\"\n",
    "\n",
    "        driver = create_driver()\n",
    "        driver.get(search_url)\n",
    "        time.sleep(3)  # wait for the page to load\n",
    "\n",
    "        # find professors' descriptions and emails\n",
    "        dsc_elements = driver.find_elements(By.CLASS_NAME, 'gs_ai_aff') \n",
    "        email_elements = driver.find_elements(By.CLASS_NAME, 'gs_ai_eml')\n",
    "\n",
    "        candidates = []\n",
    "        max_index = None\n",
    "        max_similarity = 0\n",
    "        for i in range(len(dsc_elements)):\n",
    "            if 'ntu.edu.sg' in email_elements[i].text or 'Nanyang Technological University' in dsc_elements[i].text:\n",
    "                cur_name = driver.find_elements(By.CLASS_NAME, 'gs_ai_name')[i].text\n",
    "                # Similarity score of at least 80 before we consider it a match\n",
    "                sim_score = fuzz.token_sort_ratio(name,cur_name)\n",
    "                if sim_score>=80:\n",
    "                    author_url = driver.find_elements(By.CLASS_NAME, 'gs_ai_name')[i].find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "                    candidates.append((cur_name,author_url))\n",
    "                    if len(candidates)==1:\n",
    "                        max_index=0\n",
    "                        max_similarity = sim_score\n",
    "                    elif sim_score>max_similarity:\n",
    "                        max_index = len(candidates)-1\n",
    "                        max_similarity = sim_score\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        if max_index != None:\n",
    "            candidate = candidates[max_index]\n",
    "            cur_name = candidate[0]\n",
    "            author_url = candidate[1]\n",
    "            prof_result['goog_sch_url'] = author_url\n",
    "            driver = create_driver()\n",
    "            driver.get(author_url+\"&pagesize=100\")\n",
    "            time.sleep(3)  # wait for the page to load\n",
    "            \n",
    "            # extract name\n",
    "            prof_result['name'] = author_full_name\n",
    "\n",
    "            list_of_interests = []\n",
    "            try:\n",
    "                # extracting interests\n",
    "                int_element = driver.find_elements(By.ID, 'gsc_prf_int')\n",
    "                if len(int_element)>0:\n",
    "                    int_element = int_element[0]\n",
    "                    a_tags = int_element.find_elements(By.TAG_NAME, 'a')\n",
    "                    list_of_interests = [tag.text for tag in a_tags]\n",
    "                else:\n",
    "                    pass\n",
    "            except Exception as e:\n",
    "                print(f\"Error Interest List for {author_full_name}\")\n",
    "                print(e)\n",
    "                pass\n",
    "\n",
    "            prof_result['interests'] = list_of_interests\n",
    "\n",
    "\n",
    "            # extracting co_authors_url\n",
    "            co_authors_details = []\n",
    "\n",
    "            # some prof dont have co_authors\n",
    "            try:\n",
    "                co_author_table = driver.find_elements(By.ID,'gsc_rsb_co')\n",
    "                if len(co_author_table)==0:\n",
    "                    pass\n",
    "                else:\n",
    "                    co_author_table = co_author_table[0]\n",
    "\n",
    "                    co_author_view_btn = driver.find_elements(By.ID,'gsc_coauth_opn')\n",
    "                    if len(co_author_view_btn)>0:\n",
    "                        open_btn = co_author_view_btn[0]\n",
    "                        open_btn.click()\n",
    "                        time.sleep(4)\n",
    "\n",
    "                        content = driver.find_element(By.ID,'gsc_codb_content').get_attribute('outerHTML')\n",
    "                        soup = BeautifulSoup(content,'html.parser')\n",
    "                        co_authors_list = soup.find_all(name='div', attrs={'class':'gsc_ucoar gs_scl'})\n",
    "\n",
    "                        for co_author in co_authors_list:\n",
    "                            desc = co_author.find(name='div', attrs={'class':'gs_ai_t gs_ai_pss'})\n",
    "                            name_div = desc.find(name='h3', attrs={'class': 'gs_ai_name'})\n",
    "                            name = name_div.text.strip()\n",
    "                            url = name_div.find(name='a').get('href')\n",
    "                            aff = desc.find(name='div', attrs={'class':'gs_ai_aff'}).text.strip()\n",
    "                            \n",
    "                            co_authors_details.append({\n",
    "                                'name':name,\n",
    "                                'url': BASE_URL + url,\n",
    "                                'aff':aff\n",
    "                            })\n",
    "\n",
    "                        close_button = driver.find_element(By.ID,'gsc_md_cod-x')\n",
    "                        close_button.click()\n",
    "                    else:\n",
    "                        co_authors_list = co_author_table.find_element(By.CLASS_NAME,'gsc_rsb_a').find_elements(By.TAG_NAME,'li')\n",
    "                        for co_author in co_authors_list:\n",
    "                            name = co_author.find_element(By.CLASS_NAME,'gsc_rsb_a_desc').find_element(By.TAG_NAME,'a').text\n",
    "                            url = co_author.find_element(By.CLASS_NAME,'gsc_rsb_a_desc').find_element(By.TAG_NAME,'a').get_attribute('href')\n",
    "                            aff = co_author.find_element(By.CLASS_NAME,'gsc_rsb_a_ext').text\n",
    "                                \n",
    "                            co_authors_details.append({\n",
    "                                'name':name,\n",
    "                                'url': url,\n",
    "                                'aff':aff\n",
    "                            })\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in Co-Authors Table for {author_full_name}\")\n",
    "                print(e)\n",
    "                pass\n",
    "\n",
    "            prof_result['co_authors_url'] = co_authors_details\n",
    "            \n",
    "            prof_result['citation_table'] = {}\n",
    "            try:\n",
    "                # extracting citation table info\n",
    "                cols = []\n",
    "                ths = driver.find_element(By.ID, 'gsc_rsb_st').find_element(By.TAG_NAME,'thead').find_elements(By.CLASS_NAME,'gsc_rsb_sth')\n",
    "                for th in ths:\n",
    "                    if th.text!=\"\":\n",
    "                        cols.append(th.text)\n",
    "\n",
    "                prof_result['citation_table']['columns'] = cols\n",
    "                \n",
    "                trs = driver.find_element(By.ID, 'gsc_rsb_st').find_element(By.TAG_NAME, 'tbody').find_elements(By.TAG_NAME, 'tr')\n",
    "                for tr in trs:\n",
    "                    row_index = tr.find_element(By.CLASS_NAME,'gsc_rsb_sc1').text\n",
    "                    counts = [int(count.text) for count in tr.find_elements(By.CLASS_NAME,'gsc_rsb_std')]\n",
    "                    prof_result['citation_table'][row_index] = counts\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in Citation Table for {author_full_name}\") \n",
    "                print(e)\n",
    "\n",
    "            citation_graph = {}\n",
    "            try:\n",
    "                view_char_btn = driver.find_elements(By.ID, 'gsc_hist_opn')\n",
    "                if len(view_char_btn)>0:\n",
    "                    \n",
    "                    view_char_btn[0].click()\n",
    "                    time.sleep(3)\n",
    "\n",
    "                    graph = driver.find_element(By.ID,'gsc_md_hist_c').find_element(By.CLASS_NAME,'gsc_md_hist_b')\n",
    "                    years = graph.find_elements(By.CLASS_NAME,'gsc_g_t')\n",
    "                    citation_counts = graph.find_elements(By.CLASS_NAME,'gsc_g_a')\n",
    "\n",
    "                    for i in range(len(citation_counts)):\n",
    "                        style = citation_counts[i].get_attribute('style')\n",
    "                        year_index = int(style.split(':')[-1].strip(';'))\n",
    "                        citation_graph[int(years[-year_index].get_attribute('innerText'))] = int(citation_counts[i].get_attribute('textContent'))\n",
    "                    close_chart_btn = driver.find_element(By.ID,'gsc_md_hist-x')\n",
    "                    close_chart_btn.click()\n",
    "       \n",
    "            except Exception as e:\n",
    "                print(f\"Error in citation graph for {author_full_name}\")\n",
    "                print(e)\n",
    "\n",
    "            prof_result['citation_graph'] = citation_graph\n",
    "\n",
    "            articles = []\n",
    "            try:\n",
    "                # extracting articles info\n",
    "                btn = driver.find_element(By.ID,'gsc_bpf_more')\n",
    "                while btn.get_attribute('disabled') is None:\n",
    "                    btn.click()\n",
    "                    time.sleep(3)\n",
    "\n",
    "                article_url_list = []\n",
    "                trs = driver.find_element(By.ID, 'gsc_a_b').find_elements(By.CLASS_NAME, 'gsc_a_tr')\n",
    "                for tr in trs:\n",
    "                    article_url_list.append(tr.find_element(By.CLASS_NAME,'gsc_a_t').find_element(By.TAG_NAME,'a').get_attribute('href'))\n",
    "                    \n",
    "                driver.quit()\n",
    "\n",
    "                for article_url in tqdm(article_url_list,position=0,leave=True):\n",
    "                    driver = create_driver()\n",
    "                    driver.get(article_url)\n",
    "                    time.sleep(3)\n",
    "                    title = driver.find_element(By.ID, 'gsc_oci_title').text\n",
    "                    items = driver.find_element(By.ID, 'gsc_oci_table').find_elements(By.CLASS_NAME, 'gs_scl')\n",
    "\n",
    "                    article = {}\n",
    "                    article['title'] = title\n",
    "                    article['url'] = article_url\n",
    "                    for item in items:\n",
    "                        key = item.find_element(By.CLASS_NAME, 'gsc_oci_field')\n",
    "                        value = item.find_element(By.CLASS_NAME, 'gsc_oci_value')\n",
    "                        key = key.text.strip().lower().replace(' ', '_')\n",
    "                        if key =='authors':\n",
    "                            article[key] = value.text.split(', ')\n",
    "                        if key=='publication_date':\n",
    "                            article[key] = value.text\n",
    "                        if key=='journal' or key=='book' or key=='conference':\n",
    "                            article[key] = value.text\n",
    "                        if key=='description':\n",
    "                            article[key]= value.text\n",
    "                        if key=='total_citations':\n",
    "                            # total citation count\n",
    "                            article[key] = int(value.find_element(By.TAG_NAME, 'a').text.split(' ')[-1])\n",
    "\n",
    "                            # citation count over the years\n",
    "                            years = value.find_elements(By.CLASS_NAME,'gsc_oci_g_t')\n",
    "                            citations = value.find_elements(By.CLASS_NAME,'gsc_oci_g_a')\n",
    "                            value_2 = {int(year.get_attribute('innerText')):0 for year in years}\n",
    "                            for citation in citations:\n",
    "                                year = int(citation.get_attribute('href')[-4:])\n",
    "                                value_2[year] = int(citation.get_attribute('textContent'))\n",
    "                            article['citation_graph'] = value_2\n",
    "                    articles.append(article)\n",
    "                    driver.quit()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in Articles for {author_full_name}\")\n",
    "                print(e)\n",
    "            prof_result['articles'] = articles\n",
    "            \n",
    "        # No need to search through all name_permutations as best candidate has been found\n",
    "        if max_index != None:\n",
    "            break\n",
    "        \n",
    "    filename = \"goog_sch_\"+author_full_name.lower().replace(' ','_')+'.json'\n",
    "    save_prof_details(details=prof_result,filename=filename)\n",
    "                \n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.read_csv('Kee_Kai_Teng.csv')\n",
    "result_df = result_df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['Name'].apply(extract_goog_sch_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_article = [\"Dusit Niyato\",\"Cong Gao\",\"Lin Guosheng\",\"Yu Han\",\"Zhang Hanwang\",\"Luo Jun\",\"Liu Weichen\",'Luke Ong （翁之昊）',\"Qian Kemao\",'Mohamed M. Sabry',\"Owen Noel Newton Fernando\",\"Joty Shafiq Rayhan\",\"Tan Rui\",'Pan Xingang',\"Liu Yang\",'Wen Yonggang',\"Li Yi\",\"Zhang Jie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_co_author = [\"Anwitaman Datta\",\"Anupam Chattopadhyay\",\"Kwoh Chee Keong\",\"Chng Eng Siong\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232/232 [30:35<00:00,  7.91s/it]\n",
      "100%|██████████| 381/381 [49:36<00:00,  7.81s/it]\n",
      "100%|██████████| 378/378 [49:20<00:00,  7.83s/it]\n",
      "100%|██████████| 345/345 [45:11<00:00,  7.86s/it]\n"
     ]
    }
   ],
   "source": [
    "for name in missing_co_author:\n",
    "    extract_goog_sch_profile(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CZ4125",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
