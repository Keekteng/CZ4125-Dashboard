{"goog_sch_url": "https://scholar.google.com/citations?hl=en&user=uo0q9WgAAAAJ", "name": "Pan Xingang", "interests": ["Computer Vision", "Machine Learning", "Computer Graphics"], "co_authors_url": [{"name": "Ping Luo (\u7f85\u5e73)", "url": "https://scholar.google.com/citations?user=aXdjxb4AAAAJ&hl=en", "aff": "Associate Professor, The University of Hong Kong"}, {"name": "Dahua Lin", "url": "https://scholar.google.com/citations?user=GMzzRRUAAAAJ&hl=en", "aff": "The Chinese University of Hong Kong"}, {"name": "Bo Dai", "url": "https://scholar.google.com/citations?user=KNWTvgEAAAAJ&hl=en", "aff": "Shanghai AI Laboratory"}, {"name": "Xiaoou Tang", "url": "https://scholar.google.com/citations?user=qpBtpGsAAAAJ&hl=en", "aff": "The Chinese University of Hong Kong"}, {"name": "Jianping Shi", "url": "https://scholar.google.com/citations?user=mwsxrm4AAAAJ&hl=en", "aff": "SenseTime Group Limited"}, {"name": "Christian Theobalt", "url": "https://scholar.google.com/citations?user=eIWg8NMAAAAJ&hl=en", "aff": "Professor, Max Planck Institute for Informatics, Saarland Informatics Campus, Saarland University"}, {"name": "Xiaohang Zhan", "url": "https://scholar.google.com/citations?user=QfquhDEAAAAJ&hl=en", "aff": "Tencent AI Lab"}, {"name": "Ziwei Liu", "url": "https://scholar.google.com/citations?user=lc45xlcAAAAJ&hl=en", "aff": "Assistant Professor, Nanyang Technological University"}, {"name": "Chen Change Loy", "url": "https://scholar.google.com/citations?user=559LF80AAAAJ&hl=en", "aff": "MMLab@NTU, S-Lab, Nanyang Technological University"}, {"name": "Xiaogang Wang", "url": "https://scholar.google.com/citations?user=-B5JgjsAAAAJ&hl=en", "aff": "Professor of Electronic Engineering, the Chinese University of Hong Kong"}, {"name": "Ayush Tewari", "url": "https://scholar.google.com/citations?user=pDnzpeoAAAAJ&hl=en", "aff": "MIT"}, {"name": "Lingjie Liu", "url": "https://scholar.google.com/citations?user=HZPnJ9gAAAAJ&hl=en", "aff": "Assistant Professor at UPenn"}, {"name": "Mohamed Elgharib", "url": "https://scholar.google.com/citations?user=e1WLgm8AAAAJ&hl=en", "aff": "Max Planck Institute for Informatics, Research Group Leader"}, {"name": "Maneesh Agrawala", "url": "https://scholar.google.com/citations?user=YPzKczYAAAAJ&hl=en", "aff": "Stanford University"}, {"name": "Karol Myszkowski", "url": "https://scholar.google.com/citations?user=eZ40F-MAAAAJ&hl=en", "aff": "Senior Researcher, MPI Informatik"}, {"name": "Hans-Peter Seidel", "url": "https://scholar.google.com/citations?user=s2Ibok8AAAAJ&hl=en", "aff": "Professor of Computer Science, Max Planck Institute for Informatics, Saarland Informatics Campus"}], "citation_table": {"columns": ["All", "Since 2018"], "Citations": [2523, 2521], "h-index": [15, 15], "i10-index": [16, 16]}, "citation_graph": {}, "articles": [{"title": "Spatial as Deep: Spatial CNN for Traffic Scene Understanding", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&pagesize=100&citation_for_view=uo0q9WgAAAAJ:d1gkVwhDpl0C", "authors": ["Xingang Pan", "Jianping Shi", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang"], "publication_date": "2018", "conference": "AAAI Conference on Artificial Intelligence", "description": "Convolutional neural networks (CNNs) are usually built by stacking convolutional operations layer-by-layer. Although CNN has shown strong capability to extract semantics from raw pixels, its capacity to capture spatial relationships of pixels across rows and columns of an image is not fully explored. These relationships are important to learn semantic objects with strong shape priors but weak appearance coherences, such as traffic lanes, which are often occluded or not even painted on the road surface as shown in Fig. 1 (a). In this paper, we propose Spatial CNN (SCNN), which generalizes traditional deep layer-by-layer convolutions to slice-by-slice convolutions within feature maps, thus enabling message passings between pixels across rows and columns in a layer. Such SCNN is particular suitable for long continuous shape structure or large objects, with strong spatial relationship but less appearance clues, such as traffic lanes, poles, and wall. We apply SCNN on a newly released very challenging traffic lane detection dataset and Cityscapse dataset. The results show that SCNN could learn the spatial relationship for structure output and significantly improves the performance. We show that SCNN outperforms the recurrent neural network (RNN) based ReNet and MRF+ CNN (MRFNet) in the lane detection dataset by 8.7% and 4.6% respectively. Moreover, our SCNN won the 1st place on the TuSimple Benchmark Lane Detection Challenge, with an accuracy of 96.53%.", "total_citations": 814, "citation_graph": {"2018": 17, "2019": 68, "2020": 111, "2021": 185, "2022": 224, "2023": 206}}, {"title": "Two at Once: Enhancing Learning and Generalization Capacities via IBN-Net", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&pagesize=100&citation_for_view=uo0q9WgAAAAJ:2osOgNQ5qMEC", "authors": ["Xingang Pan", "Ping Luo", "Jianping Shi", "Xiaoou Tang"], "publication_date": "2018", "conference": "European Conference on Computer Vision", "description": "Convolutional neural networks (CNNs) have achieved great successes in many computer vision problems. Unlike existing works that designed CNN architectures to improve performance on a single task of a single domain and not generalizable, we present IBN-Net, a novel convolutional architecture, which remarkably enhances a CNN\u2019s modeling ability on one domain (eg Cityscapes) as well as its generalization capacity on another domain (eg GTA5) without finetuning. IBN-Net carefully integrates Instance Normalization (IN) and Batch Normalization (BN) as building blocks, and can be wrapped into many advanced deep networks to improve their performances. This work has three key contributions.(1) By delving into IN and BN, we disclose that IN learns features that are invariant to appearance changes, such as colors, styles, and virtuality/reality, while BN is essential for preserving content related information.(2) IBN-Net can be applied to many advanced deep architectures, such as DenseNet, ResNet, ResNeXt, and SENet, and consistently improve their performance without increasing computational cost.(3) When applying the trained networks to new domains, eg from GTA5 to Cityscapes, IBN-Net achieves comparable improvements as domain adaptation methods, even without using data from the target domain. With IBN-Net, we won the 1st place on the WAD 2018 Challenge Drivable Area track, with an mIoU of 86.18%.", "total_citations": 644, "citation_graph": {"2019": 28, "2020": 75, "2021": 152, "2022": 201, "2023": 187}}, {"title": "Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&pagesize=100&citation_for_view=uo0q9WgAAAAJ:_kc_bZDykSQC", "authors": ["Xingang Pan", "Xiaohang Zhan", "Bo Dai", "Dahua Lin", "Chen Change Loy", "Ping Luo"], "publication_date": "2021/9/24", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "description": "Learning a good image prior is a long-term goal for image restoration and manipulation. While existing methods like deep image prior (DIP) capture low-level image statistics, there are still gaps toward an image prior that captures rich image semantics including color, spatial coherence, textures, and high-level concepts. This work presents an effective way to exploit the image prior captured by a generative adversarial network (GAN) trained on large-scale natural images. As shown in Fig. 1, the deep generative prior (DGP) provides compelling results to restore missing semantics, e.g., color, patch, resolution, of various degraded images. It also enables diverse image manipulation including random jittering, image morphing, and category transfer. Such highly flexible restoration and manipulation are made possible through relaxing the assumption of existing GAN inversion methods, which tend to fix the generator \u2026", "total_citations": 249, "citation_graph": {"2020": 7, "2021": 58, "2022": 99, "2023": 83}}, {"title": "Open Compound Domain Adaptation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&pagesize=100&citation_for_view=uo0q9WgAAAAJ:Tyk-4Ss8FVUC", "authors": ["Ziwei Liu", "Zhongqi Miao", "Xingang Pan", "Xiaohang Zhan", "Stella X Yu", "Dahua Lin", "Boqing Gong"], "publication_date": "2020", "conference": "IEEE/CVF Conference on Computer Vision and Pattern Recognition", "description": "A typical domain adaptation approach is to adapt models trained on the annotated data in a source domain (eg, sunny weather) for achieving high performance on the test data in a target domain (eg, rainy weather). Whether the target contains a single homogeneous domain or multiple heterogeneous domains, existing works always assume that there exist clear distinctions between the domains, which is often not true in practice (eg, changes in weather). We study an open compound domain adaptation (OCDA) problem, in which the target is a compound of multiple homogeneous domains without domain labels, reflecting realistic data collection from mixed and novel situations. We propose a new approach based on two technical insights into OCDA: 1) a curriculum domain adaptation strategy to bootstrap generalization across domains in a data-driven self-organizing fashion and 2) a memory module to increase the model's agility towards novel domains. Our experiments on digit classification, facial expression recognition, semantic segmentation, and reinforcement learning demonstrate the effectiveness of our approach.", "total_citations": 130, "citation_graph": {"2019": 1, "2020": 15, "2021": 34, "2022": 39, "2023": 40}}]}