{"goog_sch_url": "https://scholar.google.com/citations?user=QwL4z2UAAAAJ&hl=en", "name": "Li Boyang", "interests": ["Artificial Intelligence", "Narrative Intelligence", "Multimodal Learning", "Machine Learning", "Storytelling"], "co_authors_url": [{"name": "Mark Riedl", "url": "https://scholar.google.com/citations?hl=en&user=Yg_QjxcAAAAJ", "aff": "Professor of Computing, Georgia Institute of Technology", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [10316, 6153], "h-index": [52, 40], "i10-index": [171, 120]}}, {"name": "Chunyan Miao", "url": "https://scholar.google.com/citations?hl=en&user=fmXGRJgAAAAJ", "aff": "Nanyang Technological University", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [18251, 13476], "h-index": [65, 56], "i10-index": [312, 229]}}, {"name": "Han Yu", "url": "https://scholar.google.com/citations?hl=en&user=eXgoTXMAAAAJ", "aff": "Nanyang Assistant Professor, Nanyang Technological University, Singapore", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [12505, 10893], "h-index": [48, 39], "i10-index": [122, 100]}}, {"name": "Stephen Lee-Urban", "url": "https://scholar.google.com/citations?hl=en&user=X-m72rAAAAAJ", "aff": "Senior Research Scientist, Georgia Tech Research Institute", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [1027, 382], "h-index": [14, 8], "i10-index": [15, 7]}}, {"name": "Leonid Sigal", "url": "https://scholar.google.com/citations?hl=en&user=P2mG6rcAAAAJ", "aff": "Professor, University of British Columbia", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [13418, 7035], "h-index": [56, 44], "i10-index": [123, 106]}}, {"name": "Mohamed Elhoseiny, Ph.D.", "url": "https://scholar.google.com/citations?hl=en&user=iRBUTOAAAAAJ", "aff": "Assistant Professor, KAUST (hiring postdocs & grad students)", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [8156, 7776], "h-index": [34, 32], "i10-index": [48, 47]}}, {"name": "Jun Chen", "url": "https://scholar.google.com/citations?hl=en&user=9G2OQmkAAAAJ", "aff": "PhD candidate,  King Abdullah University of Science and Technology", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [519, 519], "h-index": [8, 8], "i10-index": [8, 8]}}, {"name": "Yanwei Fu", "url": "https://scholar.google.com/citations?hl=en&user=Vg54TcsAAAAJ", "aff": "Fudan University", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [12239, 11548], "h-index": [49, 48], "i10-index": [101, 100]}}, {"name": "Yu-Gang Jiang", "url": "https://scholar.google.com/citations?hl=en&user=f3_FP8AAAAAJ", "aff": "Professor, Fudan University. Fellow of IAPR.", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [23342, 17017], "h-index": [74, 64], "i10-index": [212, 183]}}, {"name": "Dejing Dou", "url": "https://scholar.google.com/citations?hl=en&user=qBHsQ04AAAAJ", "aff": "BCG X, Baidu Research, University of Oregon, Yale", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [8016, 5832], "h-index": [41, 35], "i10-index": [149, 117]}}, {"name": "Steven CH Hoi", "url": "https://scholar.google.com/citations?hl=en&user=JoLjflYAAAAJ", "aff": "Managing Director of Salesforce Research Asia; IEEE Fellow; Professor at SMU", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [30950, 23545], "h-index": [82, 71], "i10-index": [278, 229]}}, {"name": "Matthew Guzdial", "url": "https://scholar.google.com/citations?hl=en&user=jKqmTbIAAAAJ", "aff": "Assistant Professor, University of Alberta", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [1341, 1260], "h-index": [16, 16], "i10-index": [30, 30]}}, {"name": "Iolanda Leite", "url": "https://scholar.google.com/citations?hl=en&user=tyKNvM8AAAAJ", "aff": "Associate Professor at KTH Royal Institute of Technology", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [6574, 5030], "h-index": [37, 33], "i10-index": [70, 61]}}, {"name": "Jill Fain Lehman", "url": "https://scholar.google.com/citations?hl=en&user=Ew7rg2cAAAAJ", "aff": "Senior Project Scientist, Carnegie Mellon University", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [2736, 1048], "h-index": [27, 17], "i10-index": [58, 27]}}, {"name": "Han Guo", "url": "https://scholar.google.com/citations?hl=en&user=Ix1cHeMAAAAJ", "aff": "Carnegie Mellon University", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [734, 734], "h-index": [11, 11], "i10-index": [11, 11]}}, {"name": "Xu Guo", "url": "https://scholar.google.com/citations?hl=en&user=JpRTl9kAAAAJ", "aff": "Nanyang Technological University, Singapore", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [43, 43], "h-index": [4, 4], "i10-index": [1, 1]}}, {"name": "Andr\u00e9 Pereira", "url": "https://scholar.google.com/citations?hl=en&user=fBJDdTgAAAAJ", "aff": "KTH Royal Institute Of Technology", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [3100, 1956], "h-index": [28, 23], "i10-index": [43, 39]}}, {"name": "Kenneth Church", "url": "https://scholar.google.com/citations?hl=en&user=E6aqGvYAAAAJ", "aff": "Northeastern University", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [28802, 6783], "h-index": [69, 34], "i10-index": [152, 97]}}, {"name": "Huijuan Xu", "url": "https://scholar.google.com/citations?hl=en&user=eml8HfQAAAAJ", "aff": "Assistant Professor, CSE Dept., Pennsylvania State University", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [4447, 3918], "h-index": [18, 18], "i10-index": [20, 20]}}, {"name": "Kate Saenko", "url": "https://scholar.google.com/citations?hl=en&user=9xDADY4AAAAJ", "aff": "Boston University", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [52601, 45400], "h-index": [78, 72], "i10-index": [176, 161]}}, {"name": "Yangfeng Ji", "url": "https://scholar.google.com/citations?hl=en&user=pg02-e8AAAAJ", "aff": "Computer Science, University of Virginia", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [4360, 3546], "h-index": [24, 21], "i10-index": [33, 32]}}, {"name": "Nathan Sturtevant", "url": "https://scholar.google.com/citations?hl=en&user=3utEUeoAAAAJ", "aff": "University of Alberta, Alberta Machine Intelligence Institute (Amii)", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [6542, 3897], "h-index": [42, 26], "i10-index": [100, 72]}}, {"name": "Maarten W Bos", "url": "https://scholar.google.com/citations?hl=en&user=ekCd0LoAAAAJ", "aff": "Snap Inc.", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [5022, 2570], "h-index": [21, 17], "i10-index": [30, 26]}}, {"name": "Yew-Soon Ong", "url": "https://scholar.google.com/citations?hl=en&user=h9oWOsEAAAAJ", "aff": "President Chair Professor of Computer Science, A*Star AI Chief Scientist", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [24167, 14759], "h-index": [78, 65], "i10-index": [291, 225]}}, {"name": "Hannaneh Hajishirzi", "url": "https://scholar.google.com/citations?hl=en&user=LOV6_WIAAAAJ", "aff": "University of Washington; Allen AI", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [16127, 15295], "h-index": [57, 56], "i10-index": [127, 119]}}, {"name": "Rogelio E. Cardona-Rivera", "url": "https://scholar.google.com/citations?hl=en&user=04bZH_kAAAAJ", "aff": "Assistant Professor, Division of Games, University of Utah", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [682, 531], "h-index": [15, 13], "i10-index": [23, 23]}}, {"name": "Victor Lesser", "url": "https://scholar.google.com/citations?hl=en&user=izMnGyMAAAAJ", "aff": "School of Computer Science, University of Massachusetts Amherst", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [30924, 3111], "h-index": [88, 26], "i10-index": [311, 66]}}, {"name": "Junnan Li", "url": "https://scholar.google.com/citations?hl=en&user=MuUhwi0AAAAJ", "aff": "", "citation_table": {"columns": ["All", "Since 2018"], "Citations": [5946, 5932], "h-index": [25, 25], "i10-index": [37, 37]}}], "citation_table": {"columns": ["All", "Since 2018"], "Citations": [2283, 1716], "h-index": [25, 22], "i10-index": [56, 39]}, "citation_graph": {"2010": 6, "2011": 28, "2012": 43, "2013": 67, "2014": 63, "2015": 90, "2016": 117, "2017": 132, "2018": 170, "2019": 199, "2020": 199, "2021": 230, "2022": 268, "2023": 645}, "articles": [{"title": "Story Generation with Crowdsourced Plot Graphs", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:mVmsd5A6BfQC", "authors": ["Boyang Li", "Stephen Lee-Urban", "George Johnston", "Mark O Riedl"], "publication_date": "2013", "conference": "The 27th AAAI Conference on Artificial Intelligence", "description": "Story generation is the problem of automatically selecting a sequence of events that meet a set of criteria and can be told as a story. Story generation is knowledge-intensive; traditional story generators rely on a priori defined domain models about fictional worlds, including characters, places, and actions that can be performed. Manually authoring the domain models is costly and thus not scalable. We present a novel class of story generation system that can generate stories in an unknown domain. Our system (a) automatically learns a domain model by crowdsourcing a corpus of narrative examples and (b) generates stories by sampling from the space defined by the domain model. A large-scale evaluation shows that stories generated by our system for a previously unknown topic are comparable in quality to simple stories authored by untrained humans", "total_citations": 262, "citation_graph": {"2013": 6, "2014": 12, "2015": 14, "2016": 30, "2017": 36, "2018": 29, "2019": 36, "2020": 32, "2021": 26, "2022": 22, "2023": 16}}, {"title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:A8cqit5AE6sC", "authors": ["Wenliang Dai", "Junnan Li", "Dongxu Li", "Anthony Meng Huat Tiong", "Junqi Zhao", "Weisheng Wang", "Boyang Li", "Pascale Fung", "Steven Hoi"], "publication_date": "2023/5/11", "journal": "arXiv preprint arXiv:2305.06500", "total_citations": 254, "citation_graph": {"2023": 254}}, {"title": "Heterogeneous knowledge transfer in video emotion recognition, attribution and summarization", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:CaZNVDsoPx4C", "authors": ["Baohan Xu", "Yanwei Fu", "Yu-Gang Jiang", "Boyang Li", "Leonid Sigal"], "publication_date": "2016/10/27", "journal": "IEEE Transactions on Affective Computing", "description": "Emotion is a key element in user-generated video. However, it is difficult to understand emotions conveyed in such videos due to the complex and unstructured nature of user-generated content and the sparsity of video frames expressing emotion. In this paper, for the first time, we propose a technique for transferring knowledge from heterogeneous external sources, including image and textual data, to facilitate three related tasks in understanding video emotion: emotion recognition, emotion attribution and emotion-oriented summarization. Specifically, our framework (1) learns a video encoding from an auxiliary emotional image dataset in order to improve supervised video emotion recognition, and (2) transfers knowledge from an auxiliary textual corpora for zero-shot recognition of emotion classes unseen during training. The proposed technique for knowledge transfer facilitates novel applications of emotion \u2026", "total_citations": 116, "citation_graph": {"2016": 2, "2017": 10, "2018": 12, "2019": 26, "2020": 14, "2021": 17, "2022": 19, "2023": 16}}, {"title": "VisualGPT: Data-efficient adaptation of pretrained language models for image captioning", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:DrR-2ekChdkC", "authors": ["Jun Chen", "Han Guo", "Kai Yi", "Boyang Li", "Mohamed Elhoseiny"], "publication_date": "2022", "conference": "2022 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "The limited availability of annotated data often hinders real-world applications of machine learning. To efficiently learn from small quantities of multimodal data, we leverage the linguistic knowledge from a large pre-trained language model (PLM) and quickly adapt it to new domains of image captioning. To effectively utilize a pretrained model, it is critical to balance the visual input and prior linguistic knowledge from pretraining. We propose VisualGPT, which employs a novel self-resurrecting encoder-decoder attention mechanism to quickly adapt the PLM with a small amount of in-domain image-text data. The proposed self-resurrecting activation unit produces sparse activations that prevent accidental overwriting of linguistic knowledge. When trained on 0.1%, 0.5% and 1% of the respective training sets, VisualGPT surpasses the best baseline by up to 10.0% CIDEr on MS COCO and 17.9% CIDEr on Conceptual Captions. Furthermore, VisualGPT achieves the state-of-the-art result on IU X-ray, a medical report generation dataset. Our code is available at https://github. com/Vision-CAIR/VisualGPT.", "total_citations": 101, "citation_graph": {"2021": 10, "2022": 24, "2023": 66}}, {"title": "Joint event detection and description in continuous video streams", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:prdVHNxh-e8C", "authors": ["Huijuan Xu", "Boyang Li", "Vasili Ramanishka", "Leonid Sigal", "Kate Saenko"], "publication_date": "2019/1/7", "conference": "2019 IEEE Winter Applications of Computer Vision (WACV)", "description": "Dense video captioning is a fine-grained video understanding task that involves two sub-problems: localizing distinct events in a long video stream, and generating captions for the localized events. We propose the Joint Event Detection and Description Network (JEDDi-Net), which solves the dense video captioning task in an end-to-end fashion. Our model continuously encodes the input video stream with three-dimensional convolutional layers, proposes variable-length temporal events based on pooled features, and generates their captions. Proposal features are extracted within each proposal segment through 3D Segment-of-Interest pooling from shared video feature encoding. In order to explicitly model temporal relationships between visual events and their captions in a single video, we also propose a two-level hierarchical captioning module that keeps track of context. On the large-scale ActivityNet Captions \u2026", "total_citations": 69, "citation_graph": {"2018": 4, "2019": 10, "2020": 17, "2021": 18, "2022": 13, "2023": 7}}, {"title": "Crowdsourcing Narrative Intelligence", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:4TOpqqG69KYC", "authors": ["Boyang Li", "Stephen Lee-Urban", "Darren Scott Appling", "Mark O Riedl"], "publication_date": "2012", "journal": "Advances in Cognitive Systems", "description": "Narrative intelligence is an important part of human cognition, especially in sensemaking and communicating with people. Humans draw on a lifetime of relevant experiences to explain stories, to tell stories, and to help choose the most appropriate actions in real-life settings. Manual authoring the required knowledge presents a significant bottleneck in the creation of systems demonstrating narrative intelligence. In this paper, we describe a novel technique for automatically learning script-like narrative knowledge from crowdsourcing. By leveraging human workers\u2019 collective understanding of social and procedural constructs, we can learn a potentially unlimited range of scripts regarding how real-world situations unfold. We present quantitative evaluations of the learned primitive events and the temporal ordering of events, which suggest we can identify orderings between events with high accuracy.", "total_citations": 68, "citation_graph": {"2012": 1, "2013": 6, "2014": 8, "2015": 5, "2016": 6, "2017": 8, "2018": 5, "2019": 6, "2020": 10, "2021": 7, "2022": 5, "2023": 1}}, {"title": "An offline planning approach to game plotline adaptation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:u5HHmVD_uO8C", "authors": ["Boyang Li", "Mark O Riedl"], "publication_date": "2010/10/10", "conference": "Proceedings of 6th Conference on Artificial Intelligence for Interactive Digital Entertainment (AIIDE 2010)", "description": "Role-playing games, and other types of contemporary video games, usually contain a main storyline consisting of several causally related quests. As players have different motivations, tastes and preferences, it can be beneficial to customize game plotlines. In this paper, we present an offline algorithm for adapting human-authored game plotlines for computer role-playing games to suit the unique needs of individual players, thereby customizing gaming experiences and enhancing re-playability. Our approach uses an plan refinement technique based on partial-order planning to (a) optimize the global structure of the plotline according to input from a player model,(b) maintain plotline coherence, and (c) facilitate authorial intent by preserving as much of the original plotline as possible. A theoretical analysis of the authorial leverage and a user study suggest the benefits of this approach.", "total_citations": 68, "citation_graph": {"2011": 9, "2012": 8, "2013": 6, "2014": 9, "2015": 5, "2016": 5, "2017": 5, "2018": 4, "2019": 4, "2020": 5, "2021": 4, "2022": 2, "2023": 1}}, {"title": "Video Emotion Recognition with Transferred Deep Feature Encodings", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:otzGkya1bYkC", "authors": ["Baohan Xu", "Yanwei Fu", "Yu-Gang Jiang", "Boyang Li", "Leonid Sigal"], "publication_date": "2016", "journal": "The 2016 ACM International Conference in Multimedia Retrieval", "description": "Despite growing research interest, emotion understanding for user-generated videos remains a challenging problem. Major obstacles include the diversity and complexity of video content, as well as the sparsity of expressed emotions. For the first time, we systematically study large-scale video emotion recognition by transferring deep feature encodings. In addition to the traditional, supervised recognition, we study the problem of zero-shot emotion recognition, where emotions in the test set are unseen during training. To cope with this task, we utilize knowledge transferred from auxiliary image and text corpora. A novel auxiliary Image Transfer Encoding (ITE) process is proposed to efficiently encode and generate video representation. We also thoroughly investigate different configurations of convolutional neural networks. Comprehensive experiments on multiple datasets demonstrate the effectiveness of our \u2026", "total_citations": 67, "citation_graph": {"2016": 2, "2017": 6, "2018": 7, "2019": 18, "2020": 12, "2021": 11, "2022": 6, "2023": 5}}, {"title": "Game Engine Learning from Video", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:PYBJJbyH-FwC", "authors": ["Matthew Guzdial", "Boyang Li", "Mark O Riedl"], "publication_date": "2017", "conference": "The 26th International Joint Conference on Artificial Intelligence (IJCAI)", "description": "Intelligent agents need to be able to make predictions about their environment. In this work we present a novel approach to learn a forward simulation model via simple search over pixel input. We make use of a video game, Super Mario Bros., as an initial test of our approach as it represents a physics system that is significantly less complex than reality. We demonstrate the significant improvement of our approach in predicting future states compared with a baseline CNN and apply the learned model to train a game playing agent. Thus we evaluate the algorithm in terms of the accuracy and value of its output model.", "total_citations": 59, "citation_graph": {"2017": 2, "2018": 10, "2019": 8, "2020": 17, "2021": 8, "2022": 7, "2023": 7}}, {"title": "Creating an immersive game world with evolutionary fuzzy cognitive maps", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:9yKSN-GCB0IC", "authors": ["Yundong Cai", "Chunyan Miao", "Ah-Hwee Tan", "Zhiqi Shen", "Boyang Li"], "publication_date": "2009/8/21", "journal": "IEEE computer graphics and applications", "description": "Increasingly, developers are creating serious games to enhance user experiences in education and training. To bridge the gap between game experiences in the virtual environment and in real life, it's crucial to generate believable characters and contexts in real time. However, numerous variables must be simulated for a large-scale serious game. These variables are involved in complex causal relationships, and their values change over time. Because conventional models haven't addressed world modeling well, researchers created the Evolutionary Fuzzy Cognitive Map (E-FCM) to model the characters' variables and contexts with the causalities among them in serious games. As an extension of FCM, E-FCM models not only fuzzy causal relationships but also probabilistic causal relationships among the variables. It also allows asynchronous updates of the variables, so that they can evolve dynamically and \u2026", "total_citations": 59, "citation_graph": {"2010": 1, "2011": 4, "2012": 5, "2013": 6, "2014": 0, "2015": 6, "2016": 6, "2017": 4, "2018": 4, "2019": 4, "2020": 1, "2021": 5, "2022": 7, "2023": 3}}, {"title": "Goal-Driven Conceptual Blending: A Computational Approach for Creativity", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:_FxGoFyzp5QC", "authors": ["Boyang Li", "Alexander Zook", "Nicholas Davis", "Mark O Riedl"], "publication_date": "2012", "conference": "Proceedings of the 2012 International Conference on Computational Creativity", "description": "Conceptual blending has been proposed as a creative cognitive process, but most theories focus on the analysis of existing blends rather than mechanisms for the efficient construction of novel blends. While conceptual blending is a powerful model for creativity, there are many challenges related to the computational application of blending. Inspired by recent theoretical research, we argue that contexts and context-induced goals provide insights into algorithm design for creative systems using conceptual blending. We present two case studies of creative systems that use goals and contexts to efficiently produce novel, creative artifacts in the domains of story generation and virtual characters engaged in pretend play respectively.", "total_citations": 53, "citation_graph": {"2013": 7, "2014": 5, "2015": 5, "2016": 7, "2017": 4, "2018": 9, "2019": 4, "2020": 3, "2021": 7, "2022": 2}}, {"title": "Is gpt-3 a good data annotator?", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:2ywjKiB__4kC", "authors": ["Bosheng Ding", "Chengwei Qin", "Linlin Liu", "Lidong Bing", "Shafiq Joty", "Boyang Li"], "publication_date": "2022/12/20", "journal": "arXiv preprint arXiv:2212.10450", "description": "GPT-3 (Generative Pre-trained Transformer 3) is a large-scale autoregressive language model developed by OpenAI, which has demonstrated impressive few-shot performance on a wide range of natural language processing (NLP) tasks. Hence, an intuitive application is to use it for data annotation. In this paper, we investigate whether GPT-3 can be used as a good data annotator for NLP tasks. Data annotation is the process of labeling data that could be used to train machine learning models. It is a crucial step in the development of NLP systems, as it allows the model to learn the relationship between the input data and the desired output. Given the impressive language capabilities of GPT-3, it is natural to wonder whether it can be used to effectively annotate data for NLP tasks. In this paper, we evaluate the performance of GPT-3 as a data annotator by comparing it with traditional data annotation methods and analyzing its output on a range of tasks. Through this analysis, we aim to provide insight into the potential of GPT-3 as a general-purpose data annotator in NLP.", "total_citations": 41, "citation_graph": {"2023": 41}}, {"title": "An empirical study on the relation between network interpretability and adversarial robustness", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:EPG8bYD4jVwC", "authors": ["Adam Noack", "Isaac Ahern", "Dejing Dou", "Boyang Li"], "publication_date": "2021/2", "journal": "SN Computer Science", "description": "Deep neural networks (DNNs) have had many successes, but they suffer from two major issues: (1) a vulnerability to adversarial examples and (2) a tendency to elude human interpretation. Interestingly, recent empirical and theoretical evidence suggests that these two seemingly disparate issues are actually connected. In particular, robust models tend to provide more interpretable gradients than non-robust models. However, whether this relationship works in the opposite direction remains obscure. With this paper, we seek empirical answers to the following question: can models acquire adversarial robustness when they are trained to have interpretable gradients? We introduce a theoretically inspired technique called Interpretation Regularization (IR), which encourages a model\u2019s gradients to (1) match the direction of interpretable target salience maps and (2) have small magnitude. To assess model performance \u2026", "total_citations": 40, "citation_graph": {"2018": 1, "2019": 1, "2020": 2, "2021": 11, "2022": 13, "2023": 12}}, {"title": "Crowdsourcing Open Interactive Narrative", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:PVgj2kMGcgYC", "authors": ["Matthew Guzdial", "Brent Harrison", "Boyang Li", "Mark O Riedl"], "publication_date": "2015", "conference": "The 10th International Conference on the Foundations of Digital Games", "description": "Interactive narrative is a form of digital interactive experience in which users influence a dramatic storyline through their actions. Artificial intelligence approaches to interactive narrative use a domain model to determine how the narrative should unfold based on user actions. However, domain models for interactive narrative require artificial intelligence and knowledge representation expertise. We present open interactive narrative, the problem of generating an interactive narrative experience about any possible topic. We present an open interactive narrative system\u2014Scherazade IF\u2014that learns a domain model from crowdsourced example stories so that the player can perform different actions and still receive a coherent story experience. We report on an evaluation of our system showing near-human level authoring.", "total_citations": 40, "citation_graph": {"2016": 6, "2017": 3, "2018": 9, "2019": 5, "2020": 9, "2021": 4, "2022": 0, "2023": 4}}, {"title": "Scheherazade: Crowd-Powered Interactive Narrative Generation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:2tRrZ1ZAMYUC", "authors": ["Boyang Li", "Mark O Riedl"], "publication_date": "2015", "conference": "The 29th AAAI Conferece on Artificial Intelligence", "description": "Interactive narrative is a form of storytelling in which users affect a dramatic storyline through actions by assuming the role of characters in a virtual world. This extended abstract outlines the Scheherazade-IF system, which uses crowdsourcing and artificial intelligence to automatically construct text-based interactive narrative experiences.", "total_citations": 36, "citation_graph": {"2015": 1, "2016": 2, "2017": 4, "2018": 7, "2019": 7, "2020": 6, "2021": 4, "2022": 3, "2023": 2}}, {"title": "NormLime: A new feature importance metric for explaining deep neural networks", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:-95Q15plzcUC", "authors": ["Isaac Ahern", "Adam Noack", "Luis Guzman-Nateras", "Dejing Dou", "Boyang Li", "Jun Huan"], "publication_date": "2019/9/10", "journal": "arXiv preprint arXiv:1909.04200", "description": "The problem of explaining deep learning models, and model predictions generally, has attracted intensive interest recently. Many successful approaches forgo global approximations in order to provide more faithful local interpretations of the model's behavior. LIME develops multiple interpretable models, each approximating a large neural network on a small region of the data manifold and SP-LIME aggregates the local models to form a global interpretation. Extending this line of research, we propose a simple yet effective method, NormLIME for aggregating local models into global and class-specific interpretations. A human user study strongly favored class-specific interpretations created by NormLIME to other feature importance metrics. Numerical experiments confirm that NormLIME is effective at recognizing important features.", "total_citations": 35, "citation_graph": {"2020": 9, "2021": 7, "2022": 6, "2023": 13}}, {"title": "Collaborative Storytelling between Robot and Child: A Feasibility Study", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:WHdLCjDvYFkC", "authors": ["Ming Sun", "Iolanda Leite", "Jill Fain Lehman", "Boyang Li"], "publication_date": "2017", "conference": "Proceedings of the 2017 Conference on Interaction Design and Children", "description": "Joint storytelling is a common parent-child activity and brings multiple benefits such as improved language learning for children. Most existing storytelling robots offer rigid interaction with children and do not contribute to children's stories. In this paper, we envision a robot that collaborates with a child to create oral stories in a highly interactive manner. We performed a Wizard-of-Oz feasibility study, which involved 78 children between 4 and 10 years old, to compare two collaboration strategies: (1) inserting new story content and relating it to the existing story and (2) inserting content without relating it to the existing story. We hypothesize the first strategy can foster true collaboration and create rapport, whereas the second is a safe strategy when the robot cannot understand the story. We observed that, although the first strategy creates a heavier cognitive load, it was as enjoyable as the second. We also observed \u2026", "total_citations": 34, "citation_graph": {"2017": 2, "2018": 2, "2019": 1, "2020": 3, "2021": 11, "2022": 11, "2023": 4}}, {"title": "A multi-task neural approach for emotion attribution, classification, and summarization", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:QyXJ3EUuO1IC", "authors": ["Guoyun Tu", "Yanwei Fu", "Boyang Li", "Jiarui Gao", "Yu-Gang Jiang", "Xiangyang Xue"], "publication_date": "2019/6/11", "journal": "IEEE Transactions on Multimedia", "description": "Emotional content is a crucial ingredient in user-generated videos. However, the sparsity of emotional expressions in the videos poses an obstacle to visual emotion analysis. In this paper, we propose a new neural approach, Bi-stream Emotion Attribution-Classification Network (BEAC-Net), to solve three related emotion analysis tasks: emotion recognition, emotion attribution, and emotion-oriented summarization, in a single integrated framework. BEAC-Net has two major constituents, an attribution network and a classification network. The attribution network extracts the main emotional segment that classification should focus on in order to mitigate the sparsity issue. The classification network utilizes both the extracted segment and the original video in a bi-stream architecture. We contribute a new dataset for the emotion attribution task with human-annotated ground-truth labels for emotion segments. Experiments \u2026", "total_citations": 32, "citation_graph": {"2020": 7, "2021": 11, "2022": 7, "2023": 7}}, {"title": "Noise-resistant Deep Metric Learning with Ranking-based Instance Selection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:mWEH9CqjF64C", "authors": ["Chang Liu", "Han Yu", "Boyang Li", "Zhiqi Shen", "Zhanning Gao", "Peiran Ren", "Xuansong Xie", "Lizhen Cui", "Chunyan Miao"], "publication_date": "2021/3/30", "conference": "CVPR", "description": "The existence of noisy labels in real-world data negatively impacts the performance of deep learning models. Although much research effort has been devoted to improving robustness to noisy labels in classification tasks, the problem of noisy labels in deep metric learning (DML) remains open. In this paper, we propose a noise-resistant training technique for DML, which we name Probabilistic Ranking-based Instance Selection with Memory (PRISM). PRISM identifies noisy data in a minibatch using average similarity against image features extracted by several previous versions of the neural network. These features are stored in and retrieved from a memory bank. To alleviate the high computational cost brought by the memory bank, we introduce an acceleration method that replaces individual data points with the class centers. In extensive comparisons with 12 existing approaches under both synthetic and real-world label noise, PRISM demonstrates superior performance of up to 6.06% in Precision@ 1.", "total_citations": 31, "citation_graph": {"2021": 3, "2022": 16, "2023": 12}}, {"title": "Evolutionary organizational search", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:2osOgNQ5qMEC", "authors": ["Boyang Li", "Han Yu", "Zhiqi Shen", "Chunyan Miao"], "publication_date": "2009/5/10", "book": "Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems-Volume 2", "description": "In this paper, we proposed Evolutionary Organizational Search (EOS), an optimization method for the organizational control of multi-agent systems (MASs) based on genetic programming (GP). EOS adds to the existing armory a metaheuristic extension, which is capable of efficient search and less vulnerable to stalling at local optima than greedy methods due to its stochastic nature. EOS employs a flexible genotype which can be applied to a wide range of tree-shaped organizational forms. EOS also considers special constraints of MASs. A novel mutation operator, the redistribution operator, was proposed. Experiments optimizing an information retrieval system illustrated the adaptation of solutions generated by EOS to environmental changes.", "total_citations": 30, "citation_graph": {"2011": 2, "2012": 1, "2013": 3, "2014": 5, "2015": 6, "2016": 8, "2017": 0, "2018": 1, "2019": 1, "2020": 1, "2021": 1}}, {"title": "Predicting personality from book preferences with user-generated content labels", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:wKETBy42zhYC", "authors": ["Ng Annalyn", "Maarten W Bos", "Leonid Sigal", "Boyang Li"], "publication_date": "2018/2/23", "journal": "IEEE Transactions on Affective Computing", "description": "Psychological studies have shown that personality traits are associated with book preferences. However, past findings based on questionnaires are limited to conventional book genres and do not capture niche content (e.g., family drama) and reading behaviors (e.g., backburners). For a more comprehensive measure of book content, this study harnesses a massive archive of content labels, also known as `tags', created by users of a book review website, Goodreads.com. Combined with data on preferences and personality scores collected from Facebook users, the tag labels achieve high accuracy in personality prediction by psychological standards. Additionally, we group tags into broader genres to check their validity against past findings. Our results are robust across both tag-level and genre-level analyses and are consistent with existing literature. Moreover, user-generated tag labels reveal unexpected \u2026", "total_citations": 29, "citation_graph": {"2018": 3, "2019": 3, "2020": 4, "2021": 6, "2022": 6, "2023": 7}}, {"title": "Deep static and dynamic level analysis: A study on Infinite Mario", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:kVjdVfd2voEC", "authors": ["Matthew Guzdial", "Nathan Sturtevant", "Boyang Li"], "publication_date": "2016/9/19", "journal": "Experimental AI in Games Workshop", "description": "Automatic analysis of game levels can provide as-sistance to game designers and procedural content generation. We introduce a static-dynamic scale to categorize level analysis strategies, which captures the extent that the analysis depends on player simulation. Due to its ability to automatically learn intermediate representations for the task, a convolutional neural network (CNN) provides a general tool for both types of analysis. In this paper, we explore the use of CNN to analyze 1,437 Infinite Mario levels. We further propose a deep reinforcement learning technique for dynamic analysis, which allows the simulated player to pay a penalty to reduce error in its control. We empirically demonstrate the effectiveness of our techniques and complementarity of dynamic and static analysis.", "total_citations": 29, "citation_graph": {"2016": 1, "2017": 6, "2018": 8, "2019": 5, "2020": 4, "2021": 3, "2022": 2}}, {"title": "Semi-situated learning of verbal and nonverbal content for repeated human-robot interaction", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:cK4Rrx0J3m0C", "authors": ["Iolanda Leite", "Andr\u00e9 Pereira", "Allison Funkhouser", "Boyang Li", "Jill Fain Lehman"], "publication_date": "2016/10/31", "book": "Proceedings of the 18th ACM International Conference on Multimodal Interaction", "description": "Content authoring of verbal and nonverbal behavior is a limiting factor when developing agents for repeated social interactions with the same user. We present PIP, an agent that crowdsources its own multimodal language behavior using a method we call semi-situated learning. PIP renders segments of its goal graph into brief stories that describe future situations, sends the stories to crowd workers who author and edit a single line of character dialog and its manner of expression, integrates the results into its goal state representation, and then uses the authored lines at similar moments in conversation. We present an initial case study in which the language needed to host a trivia game interaction is learned pre-deployment and tested in an autonomous system with 200 users \"in the wild.\" The interaction data suggests that the method generates both meaningful content and variety of expression.", "total_citations": 28, "citation_graph": {"2017": 4, "2018": 8, "2019": 7, "2020": 3, "2021": 2, "2022": 3}}, {"title": "From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:AYInfyleIOsC", "authors": ["Jiaxian Guo", "Junnan Li", "Dongxu Li", "Anthony Meng Huat Tiong", "Boyang Li", "Dacheng Tao", "Steven Hoi"], "publication_date": "2023", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "description": "Large language models (LLMs) have demonstrated excellent zero-shot generalization to new language tasks. However, effective utilization of LLMs for zero-shot visual question-answering (VQA) remains challenging, primarily due to the modality disconnection and task disconnection between LLM and VQA task. End-to-end training on vision and language data may bridge the disconnections, but is inflexible and computationally expensive. To address this issue, we propose Img2Prompt, a plug-and-play module that provides the prompts that can bridge the aforementioned modality and task disconnections, so that LLMs can perform zero-shot VQA tasks without end-to-end training. In order to provide such prompts, we further employ LLM-agnostic models to provide prompts that can describe image content and self-constructed question-answer pairs, which can effectively guide LLM to perform zero-shot VQA tasks. Img2Prompt offers the following benefits: 1) It can flexibly work with various LLMs to perform VQA. 2) Without the needing of end-to-end training, it significantly reduces the cost of deploying LLM for zero-shot VQA tasks. 3) It achieves comparable or better performance than methods relying on end-to-end training. For example, we outperform Flamingo by 5.6% on VQAv2. On the challenging A-OKVQA dataset, our method even outperforms few-shot methods by as much as 20%.", "total_citations": 26, "citation_graph": {"2023": 25}}, {"title": "Robust and Authorable Multiplayer Storytelling Experiences", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:YsMSGLbcyi4C", "authors": ["Mark Riedl", "Boyang Li", "Hua Ai", "Ashwin Ram"], "publication_date": "2011/9/10", "conference": "The 7th Conference on Artificial Intelligence for Interactive Digital Entertainment (AIIDE)", "description": "Interactive narrative systems attempt to tell stories to players capable of changing the direction and/or outcome of the story. Despite the growing importance of multiplayer social experiences in games, little research has focused on multiplayer interactive narrative experiences. We performed a preliminary study to determine how human directors design and execute multiplayer interactive story experiences in online and real world environments. Based on our observations, we developed the Multiplayer Storytelling Engine that manages a story world at the individual and group levels. Our flexible story representation enables human authors to naturally model multiplayer narrative experiences. An intelligent execution algorithm detects when the author's story representation fails to account for player behaviors and automatically generates a branch to restore the story to the authors' original intent, thus balancing authorability against robust multiplayer execution.", "total_citations": 25, "citation_graph": {"2012": 4, "2013": 1, "2014": 0, "2015": 0, "2016": 4, "2017": 2, "2018": 3, "2019": 4, "2020": 0, "2021": 1, "2022": 1, "2023": 4}}, {"title": "Memetic gradient search", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:d1gkVwhDpl0C", "authors": ["Boyang Li", "Yew-Soon Ong", "Minh Nghia Le", "Chi Keong Goh"], "publication_date": "2008/6/1", "conference": "IEEE Congress on Evolutionary Computation (CEC), 2008.", "description": "This paper reviews the different gradient-based schemes and the sources of gradient, their availability, precision and computational complexity, and explores the benefits of using gradient information within a memetic framework in the context of continuous parameter optimization, which is labeled here as memetic gradient search. In particular, we considered a quasi-Newton method with analytical gradient and finite differencing, as well as simultaneous perturbation stochastic approximation, used as the local searches. Empirical study on the impact of using gradient information showed that memetic gradient search outperformed the traditional GA and analytical, precise gradient brings considerable benefit to gradient-based local search (LS) schemes. Though gradient-based searches can sometimes get trapped in local optima, memetic gradient searches were still able to converge faster than the conventional GA.", "total_citations": 25, "citation_graph": {"2008": 1, "2009": 1, "2010": 1, "2011": 5, "2012": 2, "2013": 1, "2014": 2, "2015": 4, "2016": 3, "2017": 1, "2018": 0, "2019": 1, "2020": 0, "2021": 2, "2022": 0, "2023": 1}}, {"title": "Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:kJDgFkosVoMC", "authors": ["Anthony Meng Huat Tiong", "Junnan Li", "Boyang Li", "Silvio Savarese", "Steven CH Hoi"], "publication_date": "2022/11", "conference": "EMNLP Findings", "description": "Visual question answering (VQA) is a hallmark of vision and language reasoning and a challenging task under the zero-shot setting. We propose Plug-and-Play VQA (PNP-VQA), a modular framework for zero-shot VQA. In contrast to most existing works, which require substantial adaptation of pretrained language models (PLMs) for the vision modality, PNP-VQA requires no additional training of the PLMs. Instead, we propose to use natural language and network interpretation as an intermediate representation that glues pretrained models together. We first generate question-guided informative image captions, and pass the captions to a PLM as context for question answering. Surpassing end-to-end trained baselines, PNP-VQA achieves state-of-the-art results on zero-shot VQAv2 and GQA. With 11B parameters, it outperforms the 80B-parameter Flamingo model by 8.5% on VQAv2. With 738M PLM parameters \u2026", "total_citations": 24, "citation_graph": {"2023": 23}}, {"title": "Multiplicative Representations for Unsupervised Semantic Role Induction", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:ubry08Y2EpUC", "authors": ["Yi Luan", "Yangfeng Ji", "Hannaneh Hajishirzi", "Boyang Li"], "publication_date": "2016", "conference": "The 54th Annual Meeting of the Association for Computational Linguistics", "description": "In unsupervised semantic role labeling, identifying the role of an argument is usually informed by its dependency relation with the predicate. In this work, we propose a neural model to learn argument embeddings from the context by explicitly incorporating dependency relations as multiplicative factors, which bias argument embeddings according to their dependency roles. Our model outperforms existing state-of-the-art embeddings in unsupervised semantic role induction on the CoNLL 2008 dataset and the SimLex999 word similarity task. Qualitative results demonstrate our model can effectively bias argument embeddings based on their dependency role.", "total_citations": 24, "citation_graph": {"2016": 2, "2017": 5, "2018": 5, "2019": 6, "2020": 1, "2021": 3, "2022": 0, "2023": 2}}, {"title": "Automated Scenario Adaptation in Support of Intelligent Tutoring Systems", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:zYLM7Y9cAGgC", "authors": ["James Niehaus", "Boyang Li", "Mark O Riedl"], "publication_date": "2010", "conference": "24th Conference of the Florida Artificial Intelligence Research Society", "description": "Learners may develop expertise by experiencing numerous different but relevant situations. Computer games and virtual simulations can facilitate these training opportunities, however, because of the relative difficulty in authoring new scenarios, the increasing need for new and different scenarios becomes a bottleneck in the learning process. Furthermore, a one-size-fits-all scenario may not address all of the abilities, needs, or goals of a particular learner. To address these issues we present a novel technique, Automated Scenario Adaptation, to automatically\n\u201crewrite\u201d narrative scenario content to suit individual learners\u2019 needs and abilities and to incorporate recent changes from real world learning needs. Scenario adaptation acts as problem generation for intelligent tutoring systems, producing greater learning opportunities that facilitate engagement and continued learner involvement.", "total_citations": 24, "citation_graph": {"2012": 3, "2013": 6, "2014": 4, "2015": 3, "2016": 3, "2017": 1, "2018": 3, "2019": 1}}, {"title": "Automatically learning to tell stories about social situations from the crowd", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:hqOjcs7Dif8C", "authors": ["Boyang Li", "Stephen Lee-Urban", "Darren Scott Appling", "Mark O Riedl"], "publication_date": "2012", "conference": "2012 Workshop on Computational Models of Narrative", "description": "Narrative intelligence is the use of narrative to make sense of the world and to communicate with other people. The generation of stories involving social and cultural situations (eating at a restaurant, going on a date, etc.) requires an extensive amount of experiential knowledge. While this knowledge can be encoded in the form of scripts, schemas, or frames, the manual authoring of these knowledge structures presents a significant bottleneck in the creation of systems demonstrating narrative intelligence. In this paper we describe a technique for automatically learning robust, script-like knowledge from crowdsourced narratives. Crowdsourcing, the use of anonymous human workers, provides an opportunity for rapidly acquiring a corpus of highly specialized narratives about sociocultural situations. We describe a three-stage approach to script acquisition and learning. First, we query human workers to write natural language narrative examples of a given situation. Second, we learn the set of possible events that can occur in a situation by finding semantic similarities between the narrative examples. Third, we learn the relevance of any event to the situation and extract a probable temporal ordering between events. We describe how these scripts, which we call plot graphs, can be utilized to generate believable stories about social situations.", "total_citations": 23, "citation_graph": {"2012": 5, "2013": 2, "2014": 6, "2015": 2, "2016": 1, "2017": 0, "2018": 2, "2019": 1, "2020": 2, "2021": 2}}, {"title": "Proof of Learning (PoLe): empowering neural network training with consensus building on blockchains", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:LXmCCkuhhTsC", "authors": ["Yuan Liu", "Yixiao Lan", "Boyang Li", "Chunyan Miao", "Zhihong Tian"], "publication_date": "2021/12/24", "journal": "Computer Networks", "description": "The advent of neural network (NN) based deep learning, especially the recent development of the automatic design of networks, has brought unprecedented performance gains at heavy computational cost. On the other hand, in order to generate a new consensus block, Proof of Work (PoW) based blockchain systems routinely perform a huge amount of computation that does not achieve practical purposes but to solving a difficult cryptographic hash puzzle problem.In this study, we propose a new consensus mechanism, Proof of Learning (PoLe), which directs the computation spent for block consensus toward optimization of neural networks. In our design, the training and testing data are released to the entire blockchain network and the consensus nodes train NN models on the data, which serves as the proof of learning. As a core component of PoLe, we design a secure mapping layer (SML) to prevent consensus \u2026", "total_citations": 22, "citation_graph": {"2021": 1, "2022": 7, "2023": 14}}, {"title": "Proof of Learning (PoLe): Empowering machine learning with consensus building on blockchains", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:oi2SiIJ9l4AC", "authors": ["Yixiao Lan", "Yuan Liu", "Boyang Li", "Chunyan Miao"], "publication_date": "2021/5/18", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "description": "The consensus algorithm is the core component of a blockchain system, which determines the efficiency, security, and scalability of the blockchain network. The representative consensus algorithm is the proof of work (PoW) proposed in Bitcoin, where the consensus process consumes large amount of compute in solving meaningless Hash puzzel. Meanwhile, the deep learning (DL) has brought unprecedented performance gains at heavy computate cost. In this demo, we channels the otherwise wasted computational power to the practical purpose of training neural network models, through the proposed proof of learning (PoL) consensus algorithm. In PoLe, the training/testing data are released to the entire blockchain network (BCN) and the consensus nodes train NN models on the data, which serves as the proof of learning. When the consensus on the BCN considers a NN model to be valid, a new block is appended to the blockchain. Through our system, we investigate the potential of enpowering machine learning with consensus building on blockchains.", "total_citations": 22, "citation_graph": {"2021": 3, "2022": 10, "2023": 9}}, {"title": "Efficient self-supervised vision pretraining with local masked reconstruction", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:qe6vwMD2xtsC", "authors": ["Jun Chen", "Ming Hu", "Boyang Li", "Mohamed Elhoseiny"], "publication_date": "2022/6/1", "journal": "arXiv preprint arXiv:2206.00790", "description": "Self-supervised learning for computer vision has achieved tremendous progress and improved many downstream vision tasks such as image classification, semantic segmentation, and object detection. Among these, generative self-supervised vision learning approaches such as MAE and BEiT show promising performance. However, their global masked reconstruction mechanism is computationally demanding. To address this issue, we propose local masked reconstruction (LoMaR), a simple yet effective approach that performs masked reconstruction within a small window of 77 patches on a simple Transformer encoder, improving the trade-off between efficiency and accuracy compared to global masked reconstruction over the entire image. Extensive experiments show that LoMaR reaches 84.1% top-1 accuracy on ImageNet-1K classification, outperforming MAE by 0.5%. After finetuning the pretrained LoMaR on 384384 images, it can reach 85.4% top-1 accuracy, surpassing MAE by 0.6%. On MS COCO, LoMaR outperforms MAE by 0.5 on object detection and 0.5 on instance segmentation. LoMaR is especially more computation-efficient on pretraining high-resolution images, e.g., it is 3.1 faster than MAE with 0.2% higher classification accuracy on pretraining 448448 images. This local masked reconstruction learning mechanism can be easily integrated into any other generative self-supervised learning approach. Our code will be publicly available.", "total_citations": 21, "citation_graph": {"2022": 4, "2023": 17}}, {"title": "HyDRA: Hypergradient Data Relevance Analysis for Interpreting Deep Neural Networks", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:5bg8sr1QxYwC", "authors": ["Yuanyuan Chen", "Boyang Li", "Han Yu", "Pengcheng Wu", "Chunyan Miao"], "publication_date": "2021", "conference": "AAAI", "description": "The behaviors of deep neural networks (DNNs) are notoriously resistant to human interpretations. In this paper, we propose Hypergradient Data Relevance Analysis, or HyDRA, which interprets the predictions made by DNNs as effects of their training data. Existing approaches generally estimate data contributions around the final model parameters and ignore how the training data shape the optimization trajectory. By unrolling the hypergradient of test loss wrt the weights of training data, HyDRA assesses the contribution of training data toward test data points throughout the training trajectory. In order to accelerate computation, we remove the Hessian from the calculation and prove that, under moderate conditions, the approximation error is bounded. Corroborating this theoretical claim, empirical results indicate the error is indeed small. In addition, we quantitatively demonstrate that HyDRA outperforms influence functions in accurately estimating data contribution and detecting noisy data labels. The source code is available at https://github. com/cyyever/aaai_hydra.", "total_citations": 21, "citation_graph": {"2021": 5, "2022": 10, "2023": 6}}, {"title": "A Neural Multi-sequence Alignment TeCHnique (NeuMATCH)", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:-mN3Mh-tlDkC", "authors": ["Pelin Dogan", "Boyang Li", "Leonid Sigal", "Markus Gross"], "publication_date": "2018/2/19", "conference": "CVPR", "description": "The alignment of heterogeneous sequential data (video to text) is an important and challenging problem. Standard techniques for this task, including Dynamic Time Warping (DTW) and Conditional Random Fields (CRFs), suffer from inherent drawbacks. Mainly, the Markov assumption implies that, given the immediate past, future alignment decisions are independent of further history. The separation between similarity computation and alignment decision also prevents end-to-end training. In this paper, we propose an end-to-end neural architecture where alignment actions are implemented as moving data between stacks of Long Short-term Memory (LSTM) blocks. This flexible architecture supports a large variety of alignment tasks, including one-to-one, one-to-many, skipping unmatched elements, and (with extensions) non-monotonic alignment. Extensive experiments on semi-synthetic and real datasets show that our algorithm outperforms state-of-the-art baselines.", "total_citations": 21, "citation_graph": {"2017": 1, "2018": 0, "2019": 4, "2020": 5, "2021": 4, "2022": 5, "2023": 2}}, {"title": "Storytelling with adjustable narrator styles and sentiments", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:LgRImbQfgY4C", "authors": ["Boyang Li", "Mohini Thakkar", "Yijie Wang", "Mark O Riedl"], "publication_date": "2014", "conference": "Interactive Storytelling: 7th International Conference on Interactive Digital Storytelling, ICIDS 2014, Singapore, Singapore, November 3-6, 2014, Proceedings 7", "description": "Most storytelling systems to date rely on manually coded knowledge, the cost of which usually restricts such systems to operate within a few domains where knowledge has been engineered. Open Story Generation systems are capable of learning knowledge necessary for telling stories in a given domain. In this paper, we describe a technique that generates and communicates stories in language with diverse styles and sentiments based on automatically learned narrative knowledge. Diversity in storytelling style may facilitate different communicative goals and focalization in narratives. Our approach learns from large-scale data sets such as the Google N-Gram Corpus and Project Gutenberg books in addition to crowdsourced stories to instill storytelling agents with linguistic and social behavioral knowledge. A user study shows our algorithm strongly agrees with human judgment on the interestingness \u2026", "total_citations": 20, "citation_graph": {"2015": 3, "2016": 1, "2017": 6, "2018": 5, "2019": 2, "2020": 1, "2021": 1, "2022": 1}}, {"title": "Data-Driven Alibi Story Telling for Social Believability", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:tH6gc1N1XXoC", "authors": ["Boyang Li", "Mohini Thakkar", "Yijie Wang", "Mark O Riedl"], "publication_date": "2014", "conference": "The Social Believability in Games Workshops", "description": "As computer games adopt larger, more life-like virtual worlds, socially believable characters become progressively more important. Socially believable non-player characters (NPCs) must be able to act in social situations and communicate with human players. In this paper, we address one aspect of social believability: the construction and telling of alibi stories, or an artificial background that explains what a character has been doing while not in the presence of the human player. We describe a technique for generating alibi stories and communicating the alibi stories via natural language. Our approach uses machine learning to overcome knowledge engineering bottlenecks necessary to instill intelligent characters with social behavioral knowledge. Alibi stories are subsequently generated from learned social behavioral knowledge. By leveraging the Google N-Gram Corpus and Project Gutenberg books, natural language is generated with a discourse planner and text generation that incorporate different expressivity and sentiment, which can be employed to create NPCs with a variety of personal traits.", "total_citations": 20, "citation_graph": {"2014": 3, "2015": 4, "2016": 4, "2017": 3, "2018": 4, "2019": 1, "2020": 0, "2021": 0, "2022": 1}}, {"title": "The national weather sensor grid", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:u-x6o8ySG0sC", "authors": ["Hock Beng Lim", "Keck Voon Ling", "Wenqiang Wang", "Yuxia Yao", "Mudasser Iqbal", "Boyang Li", "Xiaonan Yin", "Tarun Sharma"], "publication_date": "2007/11/6", "book": "Proceedings of the 5th international conference on embedded networked sensor systems", "description": "With the rapid advances in technologies such as MEMS sensors, low-power embedded processing and wireless networking, sensor networks are becoming more powerful in terms of data acquisition and processing capabilities. Sensor networks can now be deployed in the physical world for various important applications such as environmental monitoring, weather monitoring and modeling, military surveillance, healthcare monitoring, tracking of goods and manufacturing processes, smart homes and offices, etc.", "total_citations": 20, "citation_graph": {"2008": 2, "2009": 2, "2010": 2, "2011": 1, "2012": 1, "2013": 0, "2014": 0, "2015": 1, "2016": 2, "2017": 2, "2018": 1, "2019": 2, "2020": 2, "2021": 1, "2022": 1}}, {"title": "Learning Knowledge to Support Domain-independent Narrative Intelligence", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:sJsF-0ZLhtgC", "authors": ["Boyang Li"], "publication_date": "2015/5", "description": "Our dreams and stories may contain implicit aspects of our lives even without our awareness.", "total_citations": 19, "citation_graph": {"2015": 4, "2016": 3, "2017": 3, "2018": 3, "2019": 4, "2020": 1, "2021": 0, "2022": 0, "2023": 1}}, {"title": "Predicting the Quality of Short Narratives from Social Media", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:7wO8s98CvbsC", "authors": ["Tong Wang", "Ping Chen", "Boyang Li"], "publication_date": "2017", "conference": "The 26th International Joint Conference on Artificial Intelligence (IJCAI)", "description": "An important and difficult challenge in building computational models for narratives is the automatic evaluation of narrative quality. Quality evaluation connects narrative understanding and generation as generation systems need to evaluate their own products. To circumvent difficulties in acquiring annotations, we employ upvotes in social media as an approximate measure for story quality. We collected 54,484 answers from a crowd-powered question-and-answer website, Quora, and then used active learning to build a classifier that labeled 28,320 answers as stories. To predict the number of upvotes without the use of social network features, we create neural networks that model textual regions and the interdependence among regions, which serve as strong benchmarks for future research. To our best knowledge, this is the first large-scale study for automatic evaluation of narrative quality.", "total_citations": 17, "citation_graph": {"2017": 3, "2018": 2, "2019": 3, "2020": 2, "2021": 5, "2022": 2}}, {"title": "Distributed Creative Cognition In Digital Filmmaking", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:IjCSPb-OGe4C", "authors": ["Nicholas Davis", "Boyang Li", "Brian O\u2019Neill", "Mark Riedl", "Michael Nitsche"], "publication_date": "2011", "conference": "Proceedings of the 8th ACM Conference on Creativity and Cognition", "description": "This paper reports on an empirical study that uses a Grounded Theory approach to investigate the creative practices of Machinima filmmakers. Machinima is a new digital film production technique that uses the 3D graphics and real time rendering capability of video game engines to create films. In contrast to practices used in traditional film production, we've found that Machinima filmmakers explore and evaluate ideas in real time. These filmmakers generate vague and underspecified mental images, which are then explored and refined using the real time rendering capabilities of game engines. The game engine assists the filmmaker to fill in indeterminate details, which allows creative exploration of scenes through playfully experimenting with parameters such as camera angle and position, lighting, and character position. Creative exploration distributes the cognitive task of evaluation between the human user and \u2026", "total_citations": 17, "citation_graph": {"2013": 6, "2014": 1, "2015": 3, "2016": 1, "2017": 1, "2018": 0, "2019": 1, "2020": 2, "2021": 1, "2022": 0, "2023": 1}}, {"title": "Latent-Optimized Adversarial Neural Transfer for Sarcasm Detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:SnGPuo6Feq8C", "authors": ["Xu Guo", "Boyang Li", "Han Yu", "Chunyan Miao"], "publication_date": "2021/4/19", "conference": "NAACL-HLT", "description": "The existence of multiple datasets for sarcasm detection prompts us to apply transfer learning to exploit their commonality. The adversarial neural transfer (ANT) framework utilizes multiple loss terms that encourage the source-domain and the target-domain feature distributions to be similar while optimizing for domain-specific performance. However, these objectives may be in conflict, which can lead to optimization difficulties and sometimes diminished transfer. We propose a generalized latent optimization strategy that allows different losses to accommodate each other and improves training dynamics. The proposed method outperforms transfer learning and meta-learning baselines. In particular, we achieve 10.02% absolute performance gain over the previous state of the art on the iSarcasm dataset.", "total_citations": 16, "citation_graph": {"2021": 1, "2022": 10, "2023": 5}}, {"title": "Toward Autonomous Crowd-Powered Creation of Interactive Narratives", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:MXK_kJrjxJIC", "authors": ["Boyang Li", "Stephen Lee-Urban", "Mark O Riedl"], "publication_date": "2012", "conference": "5th AAAI Workshop on Intelligent Narrative Technologies", "description": "Interactive narrative is a form of storytelling that adapts to actions performed by users who assume the roles of story characters. To date, interactive narratives are built by hand. In this paper, we introduce Scheherazade, an intelligent system that automatically creates an interactive narrative about any topic from crowdsourced narratives. Our system leverages the experience and creativity of humans by crowdsourcing a corpus of linear narrative examples. It then constructs an executable plot graph, which is a knowledge structure that defines the legal space of an interactive narrative, by learning the plot events, execution precedence, and event separations. We demonstrate the system can successfully construct an interactive narrative based on noisy human input.", "total_citations": 16, "citation_graph": {"2013": 3, "2014": 2, "2015": 5, "2016": 0, "2017": 2, "2018": 1, "2019": 0, "2020": 3}}, {"title": "Learning Sociocultural Knowledge via Crowdsourced Examples", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:Se3iqnhoufwC", "authors": ["Boyang Li", "Darren Scott Appling", "Stephen Lee-Urban", "Mark O Riedl"], "publication_date": "2012", "conference": "4th Human Computation Workshop with AAAI 2012", "description": "Computational systems can use sociocultural knowledge to understand human behavior and interact with humans in more natural ways. However, such systems are limited by their reliance on hand-authored sociocultural knowledge and models. We introduce an approach to automatically learn robust, script-like sociocultural knowledge from crowdsourced narratives. Crowdsourcing, the use of anonymous human workers, provides an opportunity for rapidly acquiring a corpus of examples of situations that are highly specialized for our purpose yet sufficiently varied, from which we can learn a versatile script. We describe a semiautomated process by which we query human workers to write natural language narrative examples of a given situation and learn the set of events that can occur and the typical even ordering.", "total_citations": 16, "citation_graph": {"2012": 4, "2013": 4, "2014": 3, "2015": 2, "2016": 2, "2017": 0, "2018": 0, "2019": 0, "2020": 1}}, {"title": "Exploring Long Tail Visual Relationship Recognition with Large Vocabulary", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:TlpoogIpr_IC", "authors": ["Sherif Abdelkarim", "Aniket Agarwal", "Panos Achlioptas", "Jun Chen", "Jiaji Huang", "Boyang Li", "Kenneth Church", "Mohamed Elhoseiny"], "publication_date": "2021", "conference": "ICCV", "description": "Several approaches have been proposed in recent literature to alleviate the long-tail problem, mainly in object classification tasks. In this paper, we make the first large-scale study concerning the task of Long-Tail Visual Relationship Recognition (LTVRR). LTVRR aims at improving the learning of structured visual relationships that come from the long-tail (eg,\" rabbit grazing on grass\"). In this setup, the subject, relation, and object classes each follow a long-tail distribution. To begin our study and make a future benchmark for the community, we introduce two LTVRR-related benchmarks, dubbed VG8K-LT and GQA-LT, built upon the widely used Visual Genome and GQA datasets. We use these benchmarks to study the performance of several state-of-the-art long-tail models on the LTVRR setup. Lastly, we propose a visiolinguistic hubless (VilHub) loss and a Mixup augmentation technique adapted to LTVRR setup, dubbed as RelMix. Both VilHub and RelMix can be easily integrated on top of existing models and despite being simple, our results show that they can remarkably improve the performance, especially on tail classes. Benchmarks, code, and models have been made available at: https://github. com/Vision-CAIR/LTVRR.", "total_citations": 15, "citation_graph": {"2021": 1, "2022": 6, "2023": 8}}, {"title": "PLOTSHOT: Generating Discourse-constrained Stories around Photos", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:v1_lew4L6wgC", "authors": ["Rogelio E Cardona-Rivera", "Boyang Li"], "publication_date": "2016", "conference": "12th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE)", "description": "Story generators typically adopt a pipelined model of generation wherein fabula structure is decided independently and prior to discourse structure. In this paper, we propose a novel story generator, PlotShot, capable of reasoning over discourse materials during fabula generation such that these materials meaningfully constrain the development of a causally and intentionally coherent story. PlotShot incorporates user-supplied photographs as optional story states through an oversubscription planning paradigm. Further, to leverage existing work on planning-based models of generation, we present a technique to compile the photo story planning problem to classical narrative planning. Our system attempts to maximize quality of an illustrated story by analyzing the affinity between a photo and the action it is meant to depict. An evaluation of generated artifacts shows advantage over heuristic baseline techniques.", "total_citations": 15, "citation_graph": {"2017": 2, "2018": 1, "2019": 7, "2020": 2, "2021": 1, "2022": 1, "2023": 1}}, {"title": "Dialog knowledge acquisition system and method", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:PyEswDtIyv0C", "publication_date": "2018/12/25", "description": "A dialog knowledge acquisition system includes a hardware processor, a memory, and hardware processor controlled input and output modules. The memory stores a dialog manager configured to instantiate a persistent interactive personality (PIP), and a dialog graph having linked dialog state nodes. The dialog manager receives dialog initiation data, identifies a first state node on the dialog graph corresponding to the dialog initiation data, determines a dialog interaction by the PIP based on the dialog initiation data and the first state node, and renders the dialog interaction. The dialog manager also receives feedback data corresponding to the dialog interaction, identifies a second state node based on the dialog initiation data, the dialog interaction, and the feedback data, and utilizes the dialog initiation data, the first state node, the dialog interaction, the feedback data, and the second state node to train the dialog \u2026", "total_citations": 14, "citation_graph": {"2004": 1, "2005": 0, "2006": 3, "2007": 0, "2008": 0, "2009": 1, "2010": 0, "2011": 1, "2012": 2, "2013": 1, "2014": 0, "2015": 2, "2016": 0, "2017": 0, "2018": 0, "2019": 1, "2020": 1, "2021": 0, "2022": 0, "2023": 1}}, {"title": "Annotating High-Level Structures of Short Stories and Personal Anecdotes", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:jU7OWUQzBzMC", "authors": ["Boyang Li", "Beth Cardier", "Tong Wang", "Florian Metze"], "publication_date": "2018", "conference": "The 11th Language Resources and Evaluation Conference (LREC)", "description": "Stories are a vital form of communication in human culture; they are employed daily to persuade, to elicit sympathy, or to convey a message. Computational understanding of human narratives, especially high-level narrative structures, remain limited to date. Multiple literary theories for narrative structures exist, but operationalization of the theories has remained a challenge. We developed an annotation scheme by consolidating and extending existing narratological theories, including Labov and Waletsky's (1967) functional categorization scheme and Freytag's (1863) pyramid of dramatic tension, and present 360 annotated short stories collected from online sources. In the future, this research will support an approach that enables systems to intelligently sustain complex communications with humans.", "total_citations": 13, "citation_graph": {"2019": 4, "2020": 0, "2021": 3, "2022": 4, "2023": 2}}, {"title": "An evolutionary framework for multi-agent organizations", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:buQ7SEKw-1sC", "authors": ["Boyang Li", "Han Yu", "Zhiqi Shen", "Lizhen Cui", "Victor R Lesser"], "publication_date": "2015/12/6", "conference": "2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)", "description": "The organizational design of a multi-agent system (MAS) is important for its efficiency, adaptability and robustness. However, finding suitable organizational structures for different MASs is a challenging problem. In this paper, we propose a Framework of Evolutionary Optimization for Agent Organizations (FEVOR) based on Genetic Programming for optimizing tree-structured MASs. FEVOR employs a flexible representation of organizations and may be applied to a wide range of organizational forms such as pure hierarchies, holarchies, and federations. Compared to existing work, FEVOR is capable of efficient quantitative search and less vulnerable to stalling at local optima due to its non-greedy nature. Extensive experiments for optimizing an information retrieval system have been conducted to demonstrate the advantages of FEVOR in generating suitable MAS organizations for adaptive environments.", "total_citations": 13, "citation_graph": {"2015": 1, "2016": 4, "2017": 1, "2018": 3, "2019": 2, "2020": 1, "2021": 0, "2022": 1}}, {"title": "Long-tail visual relationship recognition with a visiolinguistic hubless loss", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:F2UWTTQJPOcC", "authors": ["Sherif Abdelkarim", "Panos Achlioptas", "Jiaji Huang", "Boyang Li", "Kenneth Church", "Mohamed Elhoseiny"], "publication_date": "2020/3/25", "description": "Scaling up the vocabulary and complexity of current visual understanding systems is necessary in order to bridge the gap between human and machine visual intelligence. However, a crucial impediment to this end lies in the difficulty of generalizing to data distributions that come from real-world scenarios. Typically such distributions follow Zipf's law which states that only a small portion of the collected object classes will have abundant examples (head); while most classes will contain just a few (tail). In this paper, we propose to study a novel task concerning the generalization of visual relationships that are on the distribution's tail, i.e. we investigate how to help AI systems to better recognize rare relationships like <S:dog, P:riding, O:horse>, where the subject S, predicate P, and/or the object O come from the tail of the corresponding distributions. To achieve this goal, we first introduce two large-scale visual-relationship detection benchmarks built upon the widely used Visual Genome and GQA datasets. We also propose an intuitive evaluation protocol that gives credit to classifiers who prefer concepts that are semantically close to the ground truth class according to wordNet- or word2vec-induced metrics. Finally, we introduce a visiolinguistic version of a Hubless loss which we show experimentally that it consistently encourages classifiers to be more predictive of the tail classes while still being accurate on head classes. Our code and models are available on http://bit.ly/LTVRR.", "total_citations": 12, "citation_graph": {"2020": 4, "2021": 1, "2022": 2, "2023": 5}}, {"title": "Crowdsourcing Interactive Fiction Games", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:QIV2ME_5wuYC", "authors": ["Boyang Li", "Stephen Lee-Urban", "Mark O Riedl"], "publication_date": "2013", "conference": "The 8th International Conference on the Foundations of Digital Games (FDG)", "description": "Procedural generation of games has become an active research field. We present a system that automatically generates an interactive fiction (IF) by learning from crowdsourced corpora of example stories. We ask crowd workers from Amazon Mechanical Turk to write short stories about a given situation with simple language, from which a plot graph is learned, containing plot events, temporal precedence and mutual exclusion relations between the events. The plot graph describes an IF where players and non-player characters choose from executable events as determined by the plot graph. We demonstrate an IF learned from the domain of bank robbery.", "total_citations": 12, "citation_graph": {"2014": 1, "2015": 1, "2016": 1, "2017": 1, "2018": 2, "2019": 2, "2020": 2, "2021": 1, "2022": 1}}, {"title": "Teachable agents in virtual learning environments: A case study", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:qjMakFHDy7sC", "authors": ["Han Yu", "Chunyan Miao", "Xuehong Tao", "Zhiqi Shen", "Yundong Cai", "Boyang Li", "Yuan Miao"], "publication_date": "2009/10/26", "conference": "E-Learn: World Conference on E-Learning in Corporate, Government, Healthcare, and Higher Education", "description": "As intelligent agent and virtual world technologies advanced to new heights in the current decade, researchers from diverse fields have started to look into possible ways of applying these emerging technologies to improve e-learning systems. Nevertheless, most attempts focused on using pedagogical agents to achieve traditional e-learning goals which place heavy emphasis on learning outcomes. In this paper, we leverage on our previous research results\u2013the Multi-Agent Development Environment (MADE)-which drastically reduced to the technical hurdle of incorporating pedagogical agents into VLEs. We present a VLE augmented with teachable agents designed based on the experiential learning model (ELM) to facilitate the learning of high school science topics and demonstrate the efficacy of MADE in designing complex learning activities, designing teachable agents that complement immersive experiential \u2026", "total_citations": 12, "citation_graph": {"2011": 1, "2012": 0, "2013": 2, "2014": 0, "2015": 4, "2016": 4, "2017": 0, "2018": 0, "2019": 0, "2020": 0, "2021": 0, "2022": 0, "2023": 1}}, {"title": "A Phone That Cures Your Flu: Generating Imaginary Gadgets in Fictions with Planning and Analogies", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:Tyk-4Ss8FVUC", "authors": ["Boyang Li", "Mark O Riedl"], "publication_date": "2011/9/10", "conference": "4th Workshop on Intelligent Narrative Technologies (INT)", "description": "Most computational story generation systems lack the ability to generate new types of imaginary objects that play functional roles in stories, such as lightsabers in Star Wars. We present an algorithm that generates such imaginary objects, which we call gadgets, in order to extend the ontological expressivity of existing, planning-based story generation systems. The behavior of a gadget is represented as a plan including typical events that happen when the gadget is used. Our algorithm creates gadgets by extrapolating and merging one or more commonly known objects in order to achieve a narrative goal provided by an existing story generator. We extend partial-order planning to establish open conditions based on analogies between concepts related respectively to common objects and the gadget. We show the algorithm is capable of generating gadgets created by human.", "total_citations": 11, "citation_graph": {"2012": 1, "2013": 1, "2014": 0, "2015": 1, "2016": 2, "2017": 0, "2018": 2, "2019": 0, "2020": 2, "2021": 2}}, {"title": "Emotional agent in serious game (DINO)", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:_OXeSy2IsFwC", "authors": ["HL Zhang", "Zhiqi Shen", "Xuehong Tao", "Chunyan Miao", "Boyang Li", "Ailiya", "Yundong Cai"], "publication_date": "2009/5/10", "conference": "The 8th International Conference on Autonomous Agents and Multiagent Systems", "description": "In this paper, we introduce an novel emotional agent system in 3D virtual world based on OCC (Ortony, Clore and Collins) theory, FCM (Fuzzy Cognitive Map) and GoalNet. The agent system is designed based on Goal Net model. Emotional modeling and decision making are based on OCC and FCM inference. Emotions modeled by the OCC model are incorporated into FCM inference. It has been shown emotion has a great impact in decision making. The proposed agent system has been applied to design human-like dinosaur agents in the research project \u201cImmersion and Embodied Learning: Traces of Dinosaurs in Earth System Science\u201d supported by National Research Foundation (NRF) Singapore. In this project, we have developed an agent mediated immersive Interactive Digital Media that appropriately recreates and replays the traces of Earth\u2019s history using intelligent agent technology and 3D multi-user environment. The virtual world and emotional dinosaur agents are shown in the demo. The experiments conducted by students in secondary schools in Singapore have shown the emotional agents enable deep learning of earth science.", "total_citations": 11, "citation_graph": {"2010": 1, "2011": 1, "2012": 3, "2013": 1, "2014": 1, "2015": 2, "2016": 1, "2017": 1}}, {"title": "Semi-supervised federated heterogeneous transfer learning", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:g5Ck-dwhA_QC", "authors": ["Siwei Feng", "Boyang Li", "Han Yu", "Yang Liu", "Qiang Yang"], "publication_date": "2022/9/27", "journal": "Knowledge-Based Systems", "description": "Federated learning (FL) is a privacy-preserving paradigm that collaboratively train machine learning models with distributed data stored in different silos without exposing sensitive information. Different from most existing FL approaches requiring data from different parties share either the same feature space or sample ID space, federated transfer learning (FTL), which is a recently proposed FL concept, is designed for situations where data from different parties differ not only in samples but also in feature space. However, like most traditional FL approaches, FTL methods also suffer from issues caused by insufficiency of overlapping data. In this paper, we propose a novel FTL framework referred to as Semi-Supervised Federated Heterogeneous Transfer Learning (SFHTL) to leverage on the unlabeled non-overlapping samples to reduce model overfitting as a result of insufficient overlapping training samples in FL \u2026", "total_citations": 10, "citation_graph": {"2022": 5, "2023": 5}}, {"title": "Creating customized game experiences by leveraging human creative effort: A planning approach", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:Y0pCki6q_DkC", "authors": ["Boyang Li", "Mark O Riedl"], "publication_date": "2011", "journal": "Agents for Games and Simulations II: Trends in Techniques, Concepts and Design", "description": "The task of entertaining people has, until very recently, been the exclusive domain of humans. However, recent advances in Artificial Intelligence (AI) suggest that intelligent systems may be used to create dynamic and engaging real-time entertainment experiences. In this paper we consider a novel technique called Experience Adaptation. Experience Adaptation is an offline process that leverages human creative ability by taking human-authored specifications of desired user experiences and autonomously \u201cre-writing\u201d them based on unique requirements of individual users. In this chapter, we illustrate Experience Adaptation in the context of computer-based role-playing games in which player experience is highly dependent on an unfolding plotline. Our approach uses a plan refinement technique based on partial-order planning to (a) optimize the global structure of the plotline according to input from a \u2026", "total_citations": 10, "citation_graph": {"2011": 1, "2012": 0, "2013": 5, "2014": 1, "2015": 2, "2016": 0, "2017": 0, "2018": 0, "2019": 0, "2020": 0, "2021": 0, "2022": 0, "2023": 1}}, {"title": "Initialization Matters: Regularizing Manifold-informed Initialization for Neural Recommendation Systems", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:wvYxNZNCP7wC", "authors": ["Yinan Zhang", "Boyang Li", "Yong Liu", "Hao Wang", "Chunyan Miao"], "publication_date": "2021/6/9", "conference": "KDD", "description": "Proper initialization is crucial to the optimization and the generalization of neural networks. However, most existing neural recommendation systems initialize the user and item embeddings randomly. In this work, we propose a new initialization scheme for user and item embeddings called Laplacian Eigenmaps with Popularity-based Regularization for Isolated Data (LEPORID). LEPORID endows the embeddings with information regarding multi-scale neighborhood structures on the data manifold and performs adaptive regularization to compensate for high embedding variance on the tail of the data distribution. Exploiting matrix sparsity, LEPORID embeddings can be computed efficiently. We evaluate LEPORID in a wide range of neural recommendation models. In contrast to the recent surprising finding that the simple K-nearest-neighbor (KNN) method often outperforms neural recommendation systems, we show \u2026", "total_citations": 9, "citation_graph": {"2022": 5, "2023": 4}}, {"title": "Improving the Sample Efficiency of Prompt Tuning with Domain Adaptation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:X9ykpCP0fEIC", "authors": ["Xu Guo", "Boyang Li", "Han Yu"], "publication_date": "2022/11", "journal": "EMNLP Findings", "description": "Prompt tuning, or the conditioning of a frozen pretrained language model (PLM) with soft prompts learned from data, has demonstrated impressive performance on a wide range of NLP tasks. However, prompt tuning requires a large training dataset to be effective and is outperformed by finetuning the entire PLM in data-scarce regimes. Previous work \\citep{gu-etal-2022-ppt,vu-etal-2022-spot} proposed to transfer soft prompts pretrained on the source domain to the target domain. In this paper, we explore domain adaptation for prompt tuning, a problem setting where unlabeled data from the target domain are available during pretraining. We propose bOosting Prompt TunIng with doMain Adaptation (OPTIMA), which regularizes the decision boundary to be smooth around regions where source and target data distributions are similar. Extensive experiments demonstrate that OPTIMA significantly enhances the transferability and sample-efficiency of prompt tuning compared to strong baselines. Moreover, in few-shot settings, OPTIMA exceeds full-model tuning by a large margin.", "total_citations": 8, "citation_graph": {"2022": 2, "2023": 5}}, {"title": "Understanding Actors and Evaluating Personae with Gaussian Embeddings", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:-7ulzOJl1JYC", "authors": ["Hannah Kim", "Denys Katerenchuk", "Daniel Billet", "Jun Huan", "Haesun Park", "Boyang Li"], "publication_date": "2019", "conference": "The AAAI Conference on Artificial Intelligence", "description": "Understanding narrative content has become an increasingly popular topic. Nonetheless, research on identifying common types of narrative characters, or personae, is impeded by the lack of automatic and broad-coverage evaluation methods. We argue that computationally modeling actors provides benefits, including novel evaluation mechanisms for personae. Specifically, we propose two actor-modeling tasks, cast prediction and versatility ranking, which can capture complementary aspects of the relation between actors and the characters they portray. For an actor model, we present a technique for embedding actors, movies, character roles, genres, and descriptive keywords as Gaussian distributions and translation vectors, where the Gaussian variance corresponds to actors\u2019 versatility. Empirical results indicate that (1) the technique considerably outperforms TransE (Bordes et al. 2013) and ablation baselines and (2) automatically identified persona topics (Bamman, O\u2019Connor, and Smith 2013) yield statistically significant improvements in both tasks, whereas simplistic persona descriptors including age and gender perform inconsistently, validating prior research.", "total_citations": 8, "citation_graph": {"2019": 1, "2020": 2, "2021": 3, "2022": 2}}, {"title": "Learning and Reusing Dialog for Repeated Interactions with a Situated Social Agent", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:owLR8QvbtFgC", "authors": ["James Kennedy", "Iolanda Leite", "Andr\u00e9 Pereira", "Ming Sun", "Boyang Li", "Rishub Jain", "Ricson Cheng", "Eli Pincus", "Elizabeth J Carter", "Jill Fain Lehman"], "publication_date": "2017/8/27", "conference": "International Conference on Intelligent Virtual Agents (IVA)", "description": "Content authoring for conversations is a limiting factor in creating verbal interactions with intelligent virtual agents. Building on techniques utilizing semi-situated learning in an incremental crowdworking pipeline, this paper introduces an embodied agent that self-authors its own dialog for social chat. In particular, the autonomous use of crowdworkers is supplemented with a generalization method that borrows and assesses the validity of dialog across conversational states. We argue that the approach offers a community-focused tailoring of dialog responses that is not available in approaches that rely solely on statistical methods across big data. We demonstrate the advantages that this can bring to interactions through data collected from 486 conversations between a situated social agent and 22 users during a 3 week long evaluation period.", "total_citations": 8, "citation_graph": {"2018": 4, "2019": 0, "2020": 0, "2021": 2, "2022": 1, "2023": 1}}, {"title": "Scheduling Live Interactive Narratives with Mixed-Integer Linear Programming", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:CB2v5VPnA5kC", "authors": ["Sasha Azad", "Jingyang Xu", "Haining Yu", "Boyang Li"], "publication_date": "2017", "conference": "The 13th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE)", "description": "A live interactive narrative (LIN) is an experience where multiple players take on fictional roles and interact with real-world objects and actors to participate in a pre-authored narrative. Temporal properties of LINs are important to its viability and aesthetic quality and hence deserve special design consideration. In this paper, we tackle the largely overlooked problem of scheduling a multiplayer interactive narrative and propose the Live Interactive Narrative Scheduling Problem (LINSP), which handles reasoning under temporal uncertainty, resource scheduling, and non-linear plot choices. We present a mixed-integer linear programming formulation of the problem and empirically evaluates its scalability over large narrative instances.", "total_citations": 8, "citation_graph": {"2017": 1, "2018": 2, "2019": 3, "2020": 0, "2021": 1, "2022": 1}}, {"title": "Alignment of video and textual sequences for metadata analysis", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:FiDNX6EVdGUC", "publication_date": "2020/1/9", "description": "Systems, methods and computer program products related to aligning heterogeneous sequential data. A first sequential data stream and a second sequential data stream are received. An action related to aligning the first sequential data stream and the second sequential data stream is determined using an alignment neural network. The alignment neural network includes a fully connected layer that receives as input: data from the first sequential data stream, data from the second sequential data stream, and data relating to a previously determined action by the alignment neural network related to aligning the first sequential data stream and the second sequential data stream.", "total_citations": 7, "citation_graph": {"2020": 2, "2021": 2, "2022": 1, "2023": 2}}, {"title": "Creative Gadget Design in Fictions: Generalized Planning in Analogical Spaces", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:W7OEmFMy1HYC", "authors": ["Boyang Li", "Mark O Riedl"], "publication_date": "2011", "conference": "8th ACM Conference on Creativity and Cognition", "description": "Science-fiction and fantasy stories often contain objects never envisioned previously. Inventing gadgets like lightsabers or mythical creatures like griffins is a creative task. Traditional computational storytelling systems are limited in their expressivity because they cannot create new types of objects or gadgets. The Japanese manga series Doraemon exemplifies the role of new and creative gadgets in creating fun and successful stories. We surveyed five volumes of Doraemon and identified 9 cognitive strategies of gadget creation, unified in a 5-step process. We present an algorithm to create new types of gadgets in the context of story generation. The algorithm is a combination of partial-order planning and analogical reasoning. Although Doraemon is our motivating example, we can also generate gadgets commonly seen in other science fictions and fairy tales.", "total_citations": 7, "citation_graph": {"2012": 2, "2013": 0, "2014": 0, "2015": 2, "2016": 1, "2017": 0, "2018": 1, "2019": 0, "2020": 1}}, {"title": "Planning for Individualized Experiences with Quest-Centric Game Adaptation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:KlAtU1dfN6UC", "authors": ["Boyang Li", "Mark O Riedl"], "publication_date": "2010", "conference": "ICAPS 2010 Workshop on Planning in Games", "description": "Planning has been extensively used to build competent opponents in games for human players. In this paper, we focus not on winning strategies but on creating an enjoyable overall gaming experience by adapting human-authored game narratives and customizing them to the players\u2019 motivation, tastes and needs. We discuss the benefits of modeling game narratives as plans and analyze causal structures to build novel computational models of narrative coherence. A planning approach to the narrative adaptation problem is presented. The planner takes a complete storyline comprised of several quests and iteratively searches for modifications, deleting and inserting quests and events, until it meets the user\u2019s preferences. A user study strongly suggested the proposed notion of narrative coherence has positive influences on story aesthetics.", "total_citations": 7, "citation_graph": {"2010": 1, "2011": 2, "2012": 1, "2013": 0, "2014": 0, "2015": 0, "2016": 0, "2017": 0, "2018": 0, "2019": 1, "2020": 1, "2021": 1}}, {"title": "Federated learning for personalized humor recognition", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:sA9dB-pw3HoC", "authors": ["Xu Guo", "Han Yu", "Boyang Li", "Hao Wang", "Pengwei Xing", "Siwei Feng", "Zaiqing Nie", "Chunyan Miao"], "publication_date": "2022/5/4", "journal": "ACM Transactions on Intelligent Systems and Technology (TIST)", "description": "Computational understanding of humor is an important topic under creative language understanding and modeling. It can play a key role in complex human-AI interactions. The challenge here is that human perception of humorous content is highly subjective. The same joke may receive different funniness ratings from different readers. This makes it highly challenging for humor recognition models to achieve personalization in practical scenarios. Existing approaches are generally designed based on the assumption that users have a consensus on whether a given text is humorous or not. Thus, they cannot handle diverse humor preferences well. In this article, we propose the FedHumor approach for the recognition of humorous content in a personalized manner through Federated Learning (FL). Extending a pre-trained language model, FedHumor guides the fine-tuning process by considering diverse distributions \u2026", "total_citations": 6, "citation_graph": {"2022": 3, "2023": 3}}, {"title": "Synopses of movie narratives: a video-language dataset for story understanding", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:silx2ntsSuwC", "authors": ["Yidan Sun", "Qin Chao", "Yangfeng Ji", "Boyang Li"], "publication_date": "2022/3/11", "journal": "arXiv preprint arXiv:2203.05711", "description": "Despite recent advances of AI, story understanding remains an open and under-investigated problem. We collect, preprocess, and publicly release a video-language story dataset, Synopses of Movie Narratives (SyMoN), containing 5,193 video summaries of popular movies and TV series with a total length of 869 hours. SyMoN captures naturalistic storytelling videos made by human creators and intended for a human audience. As a prototypical and naturalistic story dataset, SyMoN features high coverage of multimodal story events and abundant mental-state descriptions. Its use of storytelling techniques cause cross-domain semantic gaps that provide appropriate challenges to existing models. We establish benchmarks on video-text retrieval and zero-shot alignment on movie summary videos, which showcase the importance of in-domain data and long-term memory in story understanding. With SyMoN, we hope to lay the groundwork for progress in multimodal story understanding.", "total_citations": 5, "citation_graph": {"2022": 2, "2023": 2}}, {"title": "Humor: A Dynamic and Dual-Process Theory with Computational Considerations", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:1lhNe0rCu4AC", "authors": ["Boyang Li"], "publication_date": "2016", "journal": "Advances in Cognitive Systems", "description": "The cognitive mechanism of humor has been studied for centuries, with multiple seemingly incompatible theories proposed. However, none of existing theories is capable of explaining all empirical evidence. Recent research suggests emotional appraisals are tightly coupled and closely interact with other types of cognitive processes to create complex emotions and affects. This entangled nature contributes to the difficulty of humor research. In this paper, I attempt to provide a single, unified framework of humor, grounded in recent theoretical developments on emotion and dualprocess cognition. I propose that humor comprehension consists of a quick succession of four major stages: surprise, reflection, dismissal, and compensation. The theory provides a modern update on existing theories of humor and is capable of explaining several phenomena that cannot be easily explained by existing theories. Finally, I outline a computational system that recognizes humor in the form of puns.", "total_citations": 5, "citation_graph": {"2017": 2, "2018": 2, "2019": 0, "2020": 0, "2021": 0, "2022": 1}}, {"title": "History-Aware Hierarchical Transformer for Multi-session Open-domain Dialogue System", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:eAlLMO4JVmQC", "authors": ["Tong Zhang", "Yong Liu", "Boyang Li", "Zhiwei Zeng", "Pengwei Wang", "Yuan You", "Chunyan Miao", "Lizhen Cui"], "publication_date": "2022/11", "conference": "EMNLP Findings", "description": "With the evolution of pre-trained language models, current open-domain dialogue systems have achieved great progress in conducting one-session conversations. In contrast, Multi-Session Conversation (MSC), which consists of multiple sessions over a long term with the same user, is under-investigated. In this paper, we propose History-Aware Hierarchical Transformer (HAHT) for multi-session open-domain dialogue. HAHT maintains a long-term memory of history conversations and utilizes history information to understand current conversation context and generate well-informed and context-relevant responses. Specifically, HAHT first encodes history conversation sessions hierarchically into a history memory. Then, HAHT leverages historical information to facilitate the understanding of the current conversation context by encoding the history memory together with the current context with attention-based mechanisms. Finally, to explicitly utilize historical information, HAHT uses a history-aware response generator that switches between a generic vocabulary and a history-aware vocabulary. Experimental results on a large-scale MSC dataset suggest that the proposed HAHT model consistently outperforms baseline models. Human evaluation results support that HAHT generates more human-like, context-relevant and history-relevant responses than baseline models.", "total_citations": 4, "citation_graph": {"2023": 4}}, {"title": "Systems and methods for determining semantic roles of arguments in sentences", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:L1USKYWJimsC", "publication_date": "2021/11/2", "description": "There is provided a system including a non-transitory memory storing an executable code and a hardware processor executing the executable code to receive an input sentence including a first predicate and at least a first argument depending from the first predicate, identify the first predicate, identify the first argument based on the first predicate, apply a dependency multiplication to determine a semantic role of the first argument based on the first predicate, and assign the first argument to an argument cluster including one or more similar arguments based on the semantic role of the first argument.", "total_citations": 4, "citation_graph": {"2020": 1, "2021": 1, "2022": 1, "2023": 1}}, {"title": "Long tail visual relationship recognition with hubless regularized relmix", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:aIdbFUkbNIkC", "authors": ["Sherif Abdelkarim", "Aniket Agarwal", "Panos Achlioptas", "Jun Chen", "Jiaji Huang", "Boyang Li", "Kenneth Church", "Mohamed Elhoseiny"], "publication_date": "2020", "journal": "arXiv preprint arXiv:2004.00436", "total_citations": 4, "citation_graph": {"2019": 1, "2020": 0, "2021": 1, "2022": 1, "2023": 1}}, {"title": "A Dynamic and Dual-Process Theory of Humor", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:uDGL6kOW6j0C", "authors": ["Boyang Li"], "publication_date": "2015/5", "conference": "Third Annual Conference on Advances in Cognitive Systems", "description": "The cognitive mechanism of humor has been studied for centuries, with multiple seemingly incompatible theories proposed. Recent research in emotions suggests human emotions are tightly coupled and closely interact with other types of cognitive processes. This entangled nature contributes to the difficulty of humor research. In this paper, I attempt to provide a single, unified framework of humor, grounded in recent developments on emotion and dual-process cognition. I propose that humor comprehension consists of a four-step dynamic process: surprise, reflection, dismissal and compensation. The proposed theory provides a modern update on existing theories of humor, and is capable of explaining several phenomena that cannot be easily explained by existing theories. I also discuss the implication of the theory on creating computational systems that can create or perceive humor.", "total_citations": 4, "citation_graph": {"2016": 1, "2017": 0, "2018": 0, "2019": 1, "2020": 1, "2021": 1}}, {"title": "Learning chinese characters with gestures", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:GFxP56DSvIMC", "authors": ["Jun Ji", "Han Yu", "Boyang Li", "Zhiqi Shen", "Chunyan Miao"], "publication_date": "2013", "journal": "International Journal of Information Technology", "description": "As China\u2019s economy continues to grow rapidly, there is increasing worldwide interest in learning the Chinese language. A significant common obstacle is learning to write the numerous Chinese characters, which bear little resemblance to Western alphabets. This research explores gesture-based interaction that allows the users to practice writing Chinese characters, each within a few minutes, and cultivates an in-depth and embodied understanding of the spatial organization and writing order of the strokes in the characters. We implement a prototype system based on Microsoft Kinect and report a study involving ten users. The user study demonstrates the success of our design in preventing unintentional commands with sporadic input. We identify design challenges of writing Chinese characters via 3-dimensional gestures without haptic feedback and discuss potential solutions.", "total_citations": 4, "citation_graph": {"2015": 1, "2016": 2, "2017": 0, "2018": 1}}, {"title": "Toward Knowledge-Enriched Conversational Recommendation Systems", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:SIv7DqKytYAC", "authors": ["Tong Zhang", "Yong Liu", "Boyang Li", "Peixiang Zhong", "Chen Zhang", "Hao Wang", "Chunyan Miao"], "publication_date": "2022", "conference": "The 4th Workshop on NLP for Conversational AI", "description": "Conversational Recommendation Systems recommend items through language based interactions with users. In order to generate naturalistic conversations and effectively utilize knowledge graphs (KGs) containing background information, we propose a novel Bag-of-Entities loss, which encourages the generated utterances to mention concepts related to the item being recommended, such as the genre or director of a movie. We also propose an alignment loss to further integrate KG entities into the response generation network. Experiments on the large-scale REDIAL dataset demonstrate that the proposed system consistently outperforms state-of-the-art baselines.", "total_citations": 3, "citation_graph": {"2023": 3}}, {"title": "Searching for Stage-wise Neural Graphs In the Limit", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:uVUOdF_882EC", "authors": ["Xin Zhou", "Dejing Dou", "Boyang Li"], "publication_date": "2019/12", "description": "Search space is a key consideration for neural architecture search. Recently, Xie et al. (2019) found that randomly generated networks from the same distribution perform similarly, which suggests we should search for random graph distributions instead of graphs. We propose graphon as a new search space. A graphon is the limit of Cauchy sequence of graphs and a scale-free probabilistic distribution, from which graphs of different number of nodes can be drawn. By utilizing properties of the graphon space and the associated cut-distance metric, we develop theoretically motivated techniques that search for and scale up small-capacity stage-wise graphs found on small datasets to large-capacity graphs that can handle ImageNet. The scaled stage-wise graphs outperform DenseNet and randomly wired Watts-Strogatz networks, indicating the benefits of graphon theory in NAS applications.", "total_citations": 3, "citation_graph": {"2021": 3}}, {"title": "Improving tail-class representation with centroid contrastive learning", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:UuEBAcK4md4C", "authors": ["Anthony Meng Huat Tiong", "Junnan Li", "Guosheng Lin", "Boyang Li", "Caiming Xiong", "Steven CH Hoi"], "publication_date": "2023/4/1", "journal": "Pattern Recognition Letters", "description": "In vision domain, large-scale natural datasets typically exhibit long-tailed distribution which has large class imbalance between head and tail classes. This distribution poses difficulty in learning good representations for tail classes. Recent developments have shown good long-tailed model can be learnt by decoupling the training into representation learning and classifier balancing. However, these works pay insufficient consideration on the long-tailed effect on representation learning. In this work, we propose interpolative centroid contrastive learning (ICCL) to improve long-tailed representation learning. ICCL interpolates two images from a class-agnostic sampler and a class-aware sampler, and trains the model such that the representation of the interpolative image can be used to retrieve the centroids for both source classes. We demonstrate the effectiveness of our approach on multiple long-tailed image \u2026", "total_citations": 2, "citation_graph": {"2023": 2}}, {"title": "Minimalist and High-performance Conversational Recommendation with Uncertainty Estimation for User Preference", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:8Xgff_V0N9gC", "authors": ["Yinan Zhang", "Boyang Li", "Yong Liu", "You Yuan", "Chunyan Miao"], "publication_date": "2022/6/29", "journal": "arXiv preprint arXiv:2206.14468", "description": "Conversational recommendation system (CRS) is emerging as a user-friendly way to capture users' dynamic preferences over candidate items and attributes. Multi-shot CRS is designed to make recommendations multiple times until the user either accepts the recommendation or leaves at the end of their patience. Existing works are trained with reinforcement learning (RL), which may suffer from unstable learning and prohibitively high demands for computing. In this work, we propose a simple and efficient CRS, MInimalist Non-reinforced Interactive COnversational Recommender Network (MINICORN). MINICORN models the epistemic uncertainty of the estimated user preference and queries the user for the attribute with the highest uncertainty. The system employs a simple network architecture and makes the query-vs-recommendation decision using a single rule. Somewhat surprisingly, this minimalist approach outperforms state-of-the-art RL methods on three real-world datasets by large margins. We hope that MINICORN will serve as a valuable baseline for future research.", "total_citations": 2, "citation_graph": {"2022": 1, "2023": 1}}, {"title": "Federated learning with diversified preference for humor recognition", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:WAzi4Gm8nLoC", "authors": ["Xu Guo", "Pengwei Xing", "Siwei Feng", "Boyang Li", "Chun-yan Miao"], "publication_date": "2020", "journal": "IJCAI Workshop", "description": "Understanding humor is critical to creative language modeling with many applications in human-AI interaction. However, the perception of humor is personal due to different humor preferences. Thus, a given passage can be regarded as funny to different degrees by different readers. This makes training humorous text recognition models that can adapt to diverse humor preferences highly challenging. In this paper, we propose the FedHumor approach to recognize humorous text contents in a personalized manner through federated learning (FL). It is a federated BERT model capable of jointly considering the overall distribution of humor scores with humor labels by individuals for given texts. Extensive experiments demonstrate significant advantages of FedHumor in recognizing humor contents accurately for people with diverse humor preferences compared to 9 state-of-the-art humor recognition approaches.", "total_citations": 2, "citation_graph": {"2022": 1, "2023": 1}}, {"title": "Mitigating and Evaluating Static Bias of Action Representations in the Background and the Foreground", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:T_ojBgVMvoEC", "authors": ["Haoxin Li", "Yuan Liu", "Hanwang Zhang", "Boyang Li"], "publication_date": "2023", "conference": "Proceedings of the IEEE/CVF International Conference on Computer Vision", "description": "In video action recognition, shortcut static features can interfere with the learning of motion features, resulting in poor out-of-distribution (OOD) generalization. The video background is clearly a source of static bias, but the video foreground, such as the clothing of the actor, can also provide static bias. In this paper, we empirically verify the existence of foreground static bias by creating test videos with conflicting signals from the static and moving portions of the video. To tackle this issue, we propose a simple yet effective technique, StillMix, to learn robust action representations. Specifically, StillMix identifies bias-inducing video frames using a 2D reference network and mixes them with videos for training, serving as effective bias suppression even when we cannot explicitly extract the source of bias within each video frame or enumerate types of bias. Finally, to precisely evaluate static bias, we synthesize two new benchmarks, SCUBA for static cues in the background, and SCUFO for static cues in the foreground. With extensive experiments, we demonstrate that StillMix mitigates both types of static bias and improves video representations for downstream applications.", "total_citations": 1, "citation_graph": {"2023": 1}}, {"title": "A Survey of Computer Vision Technologies In Urban and Controlled-environment Agriculture", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:zdjWy_NXXwUC", "authors": ["Jiayun Luo", "Boyang Li", "Cyril Leung"], "publication_date": "2023", "journal": "ACM Computing Surveys", "description": "In the evolution of agriculture to its next stage, Agriculture 5.0, artificial intelligence will play a central role. Controlled-environment agriculture, or CEA, is a special form of urban and suburban agricultural practice that offers numerous economic, environmental, and social benefits, including shorter transportation routes to population centers, reduced environmental impact, and increased productivity. Due to its ability to control environmental factors, CEA couples well with computer vision (CV) in the adoption of real-time monitoring of the plant conditions and autonomous cultivation and harvesting. The objective of this paper is to familiarize CV researchers with agricultural applications and agricultural practitioners with the solutions offered by CV. We identify five major CV applications in CEA, analyze their requirements and motivation, and survey the state of the art as reflected in 68 technical papers using deep learning methods. In addition, we discuss five key subareas of computer vision and how they related to these CEA problems, as well as nine vision-based CEA datasets. We hope the survey will help researchers quickly gain a bird-eye view of the striving research area and will spark inspiration for new research and development.", "total_citations": 1, "citation_graph": {"2023": 1}}, {"title": "From Data to Storytelling Agents", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:Ak0FvsSvgGUC", "authors": ["Boyang Li", "Mohini Thakkar", "Yijie Wang", "Mark O Riedl"], "publication_date": "2014", "conference": "Intelligent Virtual Agents: 14th International Conference, IVA 2014, Boston, MA, USA, August 27-29, 2014. Proceedings 14", "description": "The ability to craft, tell, and understand stories is important for virtual agents that wish to communicate with human users and simulate human capabilities. We provide an overview of an end-to-end storytelling system named Scheherazade, which learns domain-specific narrative knowledge from crowdsourced stories, generates stories and discourses, and presents stories in natural language with diverse personal styles and sentiments. Extending previous work, this paper addresses discourse planning and text generation. Discourse planning selectively omits events using typicality of events derived from graph structures. Text generation considers language features computed directly from large-scale data sets such as the Google N-Gram Corpus and Project Gutenberg books. Learning from these data sets instills virtual agents with linguistic and social behavioral knowledge.", "total_citations": 1, "citation_graph": {"2023": 1}}, {"title": "Narrative Intelligence Without (Domain) Boundaries", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:YOwf2qJgpHMC", "authors": ["Boyang Li"], "publication_date": "2012/7/10", "conference": "Doctoral Consortium, the Eighth Artificial Intelligence and Interactive Digital Entertainment Conference", "description": "Narrative Intelligence (NI) can help computational systems interact with users, such as through story generation, interactive narratives, and believable virtual characters. However, existing NI techniques generally require manually coded domain knowledge, restricting their scalability. An approach that intelligently, automatically and economically acquires script-like knowledge in any domain with strategic crowdsourcing will ease this bottleneck and broaden the application territory of narrative intelligence. This doctoral consortium paper defines the research problem, describes its significance, proposes a feasible research plan towards a Ph. D. dissertation, and reports on its current progress.", "total_citations": 1, "citation_graph": {"2015": 1}}, {"title": "Creating Customized Virtual Experiences by Leveraging Human Creative Effort: A Desideratum", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:UeHWp8X0CEIC", "authors": ["Mark O Riedl", "Boyang Li"], "publication_date": "2010", "journal": "Proceedings of the AAMAS 2010 Workshop on Collaborative Human/AI Control for Interactive Experiences", "description": "The task of entertaining people has, until very recently, been the exclusive domain of humans. However, recent work in the area of computational creativity, story generation, interactive storytelling, and autonomous believable agents suggests that AI may be used to create dynamic, interactive, and engaging real-time entertainment experiences. In this paper we consider the role of a novel technique called Experience Adaptation in the process of creating and delivering customized entertaining experiences. Experience Adaptation is an offline process that leverages human creative ability by taking human-authored storylines\u2013in this case specifications of desired future experiences\u2013and autonomously \u201cre-writing\u201d them based on unique requirements of individual users. With Experience Adaptation, we are working toward effectively scaling up entertainment computing.", "total_citations": 1, "citation_graph": {"2011": 1}}, {"title": "May I Ask a Follow-up Question? Understanding the Benefits of Conversations in Neural Network Explainability", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:z6xuaG2dYH0C", "authors": ["Tong Zhang", "X Jessie Yang", "Boyang Li"], "publication_date": "2023/9/25", "journal": "arXiv preprint arXiv:2309.13965", "description": "Research in explainable AI (XAI) aims to provide insights into the decision-making process of opaque AI models. To date, most XAI methods offer one-off and static explanations, which cannot cater to the diverse backgrounds and understanding levels of users. With this paper, we investigate if free-form conversations can enhance users' comprehension of static explanations, improve acceptance and trust in the explanation methods, and facilitate human-AI collaboration. Participants are presented with static explanations, followed by a conversation with a human expert regarding the explanations. We measure the effect of the conversation on participants' ability to choose, from three machine learning models, the most accurate one based on explanations and their self-reported comprehension, acceptance, and trust. Empirical results show that conversations significantly improve comprehension, acceptance, trust, and collaboration. Our findings highlight the importance of customized model explanations in the format of free-form conversations and provide insights for the future design of conversational explanations."}, {"title": "Training Multimedia Event Extraction With Generated Images and Captions", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:L_l9e5I586QC", "authors": ["Zilin Du", "Yunxin Li", "Xu Guo", "Yidan Sun", "Boyang Li"], "publication_date": "2023/6/15", "conference": "ACM Multimedia", "description": "Contemporary news reporting increasingly features multimedia content, motivating research on multimedia event extraction. However, the task lacks annotated multimodal training data and artificially generated training data suffer from the distribution shift from the real-world data. In this paper, we propose Cross-modality Augmented Multimedia Event Learning (CAMEL), which successfully utilizes artificially generated multimodal training data and achieves state-of-the-art performance. Conditioned on unimodal training data, we generate multimodal training data using off-the-shelf image generators like Stable Diffusion and image captioners like BLIP. In order to learn robust features that are effective across domains, we devise an iterative and gradual annealing training strategy. Substantial experiments show that CAMEL surpasses state-of-the-art (SOTA) baselines on the M2E2 benchmark. On multimedia events in particular, we outperform the prior SOTA by 4.2\\% F1 on event mention identification and by 9.8\\% F1 on argument identification, which demonstrates that CAMEL learns synergistic representations from the two modalities."}, {"title": "Movie Box Office Prediction With Self-Supervised and Visually Grounded Pretraining", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:C33y2ycGS3YC", "authors": ["Qin Chao", "Eunsoo Kim", "Boyang Li"], "publication_date": "2023/4/20", "conference": "ICME", "description": "Investments in movie production are associated with a high level of risk as movie revenues have long-tailed and bimodal distributions. Accurate prediction of box-office revenue may mitigate the uncertainty and encourage investment. However, learning effective representations for actors, directors, and user-generated content-related keywords remains a challenging open problem. In this work, we investigate the effects of self-supervised pretraining and propose visual grounding of content keywords in objects from movie posters as a pertaining objective. Experiments on a large dataset of 35,794 movies demonstrate significant benefits of self-supervised training and visual grounding. In particular, visual grounding pretraining substantially improves learning on movies with content keywords and achieves 14.5% relative performance gains compared to a finetuned BERT model with identical architecture."}, {"title": "Smart Decision-Support System for Pig Farming", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:eGYfIraVYiQC", "authors": ["Hao Wang", "Boyang Li", "Haoming Zhong", "Ahong Xu", "Yingjie Huang", "Jingfu Zou", "Yuanyuan Chen", "Pengcheng Wu", "Yiqiang Chen", "Cyril Leung", "Chunyan Miao"], "publication_date": "2022/11/30", "journal": "Drones", "description": "There are multiple participants, such as farmers, wholesalers, retailers, financial institutions, etc., involved in the modern food production process. All of these participants and stakeholders have a shared goal, which is to gather information on the food production process so that they can make appropriate decisions to increase productivity and reduce risks. However, real-time data collection and analysis continue to be difficult tasks, particularly in developing nations, where agriculture is the primary source of income for the majority of the population. In this paper, we present a smart decision-support system for pig farming. Specifically, we first adopt rail-based unmanned vehicles to capture pigsty images. We then conduct image stitching to avoid double-counting pigs so that we can use image segmentation method to give precise masks for each pig. Based on the segmentation masks, the pig weights can be estimated, and data can be integrated in our developed mobile app. The proposed system enables the above participants and stakeholders to have real-time data and intelligent analysis reports to help their decision-making."}, {"title": "Noise-Resistant Deep Metric Learning with Probabilistic Instance Filtering", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:jSAVyFp_754C", "authors": ["Chang Liu", "Han Yu", "Boyang Li", "Zhiqi Shen", "Zhanning Gao", "Peiran Ren", "Xuansong Xie", "Lizhen Cui", "Chunyan Miao"], "publication_date": "2021/8/3", "journal": "arXiv preprint arXiv:2108.01431", "description": "Noisy labels are commonly found in real-world data, which cause performance degradation of deep neural networks. Cleaning data manually is labour-intensive and time-consuming. Previous research mostly focuses on enhancing classification models against noisy labels, while the robustness of deep metric learning (DML) against noisy labels remains less well-explored. In this paper, we bridge this important gap by proposing Probabilistic Ranking-based Instance Selection with Memory (PRISM) approach for DML. PRISM calculates the probability of a label being clean, and filters out potentially noisy samples. Specifically, we propose a novel method, namely the von Mises-Fisher Distribution Similarity (vMF-Sim), to calculate this probability by estimating a von Mises-Fisher (vMF) distribution for each data class. Compared with the existing average similarity method (AvgSim), vMF-Sim considers the variance of each class in addition to the average similarity. With such a design, the proposed approach can deal with challenging DML situations in which the majority of the samples are noisy. Extensive experiments on both synthetic and real-world noisy dataset show that the proposed approach achieves up to 8.37% higher Precision@1 compared with the best performing state-of-the-art baseline approaches, within reasonable training time."}, {"title": "Alignment of video and textual sequences for metadata analysis", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:-nhnvRiOwuoC", "publication_date": "2021/3/23", "description": "Systems, methods and computer program products related to aligning heterogeneous sequential data are disclosed. Video data in a media presentation and textual data corresponding to content of the media presentation are received. An action related to aligning the video data and the textual data is determined using an alignment neural network, such that the video data and the textual data are at least partially aligned following the action. The alignment neural network includes a first fully connected layer that receives as input the video data, the textual data, and data relating to a previously determined action by the alignment neural network related to aligning the video data and the textual data. The determined action related to aligning the video data and the textual data is performed."}, {"title": "Vision-CAIR/VisualGPT: Code Repository", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:0aBXIfxlw9sC", "authors": ["Jun Chen", "Han Guo", "Kai Yi", "Boyang Li", "Mohamed Elhoseiny"], "publication_date": "2021/2/15"}, {"title": "Supplemental Material for HYDRA: Hypergradient Data Relevance Analysis for Interpreting Deep Neural Networks", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:eO3_k5sD8BwC", "authors": ["Yuanyuan Chen", "Boyang Li", "Han Yu", "Pengcheng Wu", "Chunyan Miao"], "publication_date": "2021", "description": "MNIST and Fashion-MNIST are divided into training sets of 50,000, validation sets of 10,000, and test sets of 10,000 samples, respectively. CIFAR-10 is divided into a training set of 40,000, a validation set of 10,000, and a test set of 10,000 samples, respectively. LeNet-5 consists of 61,706 trainable parameters. We replace all of the tanh activation function with ReLU in our experiments. We also use the DenseNet-40 model with a growth rate of 12, which contains 176,122 trainable parameters."}, {"title": "Data-efficient Alignment of Multimodal Sequences by Aligning Gradient Updates and Internal Feature Distributions", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:raTqNPD5sRQC", "authors": ["Jianan Wang", "Boyang Li", "Xiangyu Fan", "Jing Lin", "Yanwei Fu"], "publication_date": "2021", "conference": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision", "description": "The task of video and text sequence alignment is a prerequisite step toward joint understanding of movie videos and screenplays. However, supervised methods face the obstacle of limited realistic training data. With this paper, we attempt to enhance data efficiency of the end-to-end alignment network NeuMATCH [15]. Recent research [56] suggests that network components dealing with different modalities may overfit and generalize at different speeds, creating difficulties for training. We propose to employ (1) layer-wise adaptive rate scaling (LARS) to align the magnitudes of gradient updates in different layers and balance the pace of learning and (2) sequence-wise batch normalization (SBN) to align the internal feature distributions from different modalities. Finally, we leverage random projection to reduce the dimensionality of input features. On the YouTube Movie Summary dataset, the combined use of these technique closes the performance gap when the pretraining on the LSMDC dataset is omitted and achieves the state-of-the-art result. Extensive empirical comparisons and analysis reveal that these techniques improve optimization and regularize the network more effectively than two different setups of layer normalization."}, {"title": "Joint understanding of actors, literary characters, and movies", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:w0F2JDEymm0C", "publication_date": "2020/2/13", "description": "Systems, methods, and articles of manufacture are disclosed for learning models of movies, keywords, actors, and roles, and querying the same. In one embodiment, a recommendation application optimizes a model based on training data by initializing the mean and co-variance matrices of Gaussian distributions representing movies, keywords, and actors to random values, and then performing an optimization to minimize a margin loss function using symmetrical or asymmetrical measures of similarity between entities. Such training produces an optimized model with the Gaussian distributions representing movies, keywords, and actors, as well as shift vectors that change the means of movie Gaussian distributions and model archetypical roles. Subsequent to training, the same similarity measures used to train the model are used to query the model and obtain rankings of entities based on similarity to terms in the \u2026"}, {"title": "Supplementary Material for A Neural Multi-sequence Alignment TeCHnique (NeuMATCH)", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:1DsIQWDZLl8C", "authors": ["Pelin Dogan", "Boyang Li", "Leonid Sigal", "Markus Gross"], "publication_date": "2018", "description": "In this supplementary material, we first give details on the segmentation of videos into clips. Next, we show more alignment results computed by our approach on the datasets HM-1, HM-2, and YMS that require one-to-many matching and contain clips that do not match any sentences (ie, null clips). For illustration purposes, each figure below represents only a small portion (6-12 consecutive clips) of the entire aligned sequence. Each frame represents a video clip. The aligned sentences are shown with wide brackets below or above the clips."}, {"title": "The AIIDE 2015 workshop program", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:7H_MAutzIkAC", "authors": ["Camille Barot", "Michael Buro", "Michael Cook", "Mirjam Palosaari Eladhari", "Antonios Liapis", "Magnus Johansson", "Josh McCoy", "Santiago Onta\u00f1\u00f3n", "Jonathan Rowe", "Emmett Tomai", "Harko Verhagen", "Alexander Zook"], "publication_date": "2016/7/4", "journal": "AI Magazine", "description": "The workshop program at the Eleventh Annual AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment was held November 14\u201315, 2015 at the University of California, Santa Cruz, USA. The program included 4 workshops (one of which was a joint workshop): Artificial Intelligence in Adversarial Real-Time Games, Experimental AI in Games, Intelligent Narrative Technologies and Social Believability in Games, and Player Modeling. This article contains the reports of three of the four workshops."}, {"title": "It's a Funny Story", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:WC23djZS0W4C", "authors": ["Peter J Ludovice", "Boyang Albert Li", "Ed Greco", "Jon Gaul"], "publication_date": "2015/7/1"}, {"title": "Supplementary Material: Mitigating and Evaluating Static Bias of Action Representations in the Background and the Foreground", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:-6RzNnnwWf8C", "authors": ["Haoxin Li", "Yuan Liu", "Hanwang Zhang", "Boyang Li"], "description": "No 25.25 72.83 44.65 81.94 36.58 85.03 Mixup 27.64 74.48 47.16 81.85 36.62 82.06 VideoMix 29.37 72.50 42.59 72.21 32.68 76.07 SDN 27.14 71.14 48.47 83.83 34.87 81.88 BE 26.67 72.99 46.62 81.73 35.99 85.30 ActorCutMix 29.02 74.02 56.88 79.60 36.97 81.07 FAME 29.50 73.83 28.21 71.70 39.61 81.56 StillMix 30.77 85.51 57.30 88.80 47.38 92.46 is difficult to separate the pixels of foreground motion and foreground static cues for debiasing foreground static bias. As a result, in this paper, we propose StillMix to debias without the need to explicitly extract foreground static cues within a frame. In addition, due to this difficulty, it is hard to create test videos by simply replacing the foreground static cues and preserving the foreground motion. Thus, we alternatively create videos with conflicting foreground cues (Figure 1 of the main paper) and SCUFO videos (Sec. 4 of the main paper) to evaluate foreground static bias."}, {"title": "VisualGPT Supplementary", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QwL4z2UAAAAJ&pagesize=100&citation_for_view=QwL4z2UAAAAJ:Xz60mAmATU4C", "authors": ["Jun Chen", "Han Guo", "Kai Yi", "Boyang Li", "Mohamed Elhoseiny"], "description": "Image and Word Features. Following [1], we use a Faster R-CNN networks [10] with ResNet-101 [5] as a backbone to train on Visual Genome dataset [8], and we extract a 2048-dimensional feature vector for each object. We use the Byte Pair Encoding (BPE)[12], which effectively incorporate sub-word information and is beneficial for dealing with out-of-vocabulary words. We employ learnable positional encoding and initialize token embedding from pretrained weights of GPT-2. Architecture and Hyperparameters. We have 3 layers in the encoder and 12 layers in the decoder with 12 heads in each layer. The hidden size D in each layer is 768. We load the GPT-2 (small) pretrained weights, which has 117M parameters into the decoder. We use the learning rate of 1e\u2212 4 under XE loss and 1e\u2212 5 during the reinforcement learning. We train the models with the AdamW optimizer [9] and a batch size 25. The beam size is equal to 5. The threshold \u03c4 is tuned on the validation set for different training data. Training Details. We train all the models in two steps. We first train the models with cross-entropy (XE) loss and then finetune them using reinforcement learning. The crossentropy loss LXE is the traditional autoregressive classification loss"}]}