{"goog_sch_url": "https://scholar.google.com/citations?hl=en&user=D_S41X4AAAAJ", "name": "Lin Weisi", "interests": ["Perception-inspired signal modeling", "perceptual multimedia quality evaluation", "video compression", "image processing & analysis"], "co_authors_url": [], "citation_table": {"columns": ["All", "Since 2018"], "Citations": [27600, 17638], "h-index": [86, 68], "i10-index": [382, 302]}, "citation_graph": {"2006": 99, "2007": 125, "2008": 194, "2009": 227, "2010": 348, "2011": 455, "2012": 537, "2013": 821, "2014": 1070, "2015": 1472, "2016": 1927, "2017": 2333, "2018": 2869, "2019": 3054, "2020": 2785, "2021": 2852, "2022": 3342, "2023": 2715}, "articles": [{"title": "Perceptual visual quality metrics: A survey", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:cFHS6HbyZ2cC", "authors": ["Weisi Lin", "C-C Jay Kuo"], "publication_date": "2011/5/1", "journal": "Journal of visual communication and image representation", "description": "Visual quality evaluation has numerous uses in practice, and also plays a central role in shaping many visual processing algorithms and systems, as well as their implementation, optimization and testing. In this paper, we give a systematic, comprehensive and up-to-date review of perceptual visual quality metrics (PVQMs) to predict picture quality according to human perception. Several frequently used computational modules (building blocks of PVQMs) are discussed. These include signal decomposition, just-noticeable distortion, visual attention, and common feature and artifact detection. Afterwards, different types of existing PVQMs are presented, and further discussion is given toward feature pooling, viewing condition, computer-generated signal and visual attention. Six often-used image metrics(namely SSIM, VSNR, IFC, VIF, MSVD and PSNR) are also compared with seven public image databases (totally \u2026", "total_citations": 1045, "citation_graph": {"2011": 11, "2012": 58, "2013": 83, "2014": 103, "2015": 108, "2016": 142, "2017": 121, "2018": 106, "2019": 98, "2020": 66, "2021": 50, "2022": 57, "2023": 34}}, {"title": "Image quality assessment based on gradient similarity", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:uLbwQdceFCQC", "authors": ["Anmin Liu", "Weisi Lin", "Manish Narwaria"], "publication_date": "2011/11/15", "journal": "IEEE Transactions on Image Processing", "description": "In this paper, we propose a new image quality assessment (IQA) scheme, with emphasis on gradient similarity. Gradients convey important visual information and are crucial to scene understanding. Using such information, structural and contrast changes can be effectively captured. Therefore, we use the gradient similarity to measure the change in contrast and structure in images. Apart from the structural/contrast changes, image quality is also affected by luminance changes, which must be also accounted for complete and more robust IQA. Hence, the proposed scheme considers both luminance and contrast-structural changes to effectively assess image quality. Furthermore, the proposed scheme is designed to follow the masking effect and visibility threshold more closely, i.e., the case when both masked and masking signals are small is more effectively tackled by the proposed scheme. Finally, the effects of the \u2026", "total_citations": 809, "citation_graph": {"2012": 5, "2013": 22, "2014": 51, "2015": 93, "2016": 97, "2017": 66, "2018": 80, "2019": 104, "2020": 79, "2021": 74, "2022": 88, "2023": 42}}, {"title": "Learning a no-reference quality assessment model of enhanced images with big data", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:OLNndOjO69MC", "authors": ["Ke Gu", "Dacheng Tao", "Jun-Fei Qiao", "Weisi Lin"], "publication_date": "2017/3/6", "journal": "IEEE transactions on neural networks and learning systems", "description": "In this paper, we investigate into the problem of image quality assessment (IQA) and enhancement via machine learning. This issue has long attracted a wide range of attention in computational intelligence and image processing communities, since, for many practical applications, e.g., object detection and recognition, raw images are usually needed to be appropriately enhanced to raise the visual quality (e.g., visibility and contrast). In fact, proper enhancement can noticeably improve the quality of input images, even better than originally captured images, which are generally thought to be of the best quality. In this paper, we present two most important contributions. The first contribution is to develop a new no-reference (NR) IQA model. Given an image, our quality measure first extracts 17 features through analysis of contrast, sharpness, brightness and more, and then yields a measure of visual quality using a \u2026", "total_citations": 381, "citation_graph": {"2017": 30, "2018": 44, "2019": 56, "2020": 59, "2021": 73, "2022": 69, "2023": 47}}, {"title": "Just noticeable distortion model and its applications in video coding", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:tYavs44e6CUC", "authors": ["XK Yang", "WS Ling", "ZK Lu", "EP Ong", "SS Yao"], "publication_date": "2005/8/31", "journal": "Signal processing: image communication", "description": "We explore a new perceptually-adaptive video coding (PVC) scheme for hybrid video compression, in order to achieve better perceptual coding quality and operational efficiency. A new just noticeable distortion (JND) estimator for color video is first devised in the image domain. How to efficiently integrate masking effects together is a key issue of JND modelling. We integrate spatial masking factors with the nonlinear additivity model for masking (NAMM). The JND estimator applies to all color components and accounts for the compound impact of luminance masking, texture masking and temporal masking. Extensive subjective viewing confirms that it is capable of determining a more accurate visibility threshold that is close to the actual JND bound in human eyes. Secondly, the image-domain JND profile is incorporated into hybrid video encoding via the JND-adaptive motion estimation and residue filtering process \u2026", "total_citations": 380, "citation_graph": {"2006": 2, "2007": 1, "2008": 9, "2009": 14, "2010": 20, "2011": 24, "2012": 12, "2013": 31, "2014": 29, "2015": 33, "2016": 29, "2017": 27, "2018": 28, "2019": 28, "2020": 33, "2021": 20, "2022": 18, "2023": 20}}, {"title": "The analysis of image contrast: From quality assessment to automatic enhancement", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:kiex5tMzGo8C", "authors": ["Ke Gu", "Guangtao Zhai", "Weisi Lin", "Min Liu"], "publication_date": "2015/3/9", "journal": "IEEE transactions on cybernetics", "description": "Proper contrast change can improve the perceptual quality of most images, but it has largely been overlooked in the current research of image quality assessment (IQA). To fill this void, we in this paper first report a new large dedicated contrast-changed image database (CCID2014), which includes 655 images and associated subjective ratings recorded from 22 inexperienced observers. We then present a novel reduced-reference image quality metric for contrast change (RIQMC) using phase congruency and statistics information of the image histogram. Validation of the proposed model is conducted on contrast related CCID2014, TID2008, CSIQ and TID2013 databases, and results justify the superiority and efficiency of RIQMC over a majority of classical and state-of-the-art IQA methods. Furthermore, we combine aforesaid subjective and objective assessments to derive the RIQMC based Optimal HIstogram \u2026", "total_citations": 370, "citation_graph": {"2015": 9, "2016": 33, "2017": 43, "2018": 59, "2019": 54, "2020": 47, "2021": 40, "2022": 50, "2023": 33}}, {"title": "Saliency detection in the compressed domain for adaptive image retargeting", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:3NQIlFlcGxIC", "authors": ["Yuming Fang", "Zhenzhong Chen", "Weisi Lin", "Chia-Wen Lin"], "publication_date": "2012/5/14", "journal": "IEEE Transactions on Image Processing", "description": "Saliency detection plays important roles in many image processing applications, such as regions of interest extraction and image resizing. Existing saliency detection models are built in the uncompressed domain. Since most images over Internet are typically stored in the compressed domain such as joint photographic experts group (JPEG), we propose a novel saliency detection model in the compressed domain in this paper. The intensity, color, and texture features of the image are extracted from discrete cosine transform (DCT) coefficients in the JPEG bit-stream. Saliency value of each DCT block is obtained based on the Hausdorff distance calculation and feature map fusion. Based on the proposed saliency detection model, we further design an adaptive image retargeting algorithm in the compressed domain. The proposed image retargeting algorithm utilizes multioperator operation comprised of the block \u2026", "total_citations": 348, "citation_graph": {"2013": 12, "2014": 30, "2015": 40, "2016": 40, "2017": 55, "2018": 48, "2019": 48, "2020": 26, "2021": 19, "2022": 13, "2023": 15}}, {"title": "No-reference quality assessment of contrast-distorted images based on natural scene statistics", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:3pYxbvHKFu8C", "authors": ["Yuming Fang", "Kede Ma", "Zhou Wang", "Weisi Lin", "Zhijun Fang", "Guangtao Zhai"], "publication_date": "2014/11/20", "journal": "IEEE Signal Processing Letters", "description": "Contrast distortion is often a determining factor in human perception of image quality, but little investigation has been dedicated to quality assessment of contrast-distorted images without assuming the availability of a perfect-quality reference image. In this letter, we propose a simple but effective method for no-reference quality assessment of contrast distorted images based on the principle of natural scene statistics (NSS). A large scale image database is employed to build NSS models based on moment and entropy features. The quality of a contrast-distorted image is then evaluated based on its unnaturalness characterized by the degree of deviation from the NSS models. Support vector regression (SVR) is employed to predict human mean opinion score (MOS) from multiple NSS features as the input. Experiments based on three publicly available databases demonstrate the promising performance of the \u2026", "total_citations": 341, "citation_graph": {"2015": 7, "2016": 28, "2017": 58, "2018": 48, "2019": 46, "2020": 44, "2021": 32, "2022": 42, "2023": 34}}, {"title": "Review of visual saliency detection with comprehensive information", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:7XCffrwrS2sC", "authors": ["Runmin Cong", "Jianjun Lei", "Huazhu Fu", "Ming-Ming Cheng", "Weisi Lin", "Qingming Huang"], "publication_date": "2018/9/16", "journal": "IEEE Transactions on circuits and Systems for Video Technology", "description": "The visual saliency detection model simulates the human visual system to perceive the scene and has been widely used in many vision tasks. With the development of acquisition technology, more comprehensive information, such as depth cue, inter-image correspondence, or temporal relationship, is available to extend image saliency detection to RGBD saliency detection, co-saliency detection, or video saliency detection. The RGBD saliency detection model focuses on extracting the salient regions from RGBD images by combining the depth information. The co-saliency detection model introduces the inter-image correspondence constraint to discover the common salient object in an image group. The goal of the video saliency detection model is to locate the motion-related salient object in video sequences, which considers the motion cue and spatiotemporal constraint jointly. In this paper, we review different \u2026", "total_citations": 340, "citation_graph": {"2018": 3, "2019": 28, "2020": 61, "2021": 89, "2022": 100, "2023": 59}}, {"title": "No-reference quality metric of contrast-distorted images based on information maximization", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:QC-2xSqExF4C", "authors": ["Ke Gu", "Weisi Lin", "Guangtao Zhai", "Xiaokang Yang", "Wenjun Zhang", "Chang Wen Chen"], "publication_date": "2016/6/15", "journal": "IEEE transactions on cybernetics", "description": "The general purpose of seeing a picture is to attain information as much as possible. With it, we in this paper devise a new no-reference/blind metric for image quality assessment (IQA) of contrast distortion. For local details, we lirst roughly remove predicted regions in an image since unpredicted remains are of much information. We then compute entropy of particular unpredicted areas of maximum information via visual saliency. From global perspective, we compare the image histogram with the uniformly distributed histogram of maximum information via the symmetric Kullback-Leibler divergence. The proposed blind IQA method generates an overall quality estimation of a contrast-distorted image by properly combining local and global considerations. Thorough experiments on live databases/subsets demonstrate the superiority of our training-free blind technique over state-of-the-art fulland no-reference IQA \u2026", "total_citations": 327, "citation_graph": {"2016": 5, "2017": 20, "2018": 47, "2019": 51, "2020": 46, "2021": 46, "2022": 64, "2023": 46}}, {"title": "No-reference image sharpness assessment in autoregressive parameter space", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:WTSGYGHz1bkC", "authors": ["Ke Gu", "Guangtao Zhai", "Weisi Lin", "Xiaokang Yang", "Wenjun Zhang"], "publication_date": "2015/6/1", "journal": "IEEE Transactions on Image Processing", "description": "In this paper, we propose a new no-reference (NR)/ blind sharpness metric in the autoregressive (AR) parameter space. Our model is established via the analysis of AR model parameters, first calculating the energy- and contrast-differences in the locally estimated AR coefficients in a pointwise way, and then quantifying the image sharpness with percentile pooling to predict the overall score. In addition to the luminance domain, we further consider the inevitable effect of color information on visual perception to sharpness and thereby extend the above model to the widely used YIQ color space. Validation of our technique is conducted on the subsets with blurring artifacts from four large-scale image databases (LIVE, TID2008, CSIQ, and TID2013). Experimental results confirm the superiority and efficiency of our method over existing NR algorithms, the state-of-the-art blind sharpness/blurriness estimators, and \u2026", "total_citations": 319, "citation_graph": {"2015": 2, "2016": 28, "2017": 38, "2018": 36, "2019": 46, "2020": 37, "2021": 41, "2022": 45, "2023": 44}}, {"title": "A patch-structure representation method for quality assessment of contrast changed images", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:oYVvnHz_XzQC", "authors": ["Shiqi Wang", "Kede Ma", "Hojatollah Yeganeh", "Zhou Wang", "Weisi Lin"], "publication_date": "2015/10/5", "journal": "IEEE Signal Processing Letters", "description": "Contrast is a fundamental attribute of images that plays an important role in human visual perception of image quality. With numerous approaches proposed to enhance image contrast, much less work has been dedicated to automatic quality assessment of contrast changed images. Existing approaches rely on global statistics to estimate contrast quality. Here we propose a novel local patch-based objective quality assessment method using an adaptive representation of local patch structure, which allows us to decompose any image patch into its mean intensity, signal strength and signal structure components and then evaluate their perceptual distortions in different ways. A unique feature that differentiates the proposed method from previous contrast quality models is the capability to produce a local contrast quality map, which predicts local quality variations over space and may be employed to guide contrast \u2026", "total_citations": 309, "citation_graph": {"2015": 1, "2016": 15, "2017": 22, "2018": 30, "2019": 22, "2020": 45, "2021": 37, "2022": 77, "2023": 57}}, {"title": "Perceptual quality metric with internal generative mechanism", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:HGTzPopzzJcC", "authors": ["Jinjian Wu", "Weisi Lin", "Guangming Shi", "Anmin Liu"], "publication_date": "2012/8/17", "journal": "IEEE Transactions on Image Processing", "description": "Objective image quality assessment (IQA) aims to evaluate image quality consistently with human perception. Most of the existing perceptual IQA metrics cannot accurately represent the degradations from different types of distortion, e.g., existing structural similarity metrics perform well on content-dependent distortions while not as well as peak signal-to-noise ratio (PSNR) on content-independent distortions. In this paper, we integrate the merits of the existing IQA metrics with the guide of the recently revealed internal generative mechanism (IGM). The IGM indicates that the human visual system actively predicts sensory information and tries to avoid residual uncertainty for image perception and understanding. Inspired by the IGM theory, we adopt an autoregressive prediction algorithm to decompose an input scene into two portions, the predicted portion with the predicted visual content and the disorderly portion \u2026", "total_citations": 278, "citation_graph": {"2013": 10, "2014": 18, "2015": 32, "2016": 40, "2017": 44, "2018": 34, "2019": 30, "2020": 19, "2021": 18, "2022": 18, "2023": 13}}, {"title": "A psychovisual quality metric in free-energy principle", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:Tiz5es2fbqcC", "authors": ["Guangtao Zhai", "Xiaolin Wu", "Xiaokang Yang", "Weisi Lin", "Wenjun Zhang"], "publication_date": "2011/6/30", "journal": "IEEE Transactions on Image Processing", "description": "In this paper, we propose a new psychovisual quality metric of images based on recent developments in brain theory and neuroscience, particularly the free-energy principle. The perception and understanding of an image is modeled as an active inference process, in which the brain tries to explain the scene using an internal generative model. The psychovisual quality is thus closely related to how accurately visual sensory data can be explained by the generative model, and the upper bound of the discrepancy between the image signal and its best internal description is given by the free energy of the cognition process. Therefore, the perceptual quality of an image can be quantified using the free energy. Constructively, we develop a reduced-reference free-energy-based distortion metric (FEDM) and a no-reference free-energy-based quality metric (NFEQM). The FEDM and the NFEQM are nearly invariant to many \u2026", "total_citations": 271, "citation_graph": {"2011": 3, "2012": 4, "2013": 10, "2014": 18, "2015": 15, "2016": 33, "2017": 33, "2018": 35, "2019": 34, "2020": 22, "2021": 21, "2022": 23, "2023": 18}}, {"title": "Saliency-guided quality assessment of screen content images", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:N5XbD978G_MC", "authors": ["Ke Gu", "Shiqi Wang", "Huan Yang", "Weisi Lin", "Guangtao Zhai", "Xiaokang Yang", "Wenjun Zhang"], "publication_date": "2016/3/30", "journal": "IEEE Transactions on Multimedia", "description": "With the widespread adoption of multidevice communication, such as telecommuting, screen content images (SCIs) have become more closely and frequently related to our daily lives. For SCIs, the tasks of accurate visual quality assessment, high-efficiency compression, and suitable contrast enhancement have thus currently attracted increased attention. In particular, the quality evaluation of SCIs is important due to its good ability for instruction and optimization in various processing systems. Hence, in this paper, we develop a new objective metric for research on perceptual quality assessment of distorted SCIs. Compared to the classical MSE, our method, which mainly relies on simple convolution operators, first highlights the degradations in structures caused by different types of distortions and then detects salient areas where the distortions usually attract more attention. A comparison of our algorithm with the \u2026", "total_citations": 268, "citation_graph": {"2016": 16, "2017": 29, "2018": 44, "2019": 49, "2020": 32, "2021": 37, "2022": 38, "2023": 23}}, {"title": "No-reference image blur assessment based on discrete orthogonal moments", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:h-xndbdg2koC", "authors": ["Leida Li", "Weisi Lin", "Xuesong Wang", "Gaobo Yang", "Khosro Bahrami", "Alex C Kot"], "publication_date": "2015/1/29", "journal": "IEEE transactions on cybernetics", "description": "Blur is a key determinant in the perception of image quality. Generally, blur causes spread of edges, which leads to shape changes in images. Discrete orthogonal moments have been widely studied as effective shape descriptors. Intuitively, blur can be represented using discrete moments since noticeable blur affects the magnitudes of moments of an image. With this consideration, this paper presents a blind image blur evaluation algorithm based on discrete Tchebichef moments. The gradient of a blurred image is first computed to account for the shape, which is more effective for blur representation. Then the gradient image is divided into equal-size blocks and the Tchebichef moments are calculated to characterize image shape. The energy of a block is computed as the sum of squared non-DC moment values. Finally, the proposed image blur score is defined as the variance-normalized moment energy, which is \u2026", "total_citations": 260, "citation_graph": {"2015": 3, "2016": 21, "2017": 40, "2018": 45, "2019": 37, "2020": 27, "2021": 25, "2022": 35, "2023": 25}}, {"title": "A saliency detection model using low-level features based on wavelet transform", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:DyXnQzXoVgIC", "authors": ["Nevrez Imamoglu", "Weisi Lin", "Yuming Fang"], "publication_date": "2012/10/16", "journal": "IEEE transactions on multimedia", "description": "Researchers have been taking advantage of visual attention in various image processing applications such as image retargeting, video coding, etc. Recently, many saliency detection algorithms have been proposed by extracting features in spatial or transform domains. In this paper, a novel saliency detection model is introduced by utilizing low-level features obtained from the wavelet transform domain. Firstly, wavelet transform is employed to create the multi-scale feature maps which can represent different features from edge to texture. Then, we propose a computational model for the saliency map from these features. The proposed model aims to modulate local contrast at a location with its global saliency computed based on the likelihood of the features, and the proposed model considers local center-surround differences and global contrast in the final saliency map. Experimental evaluation depicts the \u2026", "total_citations": 250, "citation_graph": {"2013": 2, "2014": 14, "2015": 22, "2016": 41, "2017": 42, "2018": 34, "2019": 35, "2020": 27, "2021": 8, "2022": 11, "2023": 14}}, {"title": "A video saliency detection model in compressed domain", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:1xBWf43XMUgC", "authors": ["Yuming Fang", "Weisi Lin", "Zhenzhong Chen", "Chia-Ming Tsai", "Chia-Wen Lin"], "publication_date": "2013/7/16", "journal": "IEEE transactions on circuits and systems for video technology", "description": "Saliency detection is widely used to extract regions of interest in images for various image processing applications. Recently, many saliency detection models have been proposed for video in uncompressed (pixel) domain. However, video over Internet is always stored in compressed domains, such as MPEG2, H.264, and MPEG4 Visual. In this paper, we propose a novel video saliency detection model based on feature contrast in compressed domain. Four types of features including luminance, color, texture, and motion are extracted from the discrete cosine transform coefficients and motion vectors in video bitstream. The static saliency map of unpredicted frames (I frames) is calculated on the basis of luminance, color, and texture features, while the motion saliency map of predicted frames (P and B frames) is computed by motion feature. A new fusion method is designed to combine the static saliency and motion \u2026", "total_citations": 245, "citation_graph": {"2014": 7, "2015": 22, "2016": 23, "2017": 40, "2018": 51, "2019": 34, "2020": 26, "2021": 12, "2022": 12, "2023": 17}}, {"title": "Just noticeable difference for images with decomposition model for separating edge and textured regions", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:CHSYGLWDkRkC", "authors": ["Anmin Liu", "Weisi Lin", "Manoranjan Paul", "Chenwei Deng", "Fan Zhang"], "publication_date": "2010/10/14", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "description": "In just noticeable difference (JND) models, evaluation of contrast masking (CM) is a crucial step. More specifically, CM due to edge masking (EM) and texture masking (TM) needs to be distinguished due to the entropy masking property of the human visual system. However, TM is not estimated accurately in the existing JND models since they fail to distinguish TM from EM. In this letter, we propose an enhanced pixel domain JND model with a new algorithm for CM estimation. In our model, total-variation based image decomposition is used to decompose an image into structural image (i.e., cartoon like, piecewise smooth regions with sharp edges) and textural image for estimation of EM and TM, respectively. Compared with the existing models, the proposed one shows its advantages brought by the better EM and TM estimation. It has been also applied to noise shaping and visual distortion gauge, and favorable \u2026", "total_citations": 245, "citation_graph": {"2011": 9, "2012": 9, "2013": 12, "2014": 22, "2015": 22, "2016": 21, "2017": 19, "2018": 25, "2019": 35, "2020": 23, "2021": 19, "2022": 14, "2023": 14}}, {"title": "No-reference quality assessment of screen content pictures", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:XdYaqolBBq8C", "authors": ["Ke Gu", "Jun Zhou", "Jun-Fei Qiao", "Guangtao Zhai", "Weisi Lin", "Alan Conrad Bovik"], "publication_date": "2017/6/2", "journal": "IEEE Transactions on Image Processing", "description": "Recent years have witnessed a growing number of image and video centric applications on mobile, vehicular, and cloud platforms, involving a wide variety of digital screen content images. Unlike natural scene images captured with modern high fidelity cameras, screen content images are typically composed of fewer colors, simpler shapes, and a larger frequency of thin lines. In this paper, we develop a novel blind/no-reference (NR) model for accessing the perceptual quality of screen content pictures with big data learning. The new model extracts four types of features descriptive of the picture complexity, of screen content statistics, of global brightness quality, and of the sharpness of details. Comparative experiments verify the efficacy of the new model as compared with existing relevant blind picture quality assessment algorithms applied on screen content image databases. A regression module is trained on a \u2026", "total_citations": 243, "citation_graph": {"2017": 13, "2018": 39, "2019": 38, "2020": 41, "2021": 47, "2022": 42, "2023": 22}}, {"title": "Modeling visual attention's modulatory aftereffects on visual sensitivity and quality evaluation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:kNdYIx-mwKoC", "authors": ["Zhongkang Lu", "Weisi Lin", "Xiaokang Yang", "EePing Ong", "Susu Yao"], "publication_date": "2005/10/17", "journal": "IEEE transactions on Image Processing", "description": "With the fast development of visual noise-shaping related applications (visual compression, error resilience, watermarking, encryption, and display), there is an increasingly significant demand on incorporating perceptual characteristics into these applications for improved performance. In this paper, a very important mechanism of the human brain, visual attention, is introduced for visual sensitivity and visual quality evaluation. Based upon the analysis, a new numerical measure for visual attention's modulatory aftereffects, perceptual quality significance map (PQSM), is proposed. To a certain extent, the PQSM reflects the processing ability of the human brain on local visual contents statistically. The PQSM is generated with the integration of local perceptual stimuli from color contrast, texture contrast, motion, as well as cognitive features (skin color and face in this study). Experimental results with subjective viewing \u2026", "total_citations": 236, "citation_graph": {"2005": 2, "2006": 8, "2007": 12, "2008": 8, "2009": 20, "2010": 26, "2011": 25, "2012": 23, "2013": 13, "2014": 16, "2015": 18, "2016": 11, "2017": 17, "2018": 12, "2019": 3, "2020": 7, "2021": 7, "2022": 3, "2023": 3}}, {"title": "Motion-compensated residue preprocessing in video coding based on just-noticeable-distortion profile", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:Zph67rFs4hoC", "authors": ["Xiaokang Yang", "Weisi Lin", "Zhongkhang Lu", "EePing Ong", "Susu Yao"], "publication_date": "2005/5/31", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "description": "We present a motion-compensated residue signal preprocessing scheme in video coding scheme based on just-noticeable-distortion (JND) profile. Human eyes cannot sense any changes below the JND threshold around a pixel due to their underlying spatial/temporal masking properties. An appropriate (even imperfect) JND model can significantly help to improve the performance of video coding algorithms. From the viewpoint of signal compression, smaller variance of signal results in less objective distortion of the reconstructed signal for a given bit rate. In this paper, a new JND estimator for color video is devised in image-domain with the nonlinear additivity model for masking (NAMM) and is incorporated into a motion-compensated residue signal preprocessor for variance reduction toward coding quality enhancement. As the result, both perceptual quality and objective quality are enhanced in coded video at a \u2026", "total_citations": 224, "citation_graph": {"2005": 1, "2006": 6, "2007": 9, "2008": 6, "2009": 12, "2010": 7, "2011": 17, "2012": 12, "2013": 23, "2014": 17, "2015": 18, "2016": 17, "2017": 13, "2018": 16, "2019": 16, "2020": 8, "2021": 11, "2022": 9, "2023": 4}}, {"title": "Video saliency incorporating spatiotemporal cues and uncertainty weighting", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:vofGIMt6cyEC", "authors": ["Yuming Fang", "Zhou Wang", "Weisi Lin", "Zhijun Fang"], "publication_date": "2014/7/16", "journal": "IEEE transactions on image processing", "description": "We propose a novel algorithm to detect visual saliency from video signals by combining both spatial and temporal information and statistical uncertainty measures. The main novelty of the proposed method is twofold. First, separate spatial and temporal saliency maps are generated, where the computation of temporal saliency incorporates a recent psychological study of human visual speed perception. Second, the spatial and temporal saliency maps are merged into one using a spatiotemporally adaptive entropy-based uncertainty weighting approach. The spatial uncertainty weighing incorporates the characteristics of proximity and continuity of spatial saliency, while the temporal uncertainty weighting takes into account the variations of background motion and local contrast. Experimental results show that the proposed spatiotemporal uncertainty weighting algorithm significantly outperforms state-of-the-art video \u2026", "total_citations": 222, "citation_graph": {"2014": 1, "2015": 13, "2016": 17, "2017": 29, "2018": 45, "2019": 41, "2020": 27, "2021": 21, "2022": 12, "2023": 15}}, {"title": "A fast reliable image quality predictor by fusing micro-and macro-structures", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:eHo_KFcuhuIC", "authors": ["Ke Gu", "Leida Li", "Hong Lu", "Xiongkuo Min", "Weisi Lin"], "publication_date": "2017/1/16", "journal": "IEEE Transactions on Industrial Electronics", "description": "A fast reliable computational quality predictor is eagerly desired in practical image/video applications, such as serving for the quality monitoring of real-time coding and transcoding. In this paper, we propose a new perceptual image quality assessment (IQA) metric based on the human visual system (HVS). The proposed IQA model performs efficiently with convolution operations at multiscales, gradient magnitude, and color information similarity, and a perceptual-based pooling. Extensive experiments are conducted using four popular large-size image databases and two multiply distorted image databases, and results validate the superiority of our approach over modern IQA measures in efficiency and efficacy. Our metric is built on the theoretical support of the HVS with lately designed IQA methods as special cases.", "total_citations": 221, "citation_graph": {"2017": 18, "2018": 29, "2019": 48, "2020": 40, "2021": 40, "2022": 29, "2023": 16}}, {"title": "Perceptual quality assessment of screen content images", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:m44aUaJR3ikC", "authors": ["Huan Yang", "Yuming Fang", "Weisi Lin"], "publication_date": "2015/8/5", "journal": "IEEE Transactions on Image Processing", "description": "Research on screen content images (SCIs) becomes important as they are increasingly used in multi-device communication applications. In this paper, we present a study on perceptual quality assessment of distorted SCIs subjectively and objectively. We construct a large-scale screen image quality assessment database (SIQAD) consisting of 20 source and 980 distorted SCIs. In order to get the subjective quality scores and investigate, which part (text or picture) contributes more to the overall visual quality, the single stimulus methodology with 11 point numerical scale is employed to obtain three kinds of subjective scores corresponding to the entire, textual, and pictorial regions, respectively. According to the analysis of subjective data, we propose a weighting strategy to account for the correlation among these three kinds of subjective scores. Furthermore, we design an objective metric to measure the visual \u2026", "total_citations": 219, "citation_graph": {"2016": 14, "2017": 27, "2018": 31, "2019": 37, "2020": 20, "2021": 35, "2022": 29, "2023": 23}}, {"title": "Improved estimation for just-noticeable visual distortion", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:gsN89kCJA0AC", "authors": ["XH Zhang", "WS Lin", "P Xue"], "publication_date": "2005/4/1", "journal": "Signal Processing", "description": "Perceptual visibility threshold estimation, based upon characteristics of the human visual system (HVS), has wide applications in digital image/video processing. An improved scheme for estimating just-noticeable distortion (JND) is proposed in this paper. It is proved to outperform the DCTune model, with the major contributions of a new formula for luminance adaptation adjustment and the incorporation of block classification for contrast masking. The HVS visibility threshold for digital images exhibits an approximately parabolic curve versus gray levels and this has been formulated to yield a more accurate base threshold. Moreover, edge regions have been differentiated via block classification to effectively avoid over-estimation of JND in the said regions. Experiments with different images and the associated subjective tests show improved performance of the proposed scheme over the DCTune model for luminance \u2026", "total_citations": 219, "citation_graph": {"2005": 3, "2006": 6, "2007": 3, "2008": 6, "2009": 9, "2010": 17, "2011": 22, "2012": 8, "2013": 20, "2014": 14, "2015": 17, "2016": 18, "2017": 13, "2018": 17, "2019": 7, "2020": 13, "2021": 6, "2022": 11, "2023": 8}}, {"title": "Perceptual full-reference quality assessment of stereoscopic images by considering binocular visual characteristics", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:aDl3D7KC1E4C", "authors": ["Feng Shao", "Weisi Lin", "Shanbo Gu", "Gangyi Jiang", "Thambipillai Srikanthan"], "publication_date": "2013/1/14", "journal": "IEEE transactions on image processing", "description": "Perceptual quality assessment is a challenging issue in 3D signal processing research. It is important to study 3D signal directly instead of studying simple extension of the 2D metrics directly to the 3D case as in some previous studies. In this paper, we propose a new perceptual full-reference quality assessment metric of stereoscopic images by considering the binocular visual characteristics. The major technical contribution of this paper is that the binocular perception and combination properties are considered in quality assessment. To be more specific, we first perform left-right consistency checks and compare matching error between the corresponding pixels in binocular disparity calculation, and classify the stereoscopic images into non-corresponding, binocular fusion, and binocular suppression regions. Also, local phase and local amplitude maps are extracted from the original and distorted stereoscopic \u2026", "total_citations": 215, "citation_graph": {"2013": 3, "2014": 21, "2015": 42, "2016": 31, "2017": 29, "2018": 33, "2019": 19, "2020": 9, "2021": 14, "2022": 5, "2023": 4}}, {"title": "No-reference quality assessment for multiply-distorted images in gradient domain", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:QndmRo8phpgC", "authors": ["Qiaohong Li", "Weisi Lin", "Yuming Fang"], "publication_date": "2016/3/2", "journal": "IEEE Signal Processing Letters", "description": "In practice, images available to consumers usually undergo several stages of processing including acquisition, compression, transmission, and presentation, and each stage may introduce certain type of distortion. It is common that images are simultaneously distorted by multiple types of distortions. Most existing objective image quality assessment (IQA) methods have been designed to estimate perceived quality of images corrupted by a single image processing stage. In this letter, we propose a no-reference (NR) IQA method to predict the visual quality of multiply-distorted images based on structural degradation. In the proposed method, a novel structural feature is extracted as the gradient-weighted histogram of local binary pattern (LBP) calculated on the gradient map (GWH-GLBP), which is effective to describe the complex degradation pattern introduced by multiple distortions. Extensive experiments conducted \u2026", "total_citations": 213, "citation_graph": {"2016": 8, "2017": 16, "2018": 27, "2019": 32, "2020": 36, "2021": 30, "2022": 40, "2023": 23}}, {"title": "Blind quality assessment of tone-mapped images via analysis of information, naturalness, and structure", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:0vYOBEH00j0C", "authors": ["Ke Gu", "Shiqi Wang", "Guangtao Zhai", "Siwei Ma", "Xiaokang Yang", "Weisi Lin", "Wenjun Zhang", "Wen Gao"], "publication_date": "2016/1/18", "journal": "IEEE Transactions on Multimedia", "description": "High dynamic range (HDR) imaging techniques have been working constantly, actively, and validly in the fault detection and disease diagnosis in the astronomical and medical fields, and currently they have also gained much more attention from digital image processing and computer vision communities. While HDR imaging devices are starting to have friendly prices, HDR display devices are still out of reach of typical consumers. Due to the limited availability of HDR display devices, in most cases tone mapping operators (TMOs) are used to convert HDR images to standard low dynamic range (LDR) images for visualization. But existing TMOs cannot work effectively for all kinds of HDR images, with their performance largely depending on brightness, contrast, and structure properties of a scene. To accurately measure and compare the performance of distinct TMOs, in this paper develop an effective and efficient no \u2026", "total_citations": 206, "citation_graph": {"2016": 12, "2017": 17, "2018": 19, "2019": 17, "2020": 37, "2021": 34, "2022": 35, "2023": 32}}, {"title": "Image quality assessment using multi-method fusion", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:yTLRzDEmwhEC", "authors": ["Tsung-Jung Liu", "Weisi Lin", "C-C Jay Kuo"], "publication_date": "2012/12/24", "journal": "IEEE Transactions on image processing", "description": "A new methodology for objective image quality assessment (IQA) with multi-method fusion (MMF) is presented in this paper. The research is motivated by the observation that there is no single method that can give the best performance in all situations. To achieve MMF, we adopt a regression approach. The new MMF score is set to be the nonlinear combination of scores from multiple methods with suitable weights obtained by a training process. In order to improve the regression results further, we divide distorted images into three to five groups based on the distortion types and perform regression within each group, which is called \u201ccontext-dependent MMF\u201d (CD-MMF). One task in CD-MMF is to determine the context automatically, which is achieved by a machine learning approach. To further reduce the complexity of MMF, we perform algorithms to select a small subset from the candidate method set. The result is \u2026", "total_citations": 204, "citation_graph": {"2012": 1, "2013": 5, "2014": 20, "2015": 26, "2016": 18, "2017": 17, "2018": 16, "2019": 25, "2020": 24, "2021": 22, "2022": 16, "2023": 10}}, {"title": "Optimizing multistage discriminative dictionaries for blind image quality assessment", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:BxcezVm2apwC", "authors": ["Qiuping Jiang", "Feng Shao", "Weisi Lin", "Ke Gu", "Gangyi Jiang", "Huifang Sun"], "publication_date": "2017/10/16", "journal": "IEEE Transactions on Multimedia", "description": "State-of-the-art algorithms for blind image quality assessment (BIQA) typically have two categories. The first category approaches extract natural scene statistics (NSS) as features based on the statistical regularity of natural images. The second category approaches extract features by feature encoding with respect to a learned codebook. However, several problems need to be addressed in existing codebook-based BIQA methods. First, the high-dimensional codebook-based features are memory-consuming and have the risk of over-fitting. Second, there is a semantic gap between the constructed codebook by unsupervised learning and image quality. To address these problems, we propose a novel codebook-based BIQA method by optimizing multistage discriminative dictionaries (MSDDs). To be specific, MSDDs are learned by performing the label consistent K-SVD (LC-KSVD) algorithm in a stage-by-stage \u2026", "total_citations": 195, "citation_graph": {"2017": 1, "2018": 16, "2019": 14, "2020": 20, "2021": 89, "2022": 39, "2023": 15}}, {"title": "A no-reference quality metric for measuring image blur", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:YOwf2qJgpHMC", "authors": ["EePing Ong", "Weisi Lin", "Zhongkang Lu", "Xiaokang Yang", "Susu Yao", "Feng Pan", "Lijun Jiang", "Fulvio Moschetti"], "publication_date": "2003/7/4", "conference": "Seventh International Symposium on Signal Processing and Its Applications, 2003. Proceedings.", "description": "In this paper, a method for measuring the perceptual quality of blurred images has been proposed. Here, the amount of image blur is characterized by the average extent of edges in the image, or more specifically the average extent of the slope's spread of an edge in the opposing gradients' directions. The effectiveness of such method is validated using subjective tests on blurred images, including JPEG-2000 coded images, and the experimental results show that the proposed method can provide results that correlate relatively well with human subjective ratings.", "total_citations": 195, "citation_graph": {"2005": 4, "2006": 4, "2007": 6, "2008": 5, "2009": 6, "2010": 16, "2011": 15, "2012": 15, "2013": 17, "2014": 21, "2015": 21, "2016": 16, "2017": 10, "2018": 7, "2019": 8, "2020": 8, "2021": 2, "2022": 10, "2023": 3}}, {"title": "Objective image quality assessment based on support vector regression", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:j3f4tGmQtD8C", "authors": ["Manish Narwaria", "Weisi Lin"], "publication_date": "2010/1/22", "journal": "IEEE Transactions on Neural Networks", "description": "Objective image quality estimation is useful in many visual processing systems, and is difficult to perform in line with the human perception. The challenge lies in formulating effective features and fusing them into a single number to predict the quality score. In this brief, we propose a new approach to address the problem, with the use of singular vectors out of singular value decomposition (SVD) as features for quantifying major structural information in images and then support vector regression (SVR) for automatic prediction of image quality. The feature selection with singular vectors is novel and general for gauging structural changes in images as a good representative of visual quality variations. The use of SVR exploits the advantages of machine learning with the ability to learn complex data patterns for an effective and generalized mapping of features into a desired score, in contrast with the oft-utilized feature \u2026", "total_citations": 194, "citation_graph": {"2010": 2, "2011": 19, "2012": 20, "2013": 16, "2014": 20, "2015": 21, "2016": 19, "2017": 18, "2018": 15, "2019": 15, "2020": 7, "2021": 3, "2022": 9, "2023": 8}}, {"title": "Estimating just-noticeable distortion for video", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:ULOm3_A8WrAC", "authors": ["Yuting Jia", "Weisi Lin", "Ashraf A Kassim"], "publication_date": "2006/7/24", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "description": "Just-noticeable distortion (JND), which refers to the maximum distortion that the human visual system (HVS) cannot perceive, plays an important role in perceptual image and video processing. In comparison with JND estimation for images, estimation of the JND profile for video needs to take into account the temporal HVS properties in addition to the spatial properties. In this paper, we develop a spatio-temporal model estimating JND in the discrete cosine tranform domain. The proposed model incorporates the spatio-temporal contrast sensitivity function, the influence of eye movements, luminance adaptation, and contrast masking to be more consistent with human perception. It is capable of yielding JNDs for both still images and video with significant motion. The experiments conducted in this study have demonstrated that the JND values estimated for video sequences with moving objects by the model are in line \u2026", "total_citations": 194, "citation_graph": {"2007": 2, "2008": 8, "2009": 8, "2010": 15, "2011": 17, "2012": 9, "2013": 17, "2014": 17, "2015": 18, "2016": 20, "2017": 11, "2018": 13, "2019": 12, "2020": 9, "2021": 4, "2022": 9, "2023": 3}}, {"title": "Saliency detection for stereoscopic images", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:3x-KLxxGyuUC", "authors": ["Yuming Fang", "Junle Wang", "Manish Narwaria", "Patrick Le Callet", "Weisi Lin"], "publication_date": "2014/2/6", "journal": "IEEE Transactions on Image Processing", "description": "Many saliency detection models for 2D images have been proposed for various multimedia processing applications during the past decades. Currently, the emerging applications of stereoscopic display require new saliency detection models for salient region extraction. Different from saliency detection for 2D images, the depth feature has to be taken into account in saliency detection for stereoscopic images. In this paper, we propose a novel stereoscopic saliency detection framework based on the feature contrast of color, luminance, texture, and depth. Four types of features, namely color, luminance, texture, and depth, are extracted from discrete cosine transform coefficients for feature contrast calculation. A Gaussian model of the spatial distance between image patches is adopted for consideration of local and global contrast calculation. Then, a new fusion method is designed to combine the feature maps to \u2026", "total_citations": 193, "citation_graph": {"2014": 3, "2015": 15, "2016": 17, "2017": 29, "2018": 29, "2019": 22, "2020": 29, "2021": 31, "2022": 11, "2023": 5}}, {"title": "Mulsemedia: State of the art, perspectives, and challenges", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:WTQy_8Ay2UsC", "authors": ["Gheorghita Ghinea", "Christian Timmerer", "Weisi Lin", "Stephen R Gulliver"], "publication_date": "2014/10/1", "description": "Mulsemedia\u2014multiple sensorial media\u2014captures a wide variety of research efforts and applications. This article presents a historic perspective on mulsemedia work and reviews current developments in the area. These take place across the traditional multimedia spectrum\u2014from virtual reality applications to computer games\u2014as well as efforts in the arts, gastronomy, and therapy, to mention a few. We also describe standardization efforts, via the MPEG-V standard, and identify future developments and exciting challenges the community needs to overcome.", "total_citations": 192, "citation_graph": {"2015": 8, "2016": 15, "2017": 21, "2018": 30, "2019": 31, "2020": 23, "2021": 26, "2022": 21, "2023": 17}}, {"title": "Additive white Gaussian noise level estimation in SVD domain for images", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:kzcSZmkxUKAC", "authors": ["Wei Liu", "Weisi Lin"], "publication_date": "2012/9/18", "journal": "IEEE Transactions on Image processing", "description": "Accurate estimation of Gaussian noise level is of fundamental interest in a wide variety of vision and image processing applications as it is critical to the processing techniques that follow. In this paper, a new effective noise level estimation method is proposed on the basis of the study of singular values of noise-corrupted images. Two novel aspects of this paper address the major challenges in noise estimation: 1) the use of the tail of singular values for noise estimation to alleviate the influence of the signal on the data basis for the noise estimation process and 2) the addition of known noise to estimate the content-dependent parameter, so that the proposed scheme is adaptive to visual signals, thereby enabling a wider application scope of the proposed scheme. The analysis and experiment results demonstrate that the proposed algorithm can reliably infer noise levels and show robust behavior over a wide range of \u2026", "total_citations": 191, "citation_graph": {"2013": 3, "2014": 5, "2015": 17, "2016": 15, "2017": 19, "2018": 18, "2019": 27, "2020": 25, "2021": 24, "2022": 25, "2023": 12}}, {"title": "Saliency-based defect detection in industrial images by using phase spectrum", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:DwFgw5hZUzMC", "authors": ["Xiaolong Bai", "Yuming Fang", "Weisi Lin", "Lipo Wang", "Bing-Feng Ju"], "publication_date": "2014/9/22", "journal": "IEEE Transactions on Industrial Informatics", "description": "For computer vision-based inspection of electronic chips or dies in semiconductor production lines, we propose a new method to effectively and efficiently detect defects in images. Different from the traditional methods that compare the image of each test chip or die with the template image one by one, which are sensitive to misalignment between the test and template images, a collection of multiple test images are used as the input image for processing simultaneously in our method with two steps. The first step is to obtain salient regions of the whole collection of test images, and the second step is to evaluate local discrepancy between salient regions in test images and the corresponding regions in the defect-free template image. To be more specific, in the first step of our method, phase-only Fourier transform (POFT), which is computationally efficient for online applications in industry, is used for saliency detection \u2026", "total_citations": 189, "citation_graph": {"2015": 10, "2016": 13, "2017": 16, "2018": 21, "2019": 30, "2020": 21, "2021": 20, "2022": 27, "2023": 30}}, {"title": "Unified blind quality assessment of compressed natural, graphic, and screen content images", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:bczYY1dZPtQC", "authors": ["Xiongkuo Min", "Kede Ma", "Ke Gu", "Guangtao Zhai", "Zhou Wang", "Weisi Lin"], "publication_date": "2017/8/2", "journal": "IEEE Transactions on Image Processing", "description": "Digital images in the real world are created by a variety of means and have diverse properties. A photographical natural scene image (NSI) may exhibit substantially different characteristics from a computer graphic image (CGI) or a screen content image (SCI). This casts major challenges to objective image quality assessment, for which existing approaches lack effective mechanisms to capture such content type variations, and thus are difficult to generalize from one type to another. To tackle this problem, we first construct a cross-content-type (CCT) database, which contains 1,320 distorted NSIs, CGIs, and SCIs, compressed using the high efficiency video coding (HEVC) intra coding method and the screen content compression (SCC) extension of HEVC. We then carry out a subjective experiment on the database in a well-controlled laboratory environment. Moreover, we propose a unified content-type adaptive \u2026", "total_citations": 185, "citation_graph": {"2017": 3, "2018": 30, "2019": 32, "2020": 21, "2021": 32, "2022": 35, "2023": 32}}, {"title": "Bottom-up saliency detection model based on human visual sensitivity and amplitude spectrum", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:ufrVoPGSRksC", "authors": ["Yuming Fang", "Weisi Lin", "Bu-Sung Lee", "Chiew-Tong Lau", "Zhenzhong Chen", "Chia-Wen Lin"], "publication_date": "2011/9/26", "journal": "IEEE Transactions on Multimedia", "description": "With the wide applications of saliency information in visual signal processing, many saliency detection methods have been proposed. However, some key characteristics of the human visual system (HVS) are still neglected in building these saliency detection models. In this paper, we propose a new saliency detection model based on the human visual sensitivity and the amplitude spectrum of quaternion Fourier transform (QFT). We use the amplitude spectrum of QFT to represent the color, intensity, and orientation distributions for image patches. The saliency value for each image patch is calculated by not only the differences between the QFT amplitude spectrum of this patch and other patches in the whole image, but also the visual impacts for these differences determined by the human visual sensitivity. The experiment results show that the proposed saliency detection model outperforms the state-of-the-art \u2026", "total_citations": 180, "citation_graph": {"2010": 1, "2011": 0, "2012": 4, "2013": 16, "2014": 9, "2015": 23, "2016": 31, "2017": 23, "2018": 19, "2019": 18, "2020": 13, "2021": 11, "2022": 5, "2023": 6}}, {"title": "SVD-based quality metric for image and video using machine learning", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:roLk4NBRz8UC", "authors": ["Manish Narwaria", "Weisi Lin"], "publication_date": "2011/9/29", "journal": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)", "description": "We study the use of machine learning for visual quality evaluation with comprehensive singular value decomposition (SVD)-based visual features. In this paper, the two-stage process and the relevant work in the existing visual quality metrics are first introduced followed by an in-depth analysis of SVD for visual quality assessment. Singular values and vectors form the selected features for visual quality assessment. Machine learning is then used for the feature pooling process and demonstrated to be effective. This is to address the limitations of the existing pooling techniques, like simple summation, averaging, Minkowski summation, etc., which tend to be ad hoc. We advocate machine learning for feature pooling because it is more systematic and data driven. The experiments show that the proposed method outperforms the eight existing relevant schemes. Extensive analysis and cross validation are performed with \u2026", "total_citations": 172, "citation_graph": {"2012": 2, "2013": 16, "2014": 14, "2015": 28, "2016": 14, "2017": 23, "2018": 25, "2019": 9, "2020": 11, "2021": 10, "2022": 11, "2023": 5}}, {"title": "Reduced-reference image quality assessment with visual information fidelity", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:cNe27ouKFcQC", "authors": ["Jinjian Wu", "Weisi Lin", "Guangming Shi", "Anmin Liu"], "publication_date": "2013/6/4", "journal": "IEEE Transactions on Multimedia", "description": "Reduced-reference (RR) image quality assessment (IQA) aims to use less data about the reference image and achieve higher evaluation accuracy. Recent research on brain theory suggests that the human visual system (HVS) actively predicts the primary visual information and tries to avoid the residual uncertainty for image perception and understanding. Therefore, the perceptual quality relies to the information fidelities of the primary visual information and the residual uncertainty. In this paper, we propose a novel RR IQA index based on visual information fidelity. We advocate that distortions on the primary visual information mainly disturb image understanding, and distortions on the residual uncertainty mainly change the comfort of perception. We separately compute the quantities of the primary visual information and the residual uncertainty of an image. Then the fidelities of the two types of information are \u2026", "total_citations": 169, "citation_graph": {"2013": 2, "2014": 6, "2015": 21, "2016": 25, "2017": 21, "2018": 19, "2019": 27, "2020": 13, "2021": 6, "2022": 19, "2023": 8}}, {"title": "Hierarchical alternate interaction network for RGB-D salient object detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:bcT4vkklUMwC", "authors": ["Gongyang Li", "Zhi Liu", "Minyu Chen", "Zhen Bai", "Weisi Lin", "Haibin Ling"], "publication_date": "2021/3/5", "journal": "IEEE Transactions on Image Processing", "description": "Existing RGB-D Salient Object Detection (SOD) methods take advantage of depth cues to improve the detection accuracy, while pay insufficient attention to the quality of depth information. In practice, a depth map is often with uneven quality and sometimes suffers from distractors, due to various factors in the acquisition procedure. In this article, to mitigate distractors in depth maps and highlight salient objects in RGB images, we propose a Hierarchical Alternate Interactions Network (HAINet) for RGB-D SOD. Specifically, HAINet consists of three key stages: feature encoding, cross-modal alternate interaction, and saliency reasoning. The main innovation in HAINet is the Hierarchical Alternate Interaction Module (HAIM), which plays a key role in the second stage for cross-modal feature interaction. HAIM first uses RGB features to filter distractors in depth features, and then the purified depth features are exploited to \u2026", "total_citations": 168, "citation_graph": {"2021": 18, "2022": 67, "2023": 80}}, {"title": "Blind image quality assessment using statistical structural and luminance features", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:EO1llL0aI9sC", "authors": ["Qiaohong Li", "Weisi Lin", "Jingtao Xu", "Yuming Fang"], "publication_date": "2016/8/16", "journal": "IEEE Transactions on Multimedia", "description": "Blind image quality assessment (BIQA) aims to develop quantitative measures to automatically and accurately estimate perceptual image quality without any prior information about the reference image. In this paper, we introduce a novel BIQA metric by structural and luminance information, based on the characteristics of human visual perception for distorted image. We extract the perceptual structural features of distorted image by the local binary pattern distribution. Besides, the distribution of normalized luminance magnitudes is extracted to represent the luminance changes in distorted image. After extracting the features for structures and luminance, support vector regression is adopted to model the complex nonlinear relationship from feature space to quality measure. The proposed BIQA model is called no-reference quality assessment using statistical structural and luminance features (NRSL). Extensive \u2026", "total_citations": 168, "citation_graph": {"2016": 1, "2017": 12, "2018": 29, "2019": 31, "2020": 26, "2021": 17, "2022": 25, "2023": 27}}, {"title": "Adaptive downsampling to improve image compression at low bit rates", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:qxL8FJ1GzNcC", "authors": ["Weisi Lin", "Li Dong"], "publication_date": "2006/8/14", "journal": "IEEE Transactions on Image Processing", "description": "At low bit rates, better coding quality can be achieved by downsampling the image prior to compression and estimating the missing portion after decompression. This paper presents a new algorithm in such a paradigm, based on the adaptive decision of appropriate downsampling directions/ratios and quantization steps, in order to achieve higher coding quality with low bit rates with the consideration of local visual significance. The full-resolution image can be restored from the DCT coefficients of the downsampled pixels so that the spatial interpolation required otherwise is avoided. The proposed algorithm significantly raises the critical bit rate to approximately 1.2 bpp, from 0.15-0.41 bpp in the existing downsample-prior-to-JPEG schemes and, therefore, outperforms the standard JPEG method in a much wider bit-rate scope. The experiments have demonstrated better PSNR improvement over the existing \u2026", "total_citations": 160, "citation_graph": {"2006": 1, "2007": 3, "2008": 6, "2009": 9, "2010": 3, "2011": 10, "2012": 6, "2013": 11, "2014": 13, "2015": 9, "2016": 9, "2017": 9, "2018": 5, "2019": 13, "2020": 13, "2021": 13, "2022": 14, "2023": 12}}, {"title": "Deep dual-channel neural network for image-based smoke detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:G60ApcfeQaAC", "authors": ["Ke Gu", "Zhifang Xia", "Junfei Qiao", "Weisi Lin"], "publication_date": "2019/7/16", "journal": "IEEE Transactions on Multimedia", "description": "Smoke detection plays an important role in industrial safety warning systems and fire prevention. Due to the complicated changes in the shape, texture, and color of smoke, identifying the smoke from a given image still remains a substantial challenge, and this has accordingly aroused a considerable amount of research attention recently. To address the problem, we devise a new deep dual-channel neural network (DCNN) for smoke detection. In contrast to popular deep convolutional networks (e.g., Alex-Net, VGG-Net, Res-Net, and Dense-Net and the DNCNN that is specifically devoted to detecting smoke), our proposed end-to-end network is mainly composed of dual channels of deep subnetworks. In the first subnetwork, we sequentially connect multiple convolutional layers and max-pooling layers. Then, we selectively append the batch normalization layer to each convolutional layer for overfitting reduction and \u2026", "total_citations": 153, "citation_graph": {"2019": 3, "2020": 24, "2021": 26, "2022": 61, "2023": 38}}, {"title": "Cross-dimensional perceptual quality assessment for low bit-rate videos", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:QIV2ME_5wuYC", "authors": ["Guangtao Zhai", "Jianfei Cai", "Weisi Lin", "Xiaokang Yang", "Wenjun Zhang", "Minoru Etoh"], "publication_date": "2008/10/28", "journal": "IEEE Transactions on Multimedia", "description": "Most studies in the literature for video quality assessment have been focused on the evaluation of quantized video sequences at fixed and high spatial and temporal resolutions. Only limited work has been reported for assessing video quality under different spatial and temporal resolutions. In this paper, we consider a wider scope of video quality assessment in the sense of considering multiple dimensions. In particular, we address the problem of evaluating perceptual visual quality of low bit-rate videos under different settings and requirements. Extensive subjective view tests for assessing the perceptual quality of low bit-rate videos have been conducted, which cover 150 test scenarios and include five distinctive dimensions: encoder type, video content, bit rate, frame size, and frame rate. Based on the obtained subjective testing results, we perform thorough statistical analysis to study the influence of different \u2026", "total_citations": 152, "citation_graph": {"2007": 1, "2008": 0, "2009": 3, "2010": 12, "2011": 9, "2012": 18, "2013": 15, "2014": 16, "2015": 17, "2016": 18, "2017": 13, "2018": 6, "2019": 8, "2020": 8, "2021": 3, "2022": 3, "2023": 2}}, {"title": "Image retargeting quality assessment: A study of subjective scores and objective metrics", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:jFemdcug13IC", "authors": ["Lin Ma", "Weisi Lin", "Chenwei Deng", "King Ngi Ngan"], "publication_date": "2012/8/7", "journal": "IEEE Journal of Selected Topics in Signal Processing", "description": "This paper presents the result of a recent large-scale subjective study of image retargeting quality on a collection of images generated by several representative image retargeting methods. Owning to many approaches to image retargeting that have been developed, there is a need for a diverse independent public database of the retargeted images and the corresponding subjective scores to be freely available. We build an image retargeting quality database, in which 171 retargeted images (obtained from 57 natural source images of different contents) were created by several representative image retargeting methods. And the perceptual quality of each image is subjectively rated by at least 30 viewers, meanwhile the mean opinion scores (MOS) were obtained. It is revealed that the subject viewers have arrived at a reasonable agreement on the perceptual quality of the retargeted image. Therefore, the MOS \u2026", "total_citations": 149, "citation_graph": {"2013": 7, "2014": 11, "2015": 17, "2016": 14, "2017": 19, "2018": 17, "2019": 16, "2020": 11, "2021": 9, "2022": 16, "2023": 7}}, {"title": "Analysis of distortion distribution for pooling in image quality prediction", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:IF0FbId8XVwC", "authors": ["Ke Gu", "Shiqi Wang", "Guangtao Zhai", "Weisi Lin", "Xiaokang Yang", "Wenjun Zhang"], "publication_date": "2016/1/22", "journal": "IEEE Transactions on Broadcasting", "description": "Image quality assessment (IQA) has been an active research area during last decades. Many existing objective IQA models share a similar two-step structure with measuring local distortion before pooling. Compared with the rapid development for local distortion measurement, seldom effort has been made dedicated to effective pooling schemes. In this paper, we design a new pooling model via the analysis of distortion distribution affected by image content and distortion. That is, distributions of distortion position, distortion intensity, frequency changes, and histogram changes are comprehensively considered to infer an overall quality score. Experimental results conducted on four large-scale image quality databases (LIVE, TID2008, CSIQ, and CCID2014) concluded with three valuable findings. First, the proposed technique leads to consistent improvement in the IQA performance for studied local distortion \u2026", "total_citations": 147, "citation_graph": {"2016": 10, "2017": 20, "2018": 22, "2019": 37, "2020": 24, "2021": 18, "2022": 13, "2023": 3}}, {"title": "A locally adaptive algorithm for measuring blocking artifacts in images and videos", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:TIZ-Mc8IlK0C", "authors": ["F Pan", "X Lin", "S Rahardja", "W Lin", "E Ong", "S Yao", "Z Lu", "X Yang"], "publication_date": "2004/7/31", "journal": "Signal Processing: Image Communication", "description": "Block transform coding is the most popular approach for image and video compression. The objective measurement of blocking artifacts plays an important role in the design, optimization, and assessment of image and video coding systems. This paper presents a new algorithm for measuring blocking artifacts in images and videos. It exhibits unique and useful features: 1) it examines the blocks individually so that it can measure the severity of blocking artifacts locally; 2) it is a one-pass algorithm in the sense that the image needs to be accessed only once; 3) it takes into account the blocking artifacts for high bit rate images and the flatness for the very low bit rate images; 4) the blocking artifacts measure is well-defined in the range of 0-10. Experiments on various still images and videos show that this blockiness measure is very efficient in terms of computational complexity and memory usage, and can produce \u2026", "total_citations": 144, "citation_graph": {"2004": 3, "2005": 5, "2006": 6, "2007": 9, "2008": 14, "2009": 5, "2010": 8, "2011": 14, "2012": 8, "2013": 9, "2014": 12, "2015": 11, "2016": 7, "2017": 8, "2018": 5, "2019": 4, "2020": 4, "2021": 6, "2022": 3, "2023": 1}}, {"title": "Enhanced just noticeable difference model for images with pattern complexity", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:69viAa4lnfgC", "authors": ["Jinjian Wu", "Leida Li", "Weisheng Dong", "Guangming Shi", "Weisi Lin", "C-C Jay Kuo"], "publication_date": "2017/3/22", "journal": "IEEE Transactions on Image Processing", "description": "The just noticeable difference (JND) in an image, which reveals the visibility limitation of the human visual system (HVS), is widely used for visual redundancy estimation in signal processing. To determine the JND threshold with the current schemes, the spatial masking effect is estimated as the contrast masking, and this cannot accurately account for the complicated interaction among visual contents. Research on cognitive science indicates that the HVS is highly adapted to extract the repeated patterns for visual content representation. Inspired by this, we formulate the pattern complexity as another factor to determine the total masking effect: the interaction is relatively straightforward with a limited masking effect in a regular pattern, and is complicated with a strong masking effect in an irregular pattern. From the orientation selectivity mechanism in the primary visual cortex, the response of each local receptive field \u2026", "total_citations": 143, "citation_graph": {"2017": 5, "2018": 13, "2019": 27, "2020": 33, "2021": 21, "2022": 20, "2023": 23}}, {"title": "Just noticeable difference estimation for images with free-energy principle", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:SrKkpNFED5gC", "authors": ["Jinjian Wu", "Guangming Shi", "Weisi Lin", "Anmin Liu", "Fei Qi"], "publication_date": "2013/6/13", "journal": "IEEE Transactions on Multimedia", "description": "In this paper, we introduce a novel just noticeable difference (JND) estimation model based on the unified brain theory, namely the free-energy principle. The existing pixel-based JND models mainly consider the orderly factors and always underestimate the JND threshold of the disorderly region. Recent research indicates that the human visual system (HVS) actively predicts the orderly information and avoids the residual disorderly uncertainty for image perception and understanding. Thus, we suggest that there exists disorderly concealment effect which results in high JND threshold of the disorderly region. Beginning with the Bayesian inference, we deduce an autoregressive model to imitate the active prediction of the HVS. Then, we estimate the disorderly concealment effect for the novel JND model. Experimental results confirm that the proposed JND model outperforms the relevant existing ones. Furthermore \u2026", "total_citations": 138, "citation_graph": {"2014": 4, "2015": 10, "2016": 14, "2017": 8, "2018": 19, "2019": 25, "2020": 14, "2021": 13, "2022": 17, "2023": 14}}, {"title": "Model-based referenceless quality metric of 3D synthesized images using local image description", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:9AucIJLffosC", "authors": ["Ke Gu", "Vinit Jakhetiya", "Jun-Fei Qiao", "Xiaoli Li", "Weisi Lin", "Daniel Thalmann"], "publication_date": "2017/7/28", "journal": "IEEE Transactions on Image Processing", "description": "New challenges have been brought out along with the emerging of 3D-related technologies, such as virtual reality, augmented reality (AR), and mixed reality. Free viewpoint video (FVV), due to its applications in remote surveillance, remote education, and so on, based on the flexible selection of direction and viewpoint, has been perceived as the development direction of next-generation video technologies and has drawn a wide range of researchers' attention. Since FVV images are synthesized via a depth image-based rendering (DIBR) procedure in the \u201cblind\u201d environment (without reference images), a reliable real-time blind quality evaluation and monitoring system is urgently required. But existing assessment metrics do not render human judgments faithfully mainly because geometric distortions are generated by DIBR. To this end, this paper proposes a novel referenceless quality metric of DIBR-synthesized \u2026", "total_citations": 137, "citation_graph": {"2017": 4, "2018": 16, "2019": 32, "2020": 29, "2021": 19, "2022": 25, "2023": 10}}, {"title": "No-reference and robust image sharpness evaluation based on multiscale spatial and spectral features", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:r655XaDZu5IC", "authors": ["Leida Li", "Wenhan Xia", "Weisi Lin", "Yuming Fang", "Shiqi Wang"], "publication_date": "2016/12/15", "journal": "IEEE Transactions on Multimedia", "description": "The human visual system exhibits multiscale characteristic when perceiving visual scenes. The hierarchical structures of an image are contained in its scale space representation, in which the image can be portrayed by a series of increasingly smoothed images. Inspired by this, this paper presents a no-reference and robust image sharpness evaluation (RISE) method by learning multiscale features extracted in both the spatial and spectral domains. For an image, the scale space is first built. Then sharpness-aware features are extracted in gradient domain and singular value decomposition domain, respectively. In order to take into account the impact of viewing distance on image quality, the input image is also down-sampled by several times, and the DCT-domain entropies are calculated as quality features. Finally, all features are utilized to learn a support vector regression model for sharpness prediction \u2026", "total_citations": 137, "citation_graph": {"2017": 10, "2018": 23, "2019": 30, "2020": 22, "2021": 16, "2022": 18, "2023": 15}}, {"title": "Just-noticeable difference estimation with pixels in images", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:mVmsd5A6BfQC", "authors": ["Xiaohui Zhang", "Weisi Lin", "Ping Xue"], "publication_date": "2008/1/1", "journal": "Journal of Visual Communication and Image Representation", "description": "Perceptual visibility threshold estimation, based upon characteristics of the human visual system (HVS), is widely used in digital image and video processing. We propose in this paper a scheme for estimating JND (just-noticeable difference) with explicit formulation for image pixels, by summing the effects of the visual thresholds in sub-bands. The factors being considered include spatial contrast sensitivity function (CSF), luminance adaptation, and adaptive inter- and intra-band contrast masking. The proposed scheme demonstrates favorable results in noise shaping and perceptual visual distortion gauge for different images, in comparison with the relevant existing JND estimators.", "total_citations": 137, "citation_graph": {"2008": 1, "2009": 5, "2010": 11, "2011": 14, "2012": 6, "2013": 12, "2014": 6, "2015": 14, "2016": 10, "2017": 9, "2018": 12, "2019": 13, "2020": 9, "2021": 5, "2022": 6, "2023": 3}}, {"title": "Objective quality assessment for image retargeting based on structural similarity", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:6Zm5LS9gQ5UC", "authors": ["Yuming Fang", "Kai Zeng", "Zhou Wang", "Weisi Lin", "Zhijun Fang", "Chia-Wen Lin"], "publication_date": "2014/1/20", "journal": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems", "description": "We propose an objective quality assessment method for image retargeting. The key step in our approach is to generate a structural similarity (SSIM) map that indicates at each spatial location in the source image how the structural information is preserved in the retargeted image. A spatial pooling method employing both bottom-up and top-down visual saliency estimations is then applied to provide an overall evaluation of the retargeted image. To evaluate the performance of the proposed IR-SSIM algorithm, we created an image database that contains images produced by different retargeting algorithms and carried out subjective tests to assess the quality of the retargeted images. Our experimental results show that IR-SSIM is better correlated with subjective evaluations than existing methods in the literature. To further demonstrate the advantages and potential applications of IR-SSIM, we embed it into a multi \u2026", "total_citations": 128, "citation_graph": {"2014": 4, "2015": 12, "2016": 24, "2017": 22, "2018": 17, "2019": 11, "2020": 13, "2021": 11, "2022": 8, "2023": 4}}, {"title": "Efficient image deblocking based on postfiltering in shifted windows", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:_Qo2XoVZTnwC", "authors": ["Guangtao Zhai", "Wenjun Zhang", "Xiaokang Yang", "Weisi Lin", "Yi Xu"], "publication_date": "2008/1/28", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "description": "We propose a simple yet effective deblocking method for JPEG compressed image through postfiltering in shifted windows (PSW) of image blocks. The MSE is compared between the original image block and the image blocks in shifted windows, so as to decide whether these altered blocks are used in the smoothing procedure. Our research indicates that there exists strong correlation between the optimal mean squared error threshold and the image quality factor Q, which is selected in the encoding end and can be computed from the quantization table embedded in the JPEG file. Also we use the standard deviation of each original block to adjust the threshold locally so as to avoid the over-smoothing of image details. With various image and bit-rate conditions, the processed image exhibits both great visual effect improvement and significant peak signal-to-noise ratio gain with fairly low computational complexity \u2026", "total_citations": 128, "citation_graph": {"2007": 1, "2008": 1, "2009": 3, "2010": 5, "2011": 7, "2012": 18, "2013": 6, "2014": 10, "2015": 10, "2016": 17, "2017": 9, "2018": 11, "2019": 7, "2020": 12, "2021": 5, "2022": 3, "2023": 2}}, {"title": "End-to-end blind image quality prediction with cascaded deep neural network", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:45AZ0Vt6gvEC", "authors": ["Jinjian Wu", "Jupo Ma", "Fuhu Liang", "Weisheng Dong", "Guangming Shi", "Weisi Lin"], "publication_date": "2020/6/19", "journal": "IEEE Transactions on image processing", "description": "The deep convolutional neural network (CNN) has achieved great success in image recognition. Many image quality assessment (IQA) methods directly use recognition-oriented CNN for quality prediction. However, the properties of IQA task is different from image recognition task. Image recognition should be sensitive to visual content and robust to distortion, while IQA should be sensitive to both distortion and visual content. In this paper, an IQA-oriented CNN method is developed for blind IQA (BIQA), which can efficiently represent the quality degradation. CNN is large-data driven, while the sizes of existing IQA databases are too small for CNN optimization. Thus, a large IQA dataset is firstly established, which includes more than one million distorted images (each image is assigned with a quality score as its substitute of Mean Opinion Score (MOS), abbreviated as pseudo-MOS). Next, inspired by the hierarchical \u2026", "total_citations": 127, "citation_graph": {"2020": 3, "2021": 20, "2022": 67, "2023": 36}}, {"title": "An iterative co-saliency framework for RGBD images", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:I__7AI8a974C", "authors": ["Runmin Cong", "Jianjun Lei", "Huazhu Fu", "Weisi Lin", "Qingming Huang", "Xiaochun Cao", "Chunping Hou"], "publication_date": "2017/11/21", "journal": "IEEE transactions on cybernetics", "description": "As a newly emerging and significant topic in computer vision community, co-saliency detection aims at discovering the common salient objects in multiple related images. The existing methods often generate the co-saliency map through a direct forward pipeline which is based on the designed cues or initialization, but lack the refinement-cycle scheme. Moreover, they mainly focus on RGB image and ignore the depth information for RGBD images. In this paper, we propose an iterative RGBD co-saliency framework, which utilizes the existing single saliency maps as the initialization, and generates the final RGBD co-saliency map by using a refinement-cycle model. Three schemes are employed in the proposed RGBD co-saliency framework, which include the addition scheme, deletion scheme, and iteration scheme. The addition scheme is used to highlight the salient regions based on intra-image depth propagation \u2026", "total_citations": 126, "citation_graph": {"2017": 1, "2018": 7, "2019": 16, "2020": 31, "2021": 26, "2022": 32, "2023": 13}}, {"title": "Image sharpness assessment by sparse representation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:H0nRGT7Dr7IC", "authors": ["Leida Li", "Dong Wu", "Jinjian Wu", "Haoliang Li", "Weisi Lin", "Alex C Kot"], "publication_date": "2016/3/22", "journal": "IEEE Transactions on Multimedia", "description": "Recent advances in sparse representation show that overcomplete dictionaries learned from natural images can capture high-level features for image analysis. Since atoms in the dictionaries are typically edge patterns and image blur is characterized by the spread of edges, an overcomplete dictionary can be used to measure the extent of blur. Motivated by this, this paper presents a no-reference sparse representation-based image sharpness index. An overcomplete dictionary is first learned using natural images. The blurred image is then represented using the dictionary in a block manner, and block energy is computed using the sparse coefficients. The sharpness score is defined as the variance-normalized energy over a set of selected high-variance blocks, which is achieved by normalizing the total block energy using the sum of block variances. The proposed method is not sensitive to training images, so a \u2026", "total_citations": 126, "citation_graph": {"2016": 1, "2017": 17, "2018": 22, "2019": 21, "2020": 22, "2021": 16, "2022": 14, "2023": 12}}, {"title": "Visual distortion gauge based on discrimination of noticeable contrast changes", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:4TOpqqG69KYC", "authors": ["Weisi Lin", "Li Dong", "Ping Xue"], "publication_date": "2005/6/27", "journal": "IEEE transactions on circuits and systems for video technology", "description": "This paper presents a method to discriminate pixel differences according to their impact toward perceived visual quality. Noticeable local contrast changes are formulated firstly since contrast is the basic sensory feature in the human visual system (HVS) perception. The analysis aims at quantifying the actual impact of such changes (further divided into increases and decreases on edges) in different signal contexts. An associated full-reference distortion metric proposed next provides better match with the HVS viewing. Experiments have used two independent visual data sets and the related subjective viewing results, and demonstrated the performance improvement of the proposed metric over the relevant existing ones with various video/images and under diversified test conditions. The proposed metric is particularly effective to visual signal with blurring and luminance fluctuations as the major artifacts, and brings \u2026", "total_citations": 126, "citation_graph": {"2005": 1, "2006": 5, "2007": 2, "2008": 5, "2009": 6, "2010": 12, "2011": 16, "2012": 10, "2013": 9, "2014": 8, "2015": 14, "2016": 7, "2017": 5, "2018": 8, "2019": 2, "2020": 3, "2021": 8, "2022": 2, "2023": 2}}, {"title": "Recurrent air quality predictor based on meteorology-and pollution-related factors", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:mqcSLZKRP28C", "authors": ["Ke Gu", "Junfei Qiao", "Weisi Lin"], "publication_date": "2018/1/15", "journal": "IEEE Transactions on Industrial Informatics", "description": "Air quality is currently arousing drastically increasing attention from the governments and populace all over the world. In this paper, we propose a heuristic recurrent air quality predictor (RAQP) to infer air quality. The RAQP exploits some key meteorology- and pollution-related variables to infer air pollutant concentrations (APCs), e.g. the fine particulate matter (PM2.5). It is natural that the meteorological factors and APCs at the current time have strong influences on air quality the next adjacent moment, that is to say, there exist high correlations between them. With this consideration, applying simple machine learners to the current meteorology- and pollution-related factors can reliably predict the air quality indices at a time later. However, owing to the nonlinear and chaotic reasons, the above correlations decline with the time interval enlarged. In such cases, it fails to forecast the air quality after several hours by only \u2026", "total_citations": 122, "citation_graph": {"2018": 4, "2019": 14, "2020": 19, "2021": 28, "2022": 28, "2023": 29}}, {"title": "A dilated inception network for visual saliency prediction", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:uHQrz-U2knEC", "authors": ["Sheng Yang", "Guosheng Lin", "Qiuping Jiang", "Weisi Lin"], "publication_date": "2019/10/14", "journal": "IEEE Transactions on Multimedia", "description": "Recently, with the advent of deep convolutional neural networks (DCNN), the improvements in visual saliency prediction research are impressive. One possible direction to approach the next improvement is to fully characterize the multi-scale saliency-influential factors with a computationally-friendly module in DCNN architectures. In this work, we propose an end-to-end dilated inception network (DINet) for visual saliency prediction. It captures multi-scale contextual features effectively with very limited extra parameters. Instead of utilizing parallel standard convolutions with different kernel sizes as the existing inception module, our proposed dilated inception module (DIM) uses parallel dilated convolutions with different dilation rates which can significantly reduce the computation load while enriching the diversity of receptive fields in feature maps. Moreover, the performance of our saliency model is further improved \u2026", "total_citations": 119, "citation_graph": {"2019": 3, "2020": 12, "2021": 38, "2022": 26, "2023": 40}}, {"title": "Learning markov clustering networks for scene text detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:d4Uf0zfqV5IC", "authors": ["Zichuan Liu", "Guosheng Lin", "Sheng Yang", "Jiashi Feng", "Weisi Lin", "Wang Ling Goh"], "publication_date": "2018/5/22", "journal": "arXiv preprint arXiv:1805.08365", "description": "A novel framework named Markov Clustering Network (MCN) is proposed for fast and robust scene text detection. MCN predicts instance-level bounding boxes by firstly converting an image into a Stochastic Flow Graph (SFG) and then performing Markov Clustering on this graph. Our method can detect text objects with arbitrary size and orientation without prior knowledge of object size. The stochastic flow graph encode objects' local correlation and semantic information. An object is modeled as strongly connected nodes, which allows flexible bottom-up detection for scale-varying and rotated objects. MCN generates bounding boxes without using Non-Maximum Suppression, and it can be fully parallelized on GPUs. The evaluation on public benchmarks shows that our method outperforms the existing methods by a large margin in detecting multioriented text objects. MCN achieves new state-of-art performance on challenging MSRA-TD500 dataset with precision of 0.88, recall of 0.79 and F-score of 0.83. Also, MCN achieves realtime inference with frame rate of 34 FPS, which is speedup when compared with the fastest scene text detection algorithm.", "total_citations": 119, "citation_graph": {"2018": 4, "2019": 25, "2020": 24, "2021": 28, "2022": 22, "2023": 16}}, {"title": "Semisupervised biased maximum margin analysis for interactive image retrieval", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:zxqBrjVgvjwC", "authors": ["Lining Zhang", "Lipo Wang", "Weisi Lin"], "publication_date": "2011/12/2", "journal": "IEEE Transactions on Image Processing", "description": "With many potential practical applications, content-based image retrieval (CBIR) has attracted substantial attention during the past few years. A variety of relevance feedback (RF) schemes have been developed as a powerful tool to bridge the semantic gap between low-level visual features and high-level semantic concepts, and thus to improve the performance of CBIR systems. Among various RF approaches, support-vector-machine (SVM)-based RF is one of the most popular techniques in CBIR. Despite the success, directly using SVM as an RF scheme has two main drawbacks. First, it treats the positive and negative feedbacks equally, which is not appropriate since the two groups of training feedbacks have distinct properties. Second, most of the SVM-based RF techniques do not take into account the unlabeled samples, although they are very helpful in constructing a good classifier. To explore solutions to \u2026", "total_citations": 117, "citation_graph": {"2012": 2, "2013": 12, "2014": 11, "2015": 19, "2016": 11, "2017": 15, "2018": 20, "2019": 11, "2020": 9, "2021": 1, "2022": 3}}, {"title": "An engineered CRISPR-Cas12a variant and DNA-RNA hybrid guides enable robust and rapid COVID-19 testing", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:Uo5fLKClJkAC", "authors": ["Kean Hean Ooi", "Mengying Mandy Liu", "Jie Wen Douglas Tay", "Seok Yee Teo", "Pornchai Kaewsapsak", "Shengyang Jin", "Chun Kiat Lee", "Jingwen Hou", "Sebastian Maurer-Stroh", "Weisi Lin", "Benedict Yan", "Gabriel Yan", "Yong-Gui Gao", "Meng How Tan"], "publication_date": "2021/3/19", "journal": "Nature communications", "description": "Extensive testing is essential to break the transmission of SARS-CoV-2, which causes the ongoing COVID-19 pandemic. Here, we present a CRISPR-based diagnostic assay that is robust to viral genome mutations and temperature, produces results fast, can be applied directly on nasopharyngeal (NP) specimens without RNA purification, and incorporates a human internal control within the same reaction. Specifically, we show that the use of an engineered AsCas12a enzyme enables detection of wildtype and mutated SARS-CoV-2 and allows us to perform the detection step with loop-mediated isothermal amplification (LAMP) at 60-65\u2009\u00b0C. We also find that the use of hybrid DNA-RNA guides increases the rate of reaction, enabling our test to be completed within 30\u2009minutes. Utilizing clinical samples from 72 patients with COVID-19 infection and 57 healthy individuals, we demonstrate that our test exhibits a \u2026", "total_citations": 116, "citation_graph": {"2021": 20, "2022": 54, "2023": 39}}, {"title": "Full-reference quality assessment of stereoscopic images by learning binocular receptive field properties", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:f13iAvnbnnYC", "authors": ["Feng Shao", "Kemeng Li", "Weisi Lin", "Gangyi Jiang", "Mei Yu", "Qionghai Dai"], "publication_date": "2015/5/21", "journal": "IEEE Transactions on Image Processing", "description": "Quality assessment of 3D images encounters more challenges than its 2D counterparts. Directly applying 2D image quality metrics is not the solution. In this paper, we propose a new full-reference quality assessment for stereoscopic images by learning binocular receptive field properties to be more in line with human visual perception. To be more specific, in the training phase, we learn a multiscale dictionary from the training database, so that the latent structure of images can be represented as a set of basis vectors. In the quality estimation phase, we compute sparse feature similarity index based on the estimated sparse coefficient vectors by considering their phase difference and amplitude difference, and compute global luminance similarity index by considering luminance changes. The final quality score is obtained by incorporating binocular combination based on sparse energy and sparse complexity \u2026", "total_citations": 116, "citation_graph": {"2015": 4, "2016": 17, "2017": 20, "2018": 20, "2019": 21, "2020": 13, "2021": 12, "2022": 5, "2023": 1}}, {"title": "No reference quality assessment for screen content images with both local and global feature representation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:SEmTV4UfSqwC", "authors": ["Yuming Fang", "Jiebin Yan", "Leida Li", "Jinjian Wu", "Weisi Lin"], "publication_date": "2017/12/8", "journal": "IEEE Transactions on Image Processing", "description": "In this paper, we propose a novel no reference quality assessment method by incorporating statistical luminance and texture features (NRLT) for screen content images (SCIs) with both local and global feature representation. The proposed method is designed inspired by the perceptual property of the human visual system (HVS) that the HVS is sensitive to luminance change and texture information for image perception. In the proposed method, we first calculate the luminance map through the local normalization, which is further used to extract the statistical luminance features in global scope. Second, inspired by existing studies from neuroscience that high-order derivatives can capture image texture, we adopt four filters with different directions to compute gradient maps from the luminance map. These gradient maps are then used to extract the second-order derivatives by local binary pattern. We further extract the \u2026", "total_citations": 113, "citation_graph": {"2018": 10, "2019": 27, "2020": 19, "2021": 27, "2022": 20, "2023": 9}}, {"title": "Learning a blind quality evaluation engine of screen content images", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:0d9pApVQ-n0C", "authors": ["Ke Gu", "Guangtao Zhai", "Weisi Lin", "Xiaokang Yang", "Wenjun Zhang"], "publication_date": "2016/7/5", "journal": "Neurocomputing", "description": "We in this paper investigate how to blindly predict the visual quality of a screen content image (SCI). With the popularity of multi-client and remote-controlling systems, SCIs and the relevant applications have been a hot research topic. In general, SCIs contain texts or graphics in cartoons, ebooks or captures of computer screens. As for blind quality assessment (QA) of natural scene images (NSIs), it has been well established since NSIs possess certain statistical properties. SCIs however do not have reliable statistic models so far and thus the associated blind QA task is hard to be addressed. Aiming at solving this problem, we first extract 13 perceptual-inspired features with the free energy based brain theory and structural degradation model. In order to avoid the overfitting and guarantee the independence of training and testing samples, we then collect 100,000 images and use their objective quality scores \u2026", "total_citations": 112, "citation_graph": {"2016": 6, "2017": 20, "2018": 16, "2019": 19, "2020": 13, "2021": 13, "2022": 20, "2023": 5}}, {"title": "Fourier transform-based scalable image quality measure", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:MpfHP-DdYjUC", "authors": ["Manish Narwaria", "Weisi Lin", "Ian Vince McLoughlin", "Sabu Emmanuel", "Liang-Tien Chia"], "publication_date": "2012/5/1", "journal": "IEEE Transactions on Image Processing", "description": "We present a new image quality assessment algorithm based on the phase and magnitude of the 2-D discrete Fourier transform. The basic idea is to compare the phase and magnitude of the reference and distorted images to compute the quality score. However, it is well known that the human visual system's sensitivity to different frequency components is not the same. We accommodate this fact via a simple yet effective strategy of non-uniform binning of the frequency components. This process also leads to reduced space representation of the image thereby enabling the reduced-reference (RR) prospects of the proposed scheme. We employ linear regression to integrate the effects of the changes in phase and magnitude. In this way, the required weights are determined via proper training and hence more convincing and effective. Last, using the fact that phase usually conveys more information than magnitude \u2026", "total_citations": 107, "citation_graph": {"2012": 1, "2013": 7, "2014": 16, "2015": 8, "2016": 13, "2017": 11, "2018": 11, "2019": 16, "2020": 4, "2021": 6, "2022": 9, "2023": 4}}, {"title": "Reduced-reference quality assessment of screen content images", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:lDOOmgye57wC", "authors": ["Shiqi Wang", "Ke Gu", "Xinfeng Zhang", "Weisi Lin", "Siwei Ma", "Wen Gao"], "publication_date": "2016/8/25", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "description": "The screen content images (SCIs) quality influences the user experience and the interactive performance of remote computing systems. With numerous approaches proposed to evaluate the quality of natural images, much less work has been dedicated to reduced-reference image quality assessment (RR-IQA) of SCIs. Here, we propose an RR-IQA method from the perspective of SCI visual perception. In particular, the quality of the distorted SCI is evaluated by comparing a set of extracted statistical features that consider both primary visual information and unpredictable uncertainty. A unique property that differentiates the proposed method from previous RR-IQA methods for natural images is the consideration of behaviors when human subjects view the screen content, which motivates us to establish the perceptual model according to the distinct properties of SCIs. Validations based on the screen content IQA \u2026", "total_citations": 102, "citation_graph": {"2017": 8, "2018": 18, "2019": 21, "2020": 14, "2021": 14, "2022": 17, "2023": 10}}, {"title": "Culturing fibroblasts in 3D human hair keratin hydrogels", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:FkBsMxS_Bp0C", "authors": ["Shuai Wang", "Zhenxing Wang", "Selin Ee Min Foo", "Nguan Soon Tan", "Yuan Yuan", "Weisi Lin", "Zhiyong Zhang", "Kee Woei Ng"], "publication_date": "2015/3/11", "journal": "ACS applied materials & interfaces", "description": "Human hair keratins are readily available, easy to extract, and eco-friendly materials with natural bioactivities. Keratin-based materials have been studied for applications such as cell culture substrates, internal hemostats for liver injury, and conduits for peripheral nerve repair. However, there are limited reports of using keratin-based 3D scaffolds for cell culture in vitro. Here, we describe the development of a 3D hair keratin hydrogel, which allows for living cell encapsulation under near physiological conditions. The convenience of making the hydrogels from keratin solutions in a simple and controllable manner is demonstrated, giving rise to constructs with tunable physical properties. This keratin hydrogel is comparable to collagen hydrogels in supporting the viability and proliferation of L929 murine fibroblasts. Notably, the keratin hydrogels contract less significantly as compared to the collagen hydrogels, over a 16 \u2026", "total_citations": 101, "citation_graph": {"2015": 2, "2016": 6, "2017": 15, "2018": 12, "2019": 11, "2020": 16, "2021": 13, "2022": 16, "2023": 10}}, {"title": "SGDNet: An end-to-end saliency-guided deep neural network for no-reference image quality assessment", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:BjLbhSWBl98C", "authors": ["Sheng Yang", "Qiuping Jiang", "Weisi Lin", "Yongtao Wang"], "publication_date": "2019/10/15", "book": "Proceedings of the 27th ACM international conference on multimedia", "description": "We propose an end-to-end saliency-guided deep neural network (SGDNet) for no-reference image quality assessment (NR-IQA). Our SGDNet is built on an end-to-end multi-task learning framework in which two sub-tasks including visual saliency prediction and image quality prediction are jointly optimized with a shared feature extractor. The existing multi-task CNN-based NR-IQA methods which usually consider distortion identification as the auxiliary sub-task cannot accurately identify the complex mixtures of distortions exist in authentically distorted images. By contrast, our saliency prediction sub-task is more universal because visual attention always exists when viewing every image, regardless of its distortion type. More importantly, related works have reported that saliency information is highly correlated with image quality while this property is fully utilized in our proposed SGNet by training the model with more \u2026", "total_citations": 97, "citation_graph": {"2020": 3, "2021": 27, "2022": 41, "2023": 26}}, {"title": "Orientation selectivity based visual pattern for reduced-reference image quality assessment", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:irE4lMk4wWMC", "authors": ["Jinjian Wu", "Weisi Lin", "Guangming Shi", "Leida Li", "Yuming Fang"], "publication_date": "2016/7/10", "journal": "Information Sciences", "description": "Image quality assessment (IQA) is in great demand for high quality image selection in the big data era. The challenge of reduced-reference (RR) IQA is how to use limited data to effectively represent the visual content of an image in the context of IQA. Research on neuroscience indicates that the human visual system (HVS) exhibits obvious orientation selectivity (OS) mechanism for visual content extraction. Inspired by this, an OS based visual pattern (OSVP) is proposed to extract visual content for RR IQA in this paper. The OS arises from the arrangement of the excitatory and inhibitory interactions among connected cortical neurons in a local receptive field. According to the OS mechanism, the similarity of preferred orientations between two nearby pixels is first analyzed. Then, the orientation similarities of pixels in a local neighborhood are arranged, and the OSVP is built for visual information representation. With \u2026", "total_citations": 96, "citation_graph": {"2016": 2, "2017": 17, "2018": 18, "2019": 16, "2020": 15, "2021": 9, "2022": 9, "2023": 9}}, {"title": "Objective quality assessment for image retargeting based on perceptual geometric distortion and information loss", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:sYWwZaPVD1oC", "authors": ["Chih-Chung Hsu", "Chia-Wen Lin", "Yuming Fang", "Weisi Lin"], "publication_date": "2014/3/14", "journal": "IEEE Journal of Selected Topics in Signal Processing", "description": "Image retargeting techniques aim to obtain retargeted images with different sizes or aspect ratios for various display screens. Various content-aware image retargeting algorithms have been proposed recently. However, there is still no effective objective metric for visual quality assessment of retargeted images. In this paper, we propose a novel full-reference objective metric for assessing visual quality of a retargeted image based on perceptual geometric distortion and information loss. The proposed metric measures the geometric distortion of a retargeted image based on the local variance of SIFT flow vector fields of the image. Furthermore, a visual saliency map is derived to characterize human perception of the geometric distortion. Besides, the information loss in the retargeted image, which is estimated based on the saliency map, is also taken into account in the proposed metric. Subjective tests are conducted to \u2026", "total_citations": 96, "citation_graph": {"2015": 14, "2016": 11, "2017": 15, "2018": 13, "2019": 13, "2020": 9, "2021": 9, "2022": 6, "2023": 5}}, {"title": "Which has better visual quality: The clear blue sky or a blurry animal?", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:MDBbo4b0KHEC", "authors": ["Dingquan Li", "Tingting Jiang", "Weisi Lin", "Ming Jiang"], "publication_date": "2018/10/11", "journal": "IEEE Transactions on Multimedia", "description": "Image content variation is a typical and challenging problem in no-reference image-quality assessment (NR-IQA). This work pays special attention to the impact of image content variation on NR-IQA methods. To better analyze this impact, we focus on blur-dominated distortions to exclude the impacts of distortion-type variations. We empirically show that current NR-IQA methods are inconsistent with human visual perception when predicting the relative quality of image pairs with different image contents. In view of deep semantic features of pretrained image classification neural networks always containing discriminative image content information, we put forward a new NR-IQA method based on semantic feature aggregation (SFA) to alleviate the impact of image content variation. Specifically, instead of resizing the image, we first crop multiple overlapping patches over the entire distorted image to avoid introducing \u2026", "total_citations": 95, "citation_graph": {"2019": 3, "2020": 6, "2021": 16, "2022": 33, "2023": 36}}, {"title": "Evaluating quality of screen content images via structural variation analysis", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:A7-hzOuI2KQC", "authors": ["Ke Gu", "Junfei Qiao", "Xiongkuo Min", "Guanghui Yue", "Weisi Lin", "Daniel Thalmann"], "publication_date": "2017/11/9", "journal": "IEEE transactions on visualization and computer graphics", "description": "With the quick development and popularity of computers, computer-generated signals have drastically invaded into our daily lives. Screen content image is a typical example, since it also includes graphic and textual images as components as compared with natural scene images which have been deeply explored, and thus screen content image has posed novel challenges to current researches, such as compression, transmission, display, quality assessment, and more. In this paper, we focus our attention on evaluating the quality of screen content images based on the analysis of structural variation, which is caused by compression, transmission, and more. We classify structures into global and local structures, which correspond to basic and detailed perceptions of humans, respectively. The characteristics of graphic and textual images, e.g., limited color variations, and the human visual system are taken into \u2026", "total_citations": 93, "citation_graph": {"2017": 1, "2018": 10, "2019": 22, "2020": 19, "2021": 19, "2022": 12, "2023": 10}}, {"title": "Towards robust curve text detection with conditional spatial expansion", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:XUmZziu-z7kC", "authors": ["Zichuan Liu", "Guosheng Lin", "Sheng Yang", "Fayao Liu", "Weisi Lin", "Wang Ling Goh"], "publication_date": "2019", "conference": "proceedings of the IEEE/CVF conference on computer vision and pattern recognition", "description": "It is challenging to detect curve texts due to their irregular shapes and varying sizes. In this paper, we first investigate the deficiency of the existing curve detection methods and then propose a novel Conditional Spatial Expansion (CSE) mechanism to improve the performance of curve detection. Instead of regarding the curve text detection as a polygon regression or a segmentation problem, we formulate it as a sequence prediction on the spatial domain. CSE starts with a seed arbitrarily chosen within a text region and progressively merges neighborhood regions based on the extracted local features by a CNN and contextual information of merged regions. The CSE is highly parameterized and can be seamlessly integrated into existing object detection frameworks. Enhanced by the data-dependent CSE mechanism, our curve text detection system provides robust instance-level text region extraction with minimal post-processing. The analysis experiment shows that our CSE can handle texts with various shapes, sizes, and orientations, and can effectively suppress the false-positives coming from text-like textures or unexpected texts included in the same RoI. Compared with the existing curve text detection algorithms, our method is more robust and enjoys a simpler processing flow. It also creates a new state-of-art performance on curve text benchmarks with F-measurement of up to 78.4%.", "total_citations": 92, "citation_graph": {"2019": 4, "2020": 24, "2021": 24, "2022": 24, "2023": 16}}, {"title": "Generalized biased discriminant analysis for content-based image retrieval", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:d1gkVwhDpl0C", "authors": ["Lining Zhang", "Lipo Wang", "Weisi Lin"], "publication_date": "2011/10/3", "journal": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)", "description": "Biased discriminant analysis (BDA) is one of the most promising relevance feedback (RF) approaches to deal with the feedback sample imbalance problem for content-based image retrieval (CBIR). However, the singular problem of the positive within-class scatter and the Gaussian distribution assumption for positive samples are two main obstacles impeding the performance of BDA RF for CBIR. To avoid both of these intrinsic problems in BDA, in this paper, we propose a novel algorithm called generalized BDA (GBDA) for CBIR. The GBDA algorithm avoids the singular problem by adopting the differential scatter discriminant criterion (DSDC) and handles the Gaussian distribution assumption by redesigning the between-class scatter with a nearest neighbor approach. To alleviate the overfitting problem, GBDA integrates the locality preserving principle; therefore, a smooth and locally consistent transform can also \u2026", "total_citations": 92, "citation_graph": {"2012": 1, "2013": 16, "2014": 10, "2015": 15, "2016": 16, "2017": 11, "2018": 7, "2019": 2, "2020": 3, "2021": 4, "2022": 2, "2023": 2}}, {"title": "Perceptual visual signal compression and transmission", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:5rMqqAh47xYC", "authors": ["Hong Ren Wu", "Amy R Reibman", "Weisi Lin", "Fernando Pereira", "Sheila S Hemami"], "publication_date": "2013/8/6", "journal": "Proceedings of the IEEE", "description": "One- and two-way communication with digital compressed visual signals is now an integral part of the daily life of millions. Such commonplace use has been realized by decades of advances in visual signal compression. The design of effective, efficient compression and transmission strategies for visual signals may benefit from proper incorporation of human visual system (HVS) characteristics. This paper overviews psychophysics and engineering associated with the communication of visual signals. It presents a short history of advances in perceptual visual signal compression, and describes perceptual models and how they are embedded into systems for compression and transmission, both with and without current compression standards.", "total_citations": 91, "citation_graph": {"2014": 11, "2015": 9, "2016": 17, "2017": 10, "2018": 5, "2019": 15, "2020": 7, "2021": 4, "2022": 9, "2023": 3}}, {"title": "Quality assessment of DIBR-synthesized images by measuring local geometric distortions and global sharpness", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:Et1yZiPVzsMC", "authors": ["Leida Li", "Yu Zhou", "Ke Gu", "Weisi Lin", "Shiqi Wang"], "publication_date": "2017/10/6", "journal": "IEEE Transactions on Multimedia", "description": "Depth-image-based rendering (DIBR) is a fundamental technique in free viewpoint video, which is widely adopted to synthesize virtual viewpoints. The warping and rendering operations in DIBR generally introduce geometric distortions and sharpness change. The state-of-the-art quality indices are limited in dealing with such images since they are sensitive to geometric changes. In this paper, a new quality model for DIBR-synthesized view images is presented by measuring LOcal Geometric distortions in disoccluded regions and global Sharpness (LOGS). A disoccluded region detection method is first proposed using SIFT-flow-based warping. Then, the sizes and distortion strength of local disoccluded regions are combined to generate a score. Furthermore, a reblurring-based strategy is proposed to quantify the global sharpness. Finally, the overall quality score is calculated by pooling the scores of local \u2026", "total_citations": 90, "citation_graph": {"2018": 6, "2019": 20, "2020": 23, "2021": 13, "2022": 22, "2023": 6}}, {"title": "Salient object detection with spatiotemporal background priors for video", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:Xtec1x7NZGAC", "authors": ["Tao Xi", "Wei Zhao", "Han Wang", "Weisi Lin"], "publication_date": "2016/11/22", "journal": "IEEE Transactions on Image Processing", "description": "Saliency detection for images has been studied for many years, for which a lot of methods have been designed. In saliency detection, background priors, which are often regarded as pseudo-background, are effective clues to find salient objects in images. Although image boundary is commonly used as background priors, it does not work well for images of complex scenes and videos. In this paper, we explore how to identify the background priors for a video and propose a saliency-based method to detect the visual objects by using the background priors. For a video, we integrate multiple pairs of scale-invariant feature transform flows from long-range frames, and a bidirectional consistency propagation is conducted to obtain the accurate and sufficient temporal background priors, which are combined with spatial background priors to generate spatiotemporal background priors. Next, a novel dual-graph-based \u2026", "total_citations": 90, "citation_graph": {"2017": 2, "2018": 14, "2019": 18, "2020": 14, "2021": 17, "2022": 18, "2023": 7}}, {"title": "Rate control for videophone using local perceptual cues", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:_kc_bZDykSQC", "authors": ["Xiaokang Yang", "Weisi Lin", "Zhongkang Lu", "Xiao Lin", "Susanto Rahardja", "EePing Ong", "Susu Yao"], "publication_date": "2005/4/4", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "description": "We present a method for extracting local visual perceptual cues and its application for rate control of videophone, in order to ensure the scarce bits to be assigned for maximum perceptual coding quality. The optimum quantization step is determined with the rate-distortion model considering the local perceptual cues in the visual signal. For extraction of the perceptual cues, luminance adaptation and texture masking are used as the stimulus-driven factors, while skin color serves as the cognition-driven factor in the current implementation. Both objective and subjective quality evaluations are given by evaluating the proposed perceptual rate control (PRC) scheme in the H.263 platform, and the evaluations show that the proposed PRC scheme achieves significant quality improvement in block-based coding for bandwidth-hungry applications.", "total_citations": 90, "citation_graph": {"2005": 1, "2006": 5, "2007": 8, "2008": 8, "2009": 7, "2010": 8, "2011": 6, "2012": 5, "2013": 10, "2014": 8, "2015": 2, "2016": 6, "2017": 3, "2018": 4, "2019": 4, "2020": 2, "2021": 1, "2022": 0, "2023": 1}}, {"title": "Three dimensional scalable video adaptation via user-end perceptual quality assessment", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:hC7cP41nSMkC", "authors": ["Guangtao Zhai", "Jianfei Cai", "Weisi Lin", "Xiaokang Yang", "Wenjun Zhang"], "publication_date": "2008/8/22", "journal": "IEEE Transactions on Broadcasting", "description": "For wireless video streaming, the three dimensional scalabilities (spatial, temporal and SNR) provided by the advanced scalable video coding (SVC) technique can be directly utilized to adapt video streams to dynamic wireless network conditions and heterogeneous wireless devices. However, the question is how to optimally trade off among the three dimensional scalabilities so as to maximize the perceived video quality, given the available resource. In this paper, we propose a low-complexity algorithm that executes at resource-limited user end to quantitatively and perceptually assess video quality under different spatial, temporal and SNR combinations. Based on the video quality measures, we further propose an efficient adaptation algorithm, which dynamically adapts scalable video to a suitable three dimension combination. Experimental results demonstrate the effectiveness of our proposed perceptual video \u2026", "total_citations": 89, "citation_graph": {"2009": 2, "2010": 4, "2011": 13, "2012": 14, "2013": 10, "2014": 13, "2015": 6, "2016": 2, "2017": 8, "2018": 4, "2019": 6, "2020": 3, "2021": 2, "2022": 1}}, {"title": "No-reference quality assessment of deblocked images", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:hXZnTIgIr50C", "authors": ["Leida Li", "Yu Zhou", "Weisi Lin", "Jinjian Wu", "Xinfeng Zhang", "Beijing Chen"], "publication_date": "2016/2/12", "journal": "Neurocomputing", "description": "JPEG is the most commonly used image compression standard. In practice, JPEG images are easily subject to blocking artifacts at low bit rates. To reduce the blocking artifacts, many deblocking algorithms have been proposed. However, they also introduce certain degree of blur, so the deblocked images contain multiple distortions. Unfortunately, the current quality metrics are not designed for multiply distorted images, so they are limited in evaluating the quality of deblocked images. To solve the problem, this paper presents a no-reference (NR) quality metric for deblocked images. A DeBlocked Image Database (DBID) is first built with subjective Mean Opinion Score (MOS) as ground truth. Then a NR DeBlocked Image Quality (DBIQ) metric is proposed by simultaneously evaluating blocking artifacts in smooth regions and blur in textured regions. Experimental results conducted on the DBID database demonstrate \u2026", "total_citations": 88, "citation_graph": {"2016": 7, "2017": 21, "2018": 23, "2019": 18, "2020": 6, "2021": 4, "2022": 4, "2023": 4}}, {"title": "Visual quality assessment: recent developments, coding applications and future trends", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:ocbgtyEEUOwC", "authors": ["Tsung-Jung Liu", "Yu-Chieh Lin", "Weisi Lin", "C-C Jay Kuo"], "publication_date": "2013", "description": "Research on visual quality assessment has been active during the last decade. In this work, we provide an in-depth review of recent developments in the field. As compared with existing survey papers, our current work has several unique contributions. First, besides image quality databases and metrics, we put equal emphasis on video quality databases and metrics as this is a less investigated area. Second, we discuss the application of visual quality evaluation to perceptual coding as an example for applications. Third, we benchmark the performance of state-of-the-art visual quality metrics with experiments. Finally, future trends in visual quality assessment are discussed.", "total_citations": 88, "citation_graph": {"2013": 1, "2014": 5, "2015": 4, "2016": 8, "2017": 5, "2018": 9, "2019": 14, "2020": 10, "2021": 11, "2022": 10, "2023": 11}}, {"title": "Robust Image Coding Based upon Compressive Sensing", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:Ak0FvsSvgGUC", "authors": ["C Deng", "W Lin", "B Lee", "C Lau"], "journal": "Multimedia, IEEE Transactions on", "description": "Multiple description coding (MDC) is one of the widely used mechanisms to combat packet-loss in non-feedback systems. However, the number of descriptions in the existing MDC schemes is very small (typically 2). With the number of descriptions increasing, the coding complexity increases drastically and many decoders would be required. In this paper, the compressive sensing (CS) principles are studied and an alternative coding paradigm with a number of descriptions is proposed based upon CS for high packet loss transmission. Two-dimentional discrete wavelet transform (DWT) is applied for sparse representation. Unlike the typical wavelet coders (e.g., JPEG 2000), DWT coefficients here are not directly encoded, but re-sampled towards equal importance of information instead. At the decoder side, by fully exploiting the intra-scale and inter-scale correlation of multiscale DWT, two different CS recovery \u2026", "total_citations": 86, "citation_graph": {"2012": 2, "2013": 6, "2014": 14, "2015": 10, "2016": 18, "2017": 8, "2018": 8, "2019": 7, "2020": 4, "2021": 3, "2022": 4}}, {"title": "A universal framework for salient object detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:fXCg-C-QWH4C", "authors": ["Jianjun Lei", "Bingren Wang", "Yuming Fang", "Weisi Lin", "Patrick Le Callet", "Nam Ling", "Chunping Hou"], "publication_date": "2016/7/18", "journal": "IEEE Transactions on Multimedia", "description": "In this paper, we propose a novel universal framework for salient object detection, which aims to enhance the performance of any existing saliency detection method. First, rough salient regions are extracted from any existing saliency detection model with distance weighting, adaptive binarization, and morphological closing. With the superpixel segmentation, a Bayesian decision model is adopted to refine the rough saliency map to obtain a more accurate saliency map. An iterative optimization method is designed to obtain better saliency results by exploiting the characteristics of the output saliency map each time. Through the iterative optimization process, the rough saliency map is updated step by step with better and better performance until an optimal saliency map is obtained. Experimental results on the public salient object detection datasets with ground truth demonstrate the promising performance of the \u2026", "total_citations": 85, "citation_graph": {"2016": 2, "2017": 21, "2018": 19, "2019": 11, "2020": 11, "2021": 7, "2022": 7, "2023": 6}}, {"title": "NMF-based image quality assessment using extreme learning machine", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:XJogQLJr2CkC", "authors": ["Shuigen Wang", "Chenwei Deng", "Weisi Lin", "Guang-Bin Huang", "Baojun Zhao"], "publication_date": "2016/2/3", "journal": "IEEE transactions on cybernetics", "description": "Numerous state-of-the-art perceptual image quality assessment (IQA) algorithms share a common two-stage process: distortion description followed by distortion effects pooling. As for the first stage, the distortion descriptors or measurements are expected to be effective representatives of human visual variations, while the second stage should well express the relationship among quality descriptors and the perceptual visual quality. However, most of the existing quality descriptors (e.g., luminance, contrast, and gradient) do not seem to be consistent with human perception, and the effects pooling is often done in ad-hoc ways. In this paper, we propose a novel full-reference IQA metric. It applies non-negative matrix factorization (NMF) to measure image degradations by making use of the parts-based representation of NMF. On the other hand, a new machine learning technique [extreme learning machine (ELM)] is \u2026", "total_citations": 85, "citation_graph": {"2016": 1, "2017": 9, "2018": 22, "2019": 13, "2020": 18, "2021": 10, "2022": 11}}, {"title": "Personality-assisted multi-task learning for generic and personalized image aesthetics assessment", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:Z610oKUqOA4C", "authors": ["Leida Li", "Hancheng Zhu", "Sicheng Zhao", "Guiguang Ding", "Weisi Lin"], "publication_date": "2020/1/28", "journal": "IEEE Transactions on Image Processing", "description": "Traditional image aesthetics assessment (IAA) approaches mainly predict the average aesthetic score of an image. However, people tend to have different tastes on image aesthetics, which is mainly determined by their subjective preferences. As an important subjective trait, personality is believed to be a key factor in modeling individual's subjective preference. In this paper, we present a personality-assisted multi-task deep learning framework for both generic and personalized image aesthetics assessment. The proposed framework comprises two stages. In the first stage, a multi-task learning network with shared weights is proposed to predict the aesthetics distribution of an image and Big-Five (BF) personality traits of people who like the image. The generic aesthetics score of the image can be generated based on the predicted aesthetics distribution. In order to capture the common representation of generic \u2026", "total_citations": 83, "citation_graph": {"2020": 5, "2021": 19, "2022": 29, "2023": 30}}, {"title": "Low-complexity video quality assessment using temporal quality variations", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:cK4Rrx0J3m0C", "authors": ["Manish Narwaria", "Weisi Lin", "Anmin Liu"], "publication_date": "2012/3/12", "journal": "IEEE Transactions on Multimedia", "description": "Objective video quality assessment (VQA) is the use of computational models to evaluate the video quality in line with the perception of the human visual system (HVS). It is challenging due to the underlying complexity, and the relatively limited understanding of the HVS and its intricate mechanisms. There are three important issues that arise in objective VQA in comparison with image quality assessment: 1) the temporal factors apart from the spatial ones also need to be considered, 2) the contribution of each factor (spatial and temporal) and their interaction to the overall video quality need to be determined, and 3) the computational complexity of the resultant method. In this paper, we seek to tackle the first issue by utilizing the worst case pooling strategy and the variations of spatial quality along the temporal axis with proper analysis and justification. The second issue is addressed by the use of machine learning \u2026", "total_citations": 82, "citation_graph": {"2012": 2, "2013": 8, "2014": 10, "2015": 14, "2016": 9, "2017": 3, "2018": 12, "2019": 10, "2020": 2, "2021": 7, "2022": 2, "2023": 3}}, {"title": "Perceptual video coding: Challenges and approaches", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:lSLTfruPkqcC", "authors": ["Zhenzhong Chen", "Weisi Lin", "King Ngi Ngan"], "publication_date": "2010/7/19", "description": "Investigation on the human perception can play an important role in video signal processing. Recently, there has been great interest in incorporating the human perception in video coding systems to enhance the perceptual quality of the represented visual signal. However, the limited understanding of the human visual system and high complexity of computational models of human visual system make it a challenging task. Furthermore, the hybrid video coding structure brings difficulties to integrate computational models with coding components to fulfill the requirements. In this paper, we review the physiological characteristics of human perception and address the most relevant aspects to video coding applications. Moreover, we discuss the computational models and metrics which guide the design and implementation of the video coding system, as well as the recent advances in perceptual video coding. To \u2026", "total_citations": 82, "citation_graph": {"2011": 7, "2012": 3, "2013": 9, "2014": 6, "2015": 9, "2016": 14, "2017": 1, "2018": 8, "2019": 4, "2020": 6, "2021": 3, "2022": 8, "2023": 3}}, {"title": "Objective quality assessment and perceptual compression of screen content images", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:kVKjLOQJbwAC", "authors": ["Shiqi Wang", "Ke Gu", "Kai Zeng", "Zhou Wang", "Weisi Lin"], "publication_date": "2016/5/25", "journal": "IEEE computer graphics and applications", "description": "Screen content image (SCI) has recently emerged as an active topic due to the rapidly increasing demand in many graphically rich services such as wireless displays and virtual desktops. SCIs are often composed of pictorial regions and computer generated textual/graphical content, which exhibit different statistical properties that often lead to different viewer behaviors. Inspired by this, we propose an objective quality assessment approach for SCIs that incorporates both visual field adaptation and information content weighting into structural similarity based local quality assessment. Furthermore, we develop a perceptual screen content coding scheme based on the newly proposed quality assessment measure, targeting at further improving the SCI compression performance. Experimental results show that the proposed quality assessment method not only better predicts the perceptual quality of SCIs, but also \u2026", "total_citations": 80, "citation_graph": {"2016": 5, "2017": 11, "2018": 12, "2019": 16, "2020": 10, "2021": 14, "2022": 6, "2023": 5}}, {"title": "Visual saliency detection with free energy theory", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:TewouNez5YAC", "authors": ["Ke Gu", "Guangtao Zhai", "Weisi Lin", "Xiaokang Yang", "Wenjun Zhang"], "publication_date": "2015/3/18", "journal": "IEEE Signal Processing Letters", "description": "Visual saliency can be thought of as the product of human brain activity. Most existing models were built upon local features or global features or both. Lately, a so-called free energy principle unifies several brain theories within one framework, and tells where easily surprise human viewers in a visual stimulus through a psychological measure. We believe that this \u201csurprise\u201d should be highly related to visual saliency, and thereby introduce a novel computational Free Energy inspired Saliency detection technique (FES). Our method computes the local entropy of the gap between an input image signal and its predicted counterpart that is reconstructed from the input one with a semi-parametric model. Experimental results prove that our algorithm predicts human fixation points accurately and is superior to classical/state-of-the-art competitors.", "total_citations": 79, "citation_graph": {"2015": 4, "2016": 20, "2017": 11, "2018": 12, "2019": 10, "2020": 9, "2021": 6, "2022": 5, "2023": 2}}, {"title": "Skin heat transfer model of facial thermograms and its application in face recognition", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:RHpTSmoSYBkC", "authors": ["Shiqian Wu", "Weisi Lin", "Shoulie Xie"], "publication_date": "2008/8/1", "journal": "Pattern Recognition", "description": "It has been found that facial thermograms vary with ambient temperature, as well as other internal and external conditions, and result in severe decline in the facial recognition rate. To tackle this problem, a skin heat transfer (SHT) model based on thermal physiology is derived in this paper. The proposed model converts the facial thermograms into blood-perfusion data, which is revealed to reduce the within-class scatter of face images. The advantage of the derived blood-perfusion data over the raw thermograms for recognition is analyzed by the normalized reverse cumulative histogram. It is shown that blood-perfusion data are more consistent in representing facial features. The experiments conducted on both same-session and time-lapse data have further demonstrated that (1) the blood-perfusion data are less sensitive to ambient temperature, physiological and psychological conditions if the human bodies are \u2026", "total_citations": 77, "citation_graph": {"2009": 2, "2010": 7, "2011": 7, "2012": 9, "2013": 9, "2014": 11, "2015": 13, "2016": 4, "2017": 1, "2018": 5, "2019": 2, "2020": 4, "2021": 0, "2022": 1, "2023": 2}}, {"title": "A paraboost method to image quality assessment", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:dyiPZ63SVtYC", "authors": ["Tsung-Jung Liu", "Kuan-Hsien Liu", "Joe Yuchieh Lin", "Weisi Lin", "C-C Jay Kuo"], "publication_date": "2015/12/17", "journal": "IEEE transactions on neural networks and learning systems", "description": "An ensemble method for full-reference image quality assessment (IQA) based on the parallel boosting (ParaBoost) idea is proposed in this paper. We first extract features from existing image quality metrics and train them to form basic image quality scorers (BIQSs). Then, we select additional features to address specific distortion types and train them to construct auxiliary image quality scorers (AIQSs). Both BIQSs and AIQSs are trained on small image subsets of certain distortion types and, as a result, they are weak performers with respect to a wide variety of distortions. Finally, we adopt the ParaBoost framework, which is a statistical scorer selection scheme for support vector regression (SVR), to fuse the scores of BIQSs and AIQSs to evaluate the images containing a wide range of distortion types. This ParaBoost methodology can be easily extended to images of new distortion types. Extensive experiments are \u2026", "total_citations": 76, "citation_graph": {"2016": 3, "2017": 10, "2018": 11, "2019": 15, "2020": 8, "2021": 9, "2022": 14, "2023": 6}}, {"title": "Toward intelligent sensing: Intermediate deep feature compression", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:DxlTmyU89zoC", "authors": ["Zhuo Chen", "Kui Fan", "Shiqi Wang", "Lingyu Duan", "Weisi Lin", "Alex Chichung Kot"], "publication_date": "2019/9/25", "journal": "IEEE Transactions on Image Processing", "description": "The recent advances of hardware technology have made the intelligent analysis equipped at the front-end with deep learning more prevailing and practical. To better enable the intelligent sensing at the front-end, instead of compressing and transmitting visual signals or the ultimately utilized top-layer deep learning features, we propose to compactly represent and convey the intermediate-layer deep learning features with high generalization capability, to facilitate the collaborating approach between front and cloud ends. This strategy enables a good balance among the computational load, transmission load and the generalization ability for cloud servers when deploying the deep neural networks for large scale cloud based visual analysis. Moreover, the presented strategy also makes the standardization of deep feature coding more feasible and promising, as a series of tasks can simultaneously benefit from the \u2026", "total_citations": 75, "citation_graph": {"2020": 8, "2021": 25, "2022": 23, "2023": 19}}, {"title": "Just noticeable difference estimation for screen content images", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:DGpvO1n63MYC", "authors": ["Shiqi Wang", "Lin Ma", "Yuming Fang", "Weisi Lin", "Siwei Ma", "Wen Gao"], "publication_date": "2016/5/26", "journal": "IEEE Transactions on Image Processing", "description": "We propose a novel just noticeable difference (JND) model for a screen content image (SCI). The distinct properties of the SCI result in different behaviors of the human visual system when viewing the textual content, which motivate us to employ a local parametric edge model with an adaptive representation of the edge profile in JND modeling. In particular, we decompose each edge profile into its luminance, contrast, and structure, and then evaluate the visibility threshold in different ways. The edge luminance adaptation, contrast masking, and structural distortion sensitivity are studied in subjective experiments, and the final JND model is established based on the edge profile reconstruction with tolerable variations. Extensive experiments are conducted to verify the proposed JND model, which confirm that it is accurate in predicting the JND profile, and outperforms the state-of-the-art schemes in terms of the \u2026", "total_citations": 75, "citation_graph": {"2016": 1, "2017": 6, "2018": 10, "2019": 20, "2020": 9, "2021": 9, "2022": 13, "2023": 5}}, {"title": "Efficient deblocking with coefficient regularization, shape-adaptive filtering, and quantization constraint", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:HDshCWvjkbEC", "authors": ["Guangtao Zhai", "Wenjun Zhang", "Xiaokang Yang", "Weisi Lin", "Yi Xu"], "publication_date": "2008/6/13", "journal": "IEEE Transactions on Multimedia", "description": "We propose an effective deblocking scheme with extremely low computational complexity. The algorithm involves three parts: local ac coefficient regularization (ACR) of shifted blocks in the discrete cosine transform (DCT) domain, block-wise shape adaptive filtering (BSAF) in the spatial domain, and quantization constraint (QC) in the DCT domain. The DCT domain ACR suppresses the grid noise (blockiness) in monotone areas. The spatial-domain BSAF alleviates the staircase noise along the edge, and the ringing near the edge and the corner outliers. The narrow quantization constraint set is imposed to prevent possible oversmoothing and improve PSNR performance. Extensive simulation results and comparative studies are provided to justify the effectiveness and efficiency of the proposed deblocking algorithm.", "total_citations": 75, "citation_graph": {"2007": 1, "2008": 0, "2009": 2, "2010": 2, "2011": 14, "2012": 6, "2013": 8, "2014": 5, "2015": 4, "2016": 6, "2017": 6, "2018": 4, "2019": 4, "2020": 4, "2021": 2, "2022": 6}}, {"title": "Subjective and objective quality assessment of compressed screen content images", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:5thYEm8kiqcC", "authors": ["Shiqi Wang", "Ke Gu", "Xiang Zhang", "Weisi Lin", "Li Zhang", "Siwei Ma", "Wen Gao"], "publication_date": "2016/8/31", "journal": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems", "description": "Objectively accessing the quality of screen content images (SCIs) is a challenging problem, as SCIs may not always have identical properties as natural scenes. Here we conduct comprehensive studies on the subjective and objective quality assessment of the compressed SCIs. Firstly, we build a database that contains the distorted SCIs generated by the high efficiency video coding standard as well as its extension on screen content compression. Subsequently, subjective experiments are conducted to evaluate the perceived quality of these SCIs with compression artifacts. To automatically predict the subjective quality, a reduced-reference quality assessment model is further learnt by a set of wavelet domain features concerning the generalized spectral behavior, the fluctuations of the energy, and the information content with relatively large scale training samples. Our experimental results show that the learnt \u2026", "total_citations": 74, "citation_graph": {"2016": 1, "2017": 8, "2018": 11, "2019": 13, "2020": 14, "2021": 9, "2022": 14, "2023": 4}}, {"title": "Scene-based movie summarization via role-community networks", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&pagesize=100&citation_for_view=D_S41X4AAAAJ:qCpRzq7zkD8C", "authors": ["Chia-Ming Tsai", "Li-Wei Kang", "Chia-Wen Lin", "Weisi Lin"], "publication_date": "2013/6/18", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "description": "Video summarization techniques aim at condensing a full-length video to a significantly shortened version that still preserves the major semantic content of the original video. Movie summarization, being a special class of video summarization, is particularly challenging since a large variety of movie scenarios and film styles complicate the problem. In this paper, we propose a two-stage scene-based movie summarization method based on mining the relationship between role-communities since the role-communities in earlier scenes are usually used to develop the role relationship in later scenes. In the analysis stage, we construct a social network to characterize the interactions between role-communities. As a result, the social power of each role-community is evaluated by the community's centrality value and the role communities are clustered into relevant groups based on the centrality values. In the \u2026", "total_citations": 74, "citation_graph": {"2014": 4, "2015": 9, "2016": 8, "2017": 10, "2018": 8, "2019": 12, "2020": 7, "2021": 7, "2022": 5, "2023": 4}}, {"title": "Direct intermode selection for H. 264 video coding using phase correlation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:vDZJ-YLwNdEC", "authors": ["Manoranjan Paul", "Lin Weisi", "TONG LAU CHIEW", "Bu-sung Lee"], "publication_date": "2011", "journal": "IEEE transactions on image processing", "description": "The H.264 video coding standard exhibits higher performance compared to the other existing standards such as H.263, MPEG-X. This improved performance is achieved mainly due to the multiple-mode motion estimation and compensation. Recent research tried to reduce the computational time using the predictive motion estimation, early zero motion vector detection, fast motion estimation, and fast mode decision, etc. These approaches reduce the computational time substantially, at the expense of degrading image quality and/or increase bitrates to a certain extent. In this paper, we use phase correlation to capture the motion information between the current and reference blocks and then devise an algorithm for direct motion estimation mode prediction, without excessive motion estimation. A bigger amount of computational time is reduced by the direct mode decision and exploitation of available motion vector \u2026", "total_citations": 74, "citation_graph": {"2011": 2, "2012": 8, "2013": 7, "2014": 8, "2015": 12, "2016": 14, "2017": 8, "2018": 5, "2019": 2, "2020": 3, "2021": 2, "2022": 1}}, {"title": "Just-noticeable-distortion profile with nonlinear additivity model for perceptual masking in color images", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:kz9GbA2Ns4gC", "authors": ["XK Yang", "WS Lin", "Zhongkang Lu", "Ee Ping Ong", "Susu Yao"], "publication_date": "2003/4/6", "conference": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP'03).", "description": "We propose a new spatial just noticeable distortion (JND) profile for color image processing. The JND threshold depends on various masking effects underlying existing in the human vision system (HVS). How to efficiently integrate different masking effects together is the key issue of modelling the JND profile. Based on recent vision research results, we model the masking effects in different stimulus dimensions as a nonlinear additivity model for masking (NAMM). It applies to all color components and accounts for the compound impact of luminance masking and texture masking to estimate the JND threshold in images. In our PSNR and subjective comparison to the related work, the proposed NAMM scheme provides a more accurate JND profile towards the actual JND bound in the HVS.", "total_citations": 73, "citation_graph": {"2003": 1, "2004": 6, "2005": 4, "2006": 5, "2007": 3, "2008": 7, "2009": 1, "2010": 1, "2011": 4, "2012": 4, "2013": 2, "2014": 7, "2015": 5, "2016": 5, "2017": 4, "2018": 3, "2019": 2, "2020": 5, "2021": 0, "2022": 0, "2023": 2}}, {"title": "Toward a blind deep quality evaluator for stereoscopic images based on monocular and binocular interactions", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:m03se8k-GH0C", "authors": ["Feng Shao", "Weijun Tian", "Weisi Lin", "Gangyi Jiang", "Qionghai Dai"], "publication_date": "2016/3/4", "journal": "IEEE Transactions on Image Processing", "description": "During recent years, blind image quality assessment (BIQA) has been intensively studied with different machine learning tools. Existing BIQA metrics, however, do not design for stereoscopic images. We believe this problem can be resolved by separating 3D images and capturing the essential attributes of images via deep neural network. In this paper, we propose a blind deep quality evaluator (DQE) for stereoscopic images (denoted by 3D-DQE) based on monocular and binocular interactions. The key technical steps in the proposed 3D-DQE are to train two separate 2D deep neural networks (2D-DNNs) from 2D monocular images and cyclopean images to model the process of monocular and binocular quality predictions, and combine the measured 2D monocular and cyclopean quality scores using different weighting schemes. Experimental results on four public 3D image quality assessment databases \u2026", "total_citations": 72, "citation_graph": {"2016": 3, "2017": 10, "2018": 17, "2019": 15, "2020": 12, "2021": 7, "2022": 5, "2023": 2}}, {"title": "HodgeRank on random graphs for subjective video quality assessment", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:7Hz3ACDFbsoC", "authors": ["Qianqian Xu", "Qingming Huang", "Tingting Jiang", "Bowei Yan", "Weisi Lin", "Yuan Yao"], "publication_date": "2012/3/14", "journal": "IEEE Transactions on Multimedia", "description": "This paper introduces a novel framework, HodgeRank on Random Graphs, based on paired comparison, for subjective video quality assessment. Two types of random graph models are studied, i.e., Erd\u00f6s-R\u00e9nyi random graphs and random regular graphs. Hodge decomposition of paired comparison data may derive, from incomplete and imbalanced data, quality scores of videos and inconsistency of participants' judgments. We demonstrate the effectiveness of the proposed framework on LIVE video database. Both of the two random designs are promising sampling methods without jeopardizing the accuracy of the results. In particular, due to balanced sampling, random regular graphs may achieve better performances when sampling rates are small. However, when the number of videos is large or when sampling rates are large, their performances are so close that Erd\u00f6s-R\u00e9nyi random graphs, as the simplest \u2026", "total_citations": 72, "citation_graph": {"2012": 1, "2013": 7, "2014": 10, "2015": 6, "2016": 11, "2017": 5, "2018": 5, "2019": 5, "2020": 4, "2021": 6, "2022": 7, "2023": 4}}, {"title": "Blind image blur identification in cepstrum domain", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:ZeXyd9-uunAC", "authors": ["Shiqian Wu", "Zhongkang Lu", "Ee Ping Ong", "Weisi Lin"], "publication_date": "2007/8/13", "conference": "2007 16th International Conference on Computer Communications and Networks", "description": "The type and extent of blur affect image quality and therefore its evaluation. This paper presents an accurate method for blur identification and parameter estimation from one image without a priori knowledge. The key idea of the proposed method is to perform Fourier transform of logarithm spectrum to detect the periodic blur pattern in cepstrum domain instead of spectral nulls. Accordingly, the estimation of blur parameters is more accurate and robust to noise. The experimental results validate the accuracy of the proposed method.", "total_citations": 72, "citation_graph": {"2008": 3, "2009": 8, "2010": 4, "2011": 10, "2012": 9, "2013": 5, "2014": 6, "2015": 8, "2016": 4, "2017": 4, "2018": 2, "2019": 4, "2020": 1, "2021": 3, "2022": 0, "2023": 1}}, {"title": "Low-rank-based nonlocal adaptive loop filter for high-efficiency video compression", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:vYYylRVofzEC", "authors": ["Xinfeng Zhang", "Ruiqin Xiong", "Weisi Lin", "Jian Zhang", "Shiqi Wang", "Siwei Ma", "Wen Gao"], "publication_date": "2016/6/15", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "description": "In video coding, the in-loop filtering has emerged as a key module due to its significant improvement on compression performance since H.264/Advanced Video Coding. Existing incorporated in-loop filters in video coding standards mainly take advantage of the local smoothness prior model used for images. In this paper, we propose a novel adaptive loop filter utilizing image nonlocal prior knowledge by imposing the low-rank constraint on similar image patches for compression noise reduction. In the filtering process, the reconstructed frame is first divided into image patch groups according to image patch similarity. The proposed in-loop filtering is formulated as an optimization problem with low-rank constraint for every group of image patches independently. It can be efficiently solved by soft-thresholding singular values of the matrix composed of image patches in the same group. To adapt the properties of the input \u2026", "total_citations": 71, "citation_graph": {"2016": 3, "2017": 5, "2018": 10, "2019": 10, "2020": 13, "2021": 15, "2022": 11, "2023": 4}}, {"title": "Explore and model better I-frames for video coding", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:8AbLer7MMksC", "authors": ["Manoranjan Paul", "Weisi Lin", "Chiew-Tong Lau", "Bu-Sung Lee"], "publication_date": "2011/4/7", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "description": "In video coding, an intra (I)-frame is used as an anchor frame for referencing the subsequence frames, as well as error propagation prevention, indexing, and so on. To get better rate-distortion performance, a frame should have the following quality to be an ideal I-frame: the best similarity with the frames in a group of picture (GOP), so that when it is used as a reference frame for a frame in the GOP we need the least bits to achieve the desired image quality, minimize the temporal fluctuation of quality, and also maintain a more consistent bit count per frame. In this paper we use a most common frame of a scene in a video sequence with dynamic background modeling and then encode it to replace the conventional I-frame. The extensive experimental results confirm the superiority of our proposed scheme in comparison with the existing state-of-art methods by significant image quality improvement and computational \u2026", "total_citations": 71, "citation_graph": {"2012": 3, "2013": 8, "2014": 8, "2015": 11, "2016": 8, "2017": 13, "2018": 4, "2019": 3, "2020": 3, "2021": 6, "2022": 0, "2023": 4}}, {"title": "Backward registration-based aspect ratio similarity for image retargeting quality assessment", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:hwlm9Y4obscC", "authors": ["Yabin Zhang", "Yuming Fang", "Weisi Lin", "Xinfeng Zhang", "Leida Li"], "publication_date": "2016/6/28", "journal": "IEEE Transactions on image processing", "description": "During the past few years, there have been various kinds of content-aware image retargeting operators proposed for image resizing. However, the lack of effective objective retargeting quality assessment metrics limits the further development of image retargeting techniques. Different from traditional image quality assessment (IQA) metrics, the quality degradation during image retargeting is caused by artificial retargeting modifications, and the difficulty for image retargeting quality assessment (IRQA) lies in the alternation of the image resolution and content, which makes it impossible to directly evaluate the quality degradation like traditional IQA. In this paper, we interpret the image retargeting in a unified framework of resampling grid generation and forward resampling. We show that the geometric change estimation is an efficient way to clarify the relationship between the images. We formulate the geometric change \u2026", "total_citations": 70, "citation_graph": {"2016": 1, "2017": 9, "2018": 11, "2019": 12, "2020": 9, "2021": 14, "2022": 9, "2023": 5}}, {"title": "Pattern masking estimation in image with structural uncertainty", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:2Q0AJrNhS-QC", "authors": ["Jinjian Wu", "Weisi Lin", "Guangming Shi", "Xiaotian Wang", "Fu Li"], "publication_date": "2013/8/30", "journal": "IEEE Transactions on Image Processing", "description": "A model of visual masking, which reveals the visibility of stimuli in the human visual system (HVS), is useful in perceptual based image/video processing. The existing visual masking function mainly considers luminance contrast, which always overestimates the visibility threshold of the edge region and underestimates that of the texture region. Recent research on visual perception indicates that the HVS is sensitive to orderly regions that possess regular structures and insensitive to disorderly regions that possess uncertain structures. Therefore, structural uncertainty is another determining factor on visual masking. In this paper, we introduce a novel pattern masking function based on both luminance contrast and structural uncertainty. Through mimicking the internal generative mechanism of the HVS, a prediction model is firstly employed to separate out the unpredictable uncertainty from an input image. In addition \u2026", "total_citations": 70, "citation_graph": {"2014": 1, "2015": 5, "2016": 8, "2017": 8, "2018": 8, "2019": 13, "2020": 12, "2021": 7, "2022": 4, "2023": 3}}, {"title": "Unified information fusion network for multi-modal RGB-D and RGB-T salient object detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:2BeMVx_SZpEC", "authors": ["Wei Gao", "Guibiao Liao", "Siwei Ma", "Ge Li", "Yongsheng Liang", "Weisi Lin"], "publication_date": "2021/5/24", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "description": "The use of complementary information, namely depth or thermal information, has shown its benefits to salient object detection (SOD) during recent years. However, the RGB-D or RGB-T SOD problems are currently only solved independently, and most of them directly extract and fuse raw features from backbones. Such methods can be easily restricted by low-quality modality data and redundant cross-modal features. In this work, a unified end-to-end framework is designed to simultaneously analyze RGB-D and RGB-T SOD tasks. Specifically, to effectively tackle multi-modal features, we propose a novel multi-stage and multi-scale fusion network (MMNet), which consists of a cross-modal multi-stage fusion module (CMFM) and a bi-directional multi-scale decoder (BMD). Similar to the visual color stage doctrine in the human visual system (HVS), the proposed CMFM aims to explore important feature representations in \u2026", "total_citations": 69, "citation_graph": {"2021": 2, "2022": 22, "2023": 44}}, {"title": "Joint bit allocation and rate control for coding multi-view video plus depth based 3D video", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:tgTmbKTkO1IC", "authors": ["Feng Shao", "Gangyi Jiang", "Weisi Lin", "Mei Yu", "Qionghai Dai"], "publication_date": "2013/6/19", "journal": "IEEE transactions on multimedia", "description": "In three-dimensional (3D) video coding, distortion in texture video and depth maps can all affect the quality of the synthesized virtual views. Therefore, under the total bitrate constraint, effective bit allocation between texture and depth information is very important for 3D video coding. In this paper, the major technical contribution is to formulate view synthesis quality for optimal resource allocation in 3D video coding, since such quality is what that matters most to the ultimate user (i.e., the viewer) of the system; to be more specific, a new joint bit allocation and rate control method for multi-view video plus depth (MVD) based 3D video coding is proposed accordingly. We firstly derive a view synthesis distortion model to characterize the effect of coding distortion of texture video and depth maps on the synthesized virtual views. Based on this model, we derive a rate-distortion model to characterize the relationship between \u2026", "total_citations": 69, "citation_graph": {"2014": 8, "2015": 12, "2016": 7, "2017": 10, "2018": 11, "2019": 10, "2020": 8, "2021": 2, "2022": 1}}, {"title": "Conjunctive patches subspace learning with side information for collaborative image retrieval", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:artPoR2Yc-kC", "authors": ["Lining Zhang", "Lipo Wang", "Weisi Lin"], "publication_date": "2012/4/17", "journal": "IEEE Transactions on Image Processing", "description": "Content-based image retrieval (CBIR) has attracted substantial attention during the past few years for its potential practical applications to image management. A variety of relevance feedback schemes have been designed to bridge the semantic gap between low-level visual features and high-level semantic concepts for an image retrieval task. Various collaborative image retrieval (CIR) schemes aim to utilize the user historical feedback log data with similar and dissimilar pairwise constraints to improve the performance of a CBIR system. However, existing subspace learning approaches with explicit label information cannot be applied for a CIR task although the subspace learning techniques play a key role in various computer vision tasks, e.g., face recognition and image classification. In this paper, we propose a novel subspace learning framework, i.e., conjunctive patches subspace learning (CPSL) with side \u2026", "total_citations": 69, "citation_graph": {"2013": 7, "2014": 3, "2015": 11, "2016": 9, "2017": 9, "2018": 9, "2019": 8, "2020": 7, "2021": 1, "2022": 2}}, {"title": "PM\u2082. \u2085 monitoring: use information abundance measurement and wide and deep learning", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:IT5EXw6i2GUC", "authors": ["Ke Gu", "Hongyan Liu", "Zhifang Xia", "Junfei Qiao", "Weisi Lin", "Daniel Thalmann"], "publication_date": "2021/8/30", "journal": "IEEE Transactions on Neural Networks and Learning Systems", "description": "This article devises a photograph-based monitoring model to estimate the real-time PM 2.5 concentrations, overcoming currently popular electrochemical sensor-based PM 2.5 monitoring methods\u2019 shortcomings such as low-density spatial distribution and time delay. Combining the proposed monitoring model, the photographs taken by various camera devices (e.g., surveillance camera, automobile data recorder, and mobile phone) can widely monitor PM 2.5 concentration in megacities. This is beneficial to offering helpful decision-making information for atmospheric forecast and control, thus reducing the epidemic of COVID-19. To specify, the proposed model fuses Information Abundance measurement and Wide and Deep learning, dubbed as IAWD, for PM 2.5 monitoring. First, our model extracts two categories of features in a newly proposed DS transform space to measure the information abundance (IA) of a \u2026", "total_citations": 68, "citation_graph": {"2021": 7, "2022": 31, "2023": 29}}, {"title": "B-SHOT: A binary feature descriptor for fast and efficient keypoint matching on 3D point clouds", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:Vch7EZszQGgC", "authors": ["Sai Manoj Prakhya", "Bingbing Liu", "Weisi Lin"], "publication_date": "2015/9/28", "conference": "2015 IEEE/RSJ international conference on intelligent robots and systems (IROS)", "description": "In this paper, we introduce the very first `binary' 3D feature descriptor, B-SHOT, for fast and efficient keypoint matching on 3D point clouds. We propose a binary quantization method that converts a real valued vector to a binary vector. We apply this method on a state-of-the-art 3D feature descriptor, SHOT [1], and create a new binary 3D feature descriptor. B-SHOT requires 32 times lesser memory for its representation while being 6 times faster in feature descriptor matching, when compared to the SHOT feature descriptor. Experimental evaluation shows that B-SHOT offers comparable keypoint matching performance to that of the state-of-the-art 3D feature descriptors on a standard benchmark dataset.", "total_citations": 68, "citation_graph": {"2016": 6, "2017": 12, "2018": 8, "2019": 13, "2020": 6, "2021": 11, "2022": 9, "2023": 3}}, {"title": "Geometric optimum experimental design for collaborative image retrieval", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:DGzKIA18-3YC", "authors": ["Lining Zhang", "Lipo Wang", "Weisi Lin", "Shuicheng Yan"], "publication_date": "2013/8/2", "journal": "IEEE Transactions on Circuits and Systems for Video technology", "description": "Relevance feedback (RF) schemes have been widely designed to improve the performance of content-based image retrieval. Despite the success, it is not appropriate to require the user to label a large number of samples in RF. Collaborative image retrieval (CIR) aims to reduce the labeling efforts of the user by resorting to the auxiliary information. Support vector machine (SVM) active learning can select ambiguous samples as the most informative ones for the user to label with the help of the optimal hyperplane of SVM, and thus alleviate the labeling efforts of conventional RF. However, the optimal hyperplane of SVM is usually unstable and inaccurate with small-sized training data, and this is always the case in image retrieval since the user would not like to label a large number of feedback samples and cannot label each sample accurately all the time. In this paper, we propose a novel active learning method, i.e \u2026", "total_citations": 68, "citation_graph": {"2013": 1, "2014": 7, "2015": 23, "2016": 8, "2017": 7, "2018": 4, "2019": 7, "2020": 2, "2021": 1, "2022": 3, "2023": 2}}, {"title": "Adaptive downsampling/upsampling for better video compression at low bit rate", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:yD5IFk8b50cC", "authors": ["Viet-Anh Nguyen", "Yap-Peng Tan", "Weisi Lin"], "publication_date": "2008/5/18", "conference": "2008 IEEE International Symposium on Circuits and Systems (ISCAS)", "description": "To transmit video contents over limited bandwidth network, video bitstreams may need to reduce the bit rate by encoding with coarse quantization parameters at the expense of degrading quality. At low bit rates, better coding quality can be achieved by downsampling the video prior to compression and upsampling later after decompression. In this paper, we present an adaptive downsampling/upsampling video coding scheme in order to achieve better video quality at low bit rates in terms of both measure and visual quality. In particular, appropriate downsampling directions/ratios and quantization step sizes are adaptively decided for encoding different regions of video frame with the consideration of local contents. Experimental results have shown the better performance of the proposed scheme over the regular coding and downsampling-based coding scheme with fixed downscaling ratio. In addition, the proposed \u2026", "total_citations": 68, "citation_graph": {"2009": 1, "2010": 2, "2011": 1, "2012": 4, "2013": 3, "2014": 4, "2015": 5, "2016": 5, "2017": 5, "2018": 7, "2019": 5, "2020": 4, "2021": 7, "2022": 9, "2023": 6}}, {"title": "Context-aware deep learning for multi-modal depression detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:KUazKHuGu6AC", "authors": ["Genevieve Lam", "Huang Dongyan", "Weisi Lin"], "publication_date": "2019/5/12", "conference": "ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP)", "description": "In this study, we focus on automated approaches to detect depression from clinical interviews using machine learning approached, which the models are trained on multi-modal data. Differentiating from successful machine learning approaches such as context-aware analysis through feature engineering and end-to-end deep neural networks to depression detection utilizing the Distress Analysis Interview Corpus, we propose a novel method that incorporates a data augmentation procedure based on topic modelling using transformer and deep 1D convolutional neural network (CNN) for acoustic feature modeling. The simulation results demonstrate the effectiveness of the proposed method for training multi-modal deep learning models. Our deep 1D CNN and transformer models achieve the state-of-the-art performance for the audio and text modalities respectively, while our multi-modal results are comparable with \u2026", "total_citations": 67, "citation_graph": {"2019": 1, "2020": 4, "2021": 16, "2022": 22, "2023": 23}}, {"title": "Salient region detection by fusing bottom-up and top-down features extracted from a single image", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:q09DtPQ_434C", "authors": ["Huawei Tian", "Yuming Fang", "Yao Zhao", "Weisi Lin", "Rongrong Ni", "Zhenfeng Zhu"], "publication_date": "2014/8/22", "journal": "IEEE Transactions on Image processing", "description": "Recently, some global contrast-based salient region detection models have been proposed based on only the low-level feature of color. It is necessary to consider both color and orientation features to overcome their limitations, and thus improve the performance of salient region detection for images with low-contrast in color and high-contrast in orientation. In addition, the existing fusion methods for different feature maps, like the simple averaging method and the selective method, are not effective sufficiently. To overcome these limitations of existing salient region detection models, we propose a novel salient region model based on the bottom-up and top-down mechanisms: the color contrast and orientation contrast are adopted to calculate the bottom-up feature maps, while the top-down cue of depth-from-focus from the same single image is used to guide the generation of final salient regions, since depth-from \u2026", "total_citations": 67, "citation_graph": {"2015": 8, "2016": 13, "2017": 9, "2018": 6, "2019": 9, "2020": 16, "2021": 4, "2022": 0, "2023": 1}}, {"title": "Blind blur assessment for vision-based applications", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:hMod-77fHWUC", "authors": ["Shiqian Wu", "Weisi Lin", "Shoulie Xie", "Zhongkang Lu", "Ee Ping Ong", "Susu Yao"], "publication_date": "2009/5/1", "journal": "Journal of Visual Communication and Image Representation", "description": "In this paper, a criterion for objective defocus blur measurement is theoretically derived from one image. The essential idea is to estimate the point spread function (PSF) from the line spread function (LSF), whereas the LSF is constructed from edge information. It is proven that an edge point corresponds to the local maximal gradient in a blurred image, and therefore edges can be extracted from blurred images by conventional edge detectors. To achieve high accuracy, local Radon transform is implemented and a number of LSFs are extracted from each edge. The experimental results on a variety of synthetic and real blurred images validate the proposed method. The algorithm can be implemented for image quality evaluation in vision-based applications as no reference images are needed.", "total_citations": 67, "citation_graph": {"2008": 1, "2009": 2, "2010": 3, "2011": 3, "2012": 8, "2013": 4, "2014": 11, "2015": 6, "2016": 6, "2017": 0, "2018": 6, "2019": 5, "2020": 4, "2021": 2, "2022": 4, "2023": 1}}, {"title": "Robust image compression based on compressive sensing", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:u9iWguZQMMsC", "authors": ["Chenwei Deng", "Weisi Lin", "Bu-sung Lee", "Chiew Tong Lau"], "publication_date": "2010/7/19", "conference": "2010 IEEE International Conference on Multimedia and Expo", "description": "The existing image compression methods (e.g., JPEG2000, etc.) are vulnerable to bit-loss, and this is usually tackled by channel coding that follows. However, source coding and channel coding have conflicting requirement. In this paper, we address the problem with an alternative paradigm, and a novel compressive sensing (CS) based compression scheme is therefore proposed. Discrete wavelet transform (DWT) is applied for sparse representation, and based on the property of 2-D DWT, a fast CS measurements taking method is presented. Unlike the unequally important discrete wavelet coefficients, the resultant CS measurements carry nearly the same amount of information and have minimal effects for bit-loss. At the decoder side, one can simply reconstruct the image via l 1 minimization. Experimental results show that the proposed CS-based image codec without resorting to error protection is more robust \u2026", "total_citations": 66, "citation_graph": {"2011": 4, "2012": 6, "2013": 8, "2014": 10, "2015": 6, "2016": 6, "2017": 9, "2018": 4, "2019": 2, "2020": 3, "2021": 4, "2022": 3}}, {"title": "Contrast signal-to-noise ratio for image quality assessment", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:WbkHhVStYXYC", "authors": ["Susu Yao", "Weisi Lin", "EePing Ong", "Zhongkang Lu"], "publication_date": "2005/9/14", "conference": "IEEE International Conference on Image Processing 2005", "description": "Peak signal-to-noise ratio (PSNR) is commonly used as an objective quality metric in signal processing. However, PSNR correlates poorly with the subjective quality rating. In this paper, we propose a new metric using contrast signal-to-noise ratio (CSNR), which measures the ratio of the contrast information level of distorted signal to the contrast level of the error signal. The performance of the proposed method has been verified using a database of images compressed with JPEG and JPEG 2000. The results show that it can achieve very good correlation with the subjective mean opinion scores in terms of prediction accuracy, monotonicity and consistency.", "total_citations": 66, "citation_graph": {"2007": 1, "2008": 0, "2009": 2, "2010": 0, "2011": 0, "2012": 2, "2013": 1, "2014": 4, "2015": 10, "2016": 6, "2017": 5, "2018": 1, "2019": 6, "2020": 6, "2021": 9, "2022": 7, "2023": 6}}, {"title": "Visual orientation selectivity based structure description", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:z-B63o8J19IC", "authors": ["Jinjian Wu", "Weisi Lin", "Guangming Shi", "Yazhong Zhang", "Weisheng Dong", "Zhibo Chen"], "publication_date": "2015/7/23", "journal": "IEEE Transactions on Image Processing", "description": "The human visual system is highly adaptive to extract structure information for scene perception, and structure character is widely used in perception-oriented image processing works. However, the existing structure descriptors mainly describe the luminance contrast of a local region, but cannot effectively represent the spatial correlation of structure. In this paper, we introduce a novel structure descriptor according to the orientation selectivity mechanism in the primary visual cortex. Research on cognitive neuroscience indicate that the arrangement of excitatory and inhibitory cortex cells arise orientation selectivity in a local receptive field, within which the primary visual cortex performs visual information extraction for scene understanding. Inspired by the orientation selectivity mechanism, we compute the correlations among pixels in a local region based on the similarities of their preferred orientation. By imitating the \u2026", "total_citations": 65, "citation_graph": {"2016": 2, "2017": 11, "2018": 9, "2019": 14, "2020": 10, "2021": 9, "2022": 7, "2023": 2}}, {"title": "Just-noticeable difference-based perceptual optimization for JPEG compression", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:bxbQgRQgr4sC", "authors": ["Xinfeng Zhang", "Shiqi Wang", "Ke Gu", "Weisi Lin", "Siwei Ma", "Wen Gao"], "publication_date": "2016/12/19", "journal": "IEEE Signal Processing Letters", "description": "The Quantization table in JPEG, which specifies the quantization scale for each discrete cosine transform (DCT) coefficient, plays an important role in image codec optimization. However, the generic quantization table design that is based on the characteristics of human visual system (HVS) cannot adapt to the variations of image content. In this letter, we propose a just-noticeable difference (JND) based quantization table derivation method for JPEG by optimizing the rate-distortion costs for all the frequency bands. To achieve better perceptual quality, the DCT domain JND-based distortion metric is utilized to model the stair distortion perceived by HVS. The rate-distortion cost for each band is derived by estimating the rate with the first-order entropy of quantized coefficients. Subsequently, the optimal quantization table is obtained by minimizing the total rate-distortion costs of all the bands. Extensive experimental \u2026", "total_citations": 63, "citation_graph": {"2017": 4, "2018": 14, "2019": 9, "2020": 14, "2021": 6, "2022": 9, "2023": 7}}, {"title": "A long-term reference frame for hierarchical B-picture-based video coding", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:koF6b02d8EEC", "authors": ["Manoranjan Paul", "Weisi Lin", "Chiew-Tong Lau", "Bu Sung Lee"], "publication_date": "2014/1/28", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "description": "Generally, H.264/AVC video coding standard with hierarchical bipredictive picture (HBP) structure outperforms the classical prediction structures such as \u201cIPPP...\u201d and \u201cIBBP...\u201d through better exploitation of data correlation using reference frames and unequal quantization setting among frames. However, multiple reference frames (MRFs) techniques are not fully exploited in the HBP scheme because of the computational requirement for B-frames, unavailability of adjacent reference frames, and with no explicit sorting of the reference frames for foreground or background being used. To exploit MRFs fully and explicitly in background referencing, we observe that not a single frame of a video is appropriate to be the reference frame as no one covers adequate background of a video. To overcome the problems, we propose a new coding scheme with the HBP, which uses the most common frame in scene (McFIS \u2026", "total_citations": 62, "citation_graph": {"2014": 1, "2015": 5, "2016": 12, "2017": 10, "2018": 13, "2019": 7, "2020": 4, "2021": 5, "2022": 1, "2023": 4}}, {"title": "Low-rank decomposition-based restoration of compressed images via adaptive noise estimation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:bYbwfsIO_fQC", "authors": ["Xinfeng Zhang", "Weisi Lin", "Ruiqin Xiong", "Xianming Liu", "Siwei Ma", "Wen Gao"], "publication_date": "2016/7/7", "journal": "IEEE Transactions on Image processing", "description": "Images coded at low bit rates in real-world applications usually suffer from significant compression noise, which significantly degrades the visual quality. Traditional denoising methods are not suitable for the content-dependent compression noise, which usually assume that noise is independent and with identical distribution. In this paper, we propose a unified framework of content-adaptive estimation and reduction for compression noise via low-rank decomposition of similar image patches. We first formulate the framework of compression noise reduction based upon low-rank decomposition. Compression noises are removed by soft thresholding the singular values in singular value decomposition of every group of similar image patches. For each group of similar patches, the thresholds are adaptively determined according to compression noise levels and singular values. We analyze the relationship of image \u2026", "total_citations": 61, "citation_graph": {"2016": 3, "2017": 10, "2018": 18, "2019": 8, "2020": 8, "2021": 7, "2022": 5, "2023": 2}}, {"title": "A closed-form estimate of 3D ICP covariance", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:RZBefGmQYygC", "authors": ["Sai Manoj Prakhya", "Liu Bingbing", "Yan Rui", "Weisi Lin"], "publication_date": "2015/5/18", "conference": "2015 14th IAPR International Conference on Machine Vision Applications (MVA)", "description": "We present a closed-form solution to estimate the covariance of the resultant transformation provided by the Iterative Closest Point (ICP) algorithm for 3D point cloud registration. We extend an existing work [1] that estimates ICP's covariance in 2D with point to plane error metric to 3D with point to point and point to plane error metrics. Moreover, we do not make any assumption on the noise present in the sensor data and have no constraints on the estimated rigid transformation. The source code of our implementation is made publicly available, which can be adapted to work for ICP with different error metrics with minor changes. Our preliminary results show that ICP's covariance is lower at a global minimum than at a local minima.", "total_citations": 61, "citation_graph": {"2014": 1, "2015": 1, "2016": 5, "2017": 7, "2018": 6, "2019": 9, "2020": 9, "2021": 5, "2022": 13, "2023": 4}}, {"title": "Selective visual attention: computational models and applications", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:uUvzmPk0f8oC", "authors": ["Liming Zhang", "Weisi Lin"], "publication_date": "2013/3/15", "description": "Visual attention is a relatively new area of study combining a number of disciplines: artificial neural networks, artificial intelligence, vision science and psychology. The aim is to build computational models similar to human vision in order to solve tough problems for many potential applications including object recognition, unmanned vehicle navigation, and image and video coding and processing. In this book, the authors provide an up to date and highly applied introduction to the topic of visual attention, aiding researchers in creating powerful computer vision systems. Areas covered include the significance of vision research, psychology and computer vision, existing computational visual attention models, and the authors' contributions on visual attention models, and applications in various image and video processing tasks. This book is geared for graduates students and researchers in neural networks, image processing, machine learning, computer vision, and other areas of biologically inspired model building and applications. The book can also be used by practicing engineers looking for techniques involving the application of image coding, video processing, machine vision and brain-like robots to real-world systems. Other students and researchers with interdisciplinary interests will also find this book appealing. Provides a key knowledge boost to developers of image processing applications Is unique in emphasizing the practical utility of attention mechanisms Includes a number of real-world examples that readers can implement in their own work: robot navigation and object selection image and video quality assessment image and video \u2026", "total_citations": 61, "citation_graph": {"2013": 1, "2014": 4, "2015": 4, "2016": 5, "2017": 10, "2018": 9, "2019": 11, "2020": 6, "2021": 5, "2022": 5, "2023": 1}}, {"title": "Content-based image compression for arbitrary-resolution display devices", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:XD-gHx7UXLsC", "authors": ["Chenwei Deng", "Weisi Lin", "Jianfei Cai"], "publication_date": "2012/3/19", "journal": "IEEE transactions on multimedia", "description": "The existing image coding methods cannot support content-based spatial scalability with high compression. In mobile multimedia communications, image retargeting is generally required at the user end. However, content-based image retargeting (e.g., seam carving) is with high computational complexity and is not suitable for mobile devices with limited computing power. The work presented in this paper addresses the increasing demand of visual signal delivery to terminals with arbitrary resolutions, without heavy computational burden to the receiving end. In this paper, the principle of seam carving is incorporated into a wavelet codec (i.e., SPIHT ). For each input image, block-based seam energy map is generated in the pixel domain. In the meantime, multilevel discrete wavelet transform (DWT) is performed. Different from the conventional wavelet-based coding schemes, DWT coefficients here are grouped and \u2026", "total_citations": 61, "citation_graph": {"2012": 2, "2013": 4, "2014": 7, "2015": 7, "2016": 10, "2017": 8, "2018": 4, "2019": 3, "2020": 8, "2021": 6, "2022": 2}}, {"title": "No-reference JPEG-2000 image quality metric", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:IWHjjKOFINEC", "authors": ["EePing Ong", "Weisi Lin", "Zhongkang Lu", "Susu Yao", "Xiaokang Yang", "Lijun Jiang"], "publication_date": "2003/7/6", "conference": "2003 international conference on multimedia and expo. ICME'03. Proceedings (cat. No. 03TH8698)", "description": "In this paper, a method for measuring the perceptual image quality of JPEG-2000 coded images has been proposed. The image quality is characterized by the average edge-spread in the image, or more specifically the average extent of the slope's spread of an edge in the opposing gradients' directions. The proposed method is, in effect, a way of measuring the amount of blurring in the image. The effectiveness of such method is validated using subjective tests and the experimental results show that the proposed method can provide results that correlate relatively well with human subjective ratings.", "total_citations": 61, "citation_graph": {"2004": 5, "2005": 2, "2006": 0, "2007": 1, "2008": 6, "2009": 2, "2010": 6, "2011": 6, "2012": 4, "2013": 7, "2014": 3, "2015": 5, "2016": 1, "2017": 2, "2018": 2, "2019": 2, "2020": 3, "2021": 0, "2022": 1, "2023": 1}}, {"title": "Multiscale natural scene statistical analysis for no-reference quality evaluation of DIBR-synthesized views", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:RW1BPcyHXiwC", "authors": ["Ke Gu", "Junfei Qiao", "Sanghoon Lee", "Hantao Liu", "Weisi Lin", "Patrick Le Callet"], "publication_date": "2019/5/1", "journal": "IEEE Transactions on Broadcasting", "description": "This paper proposes to blindly evaluate the quality of images synthesized via a depth image-based rendering (DIBR) procedure. As a significant branch of virtual reality (VR), superior DIBR techniques provide free viewpoints in many real applications, including remote surveillance and education; however, limited efforts have been made to measure the performance of DIBR techniques, or equivalently the quality of DIBR-synthesized views, especially in the condition when references are unavailable. To achieve this aim, we develop a novel blind image quality assessment (IQA) method via multiscale natural scene statistical analysis (MNSS). The design principle of our proposed MNSS metric is based on two new natural scene statistics (NSS) models specific to the DBIR-synthesized IQA. First, the DIBR-introduced geometric distortions damage the local self-similarity characteristic of natural images, and the damage \u2026", "total_citations": 60, "citation_graph": {"2019": 3, "2020": 9, "2021": 12, "2022": 19, "2023": 17}}, {"title": "BSD: Blind image quality assessment based on structural degradation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:WMj-6b1RDO4C", "authors": ["Qiaohong Li", "Weisi Lin", "Yuming Fang"], "publication_date": "2017/5/2", "journal": "Neurocomputing", "description": "Research in biological vision and neurology has evidenced that there are separate mechanisms in human visual cortex to process the first- and second-order patterns. Image structures detected by a linear filter are the first-order patterns which describe luminance changes, while patterns that are invisible to linear filters are often referred as the second-order structures. In this paper, we propose a general-purpose blind image quality assessment (BIQA) method by taking account of both the first- and second-order image structures. Specifically, the Prewitt linear filters are used to extract first-order image structures and the local contrast normalization is employed to extract second-order image structures. Perceptual features are extracted from these two image structural maps and used as the input to a support vector regression to model the nonlinear relationship between feature space to human opinion score. Extensive \u2026", "total_citations": 60, "citation_graph": {"2017": 4, "2018": 15, "2019": 11, "2020": 8, "2021": 12, "2022": 7, "2023": 2}}, {"title": "A new marker-based watershed algorithm", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:4JMBOYKVnBMC", "authors": ["Hai Gao", "Ping Xue", "Weisi Lin"], "publication_date": "2004/5/23", "conference": "2004 IEEE International Symposium on Circuits and Systems (ISCAS)", "description": "The marker-based watershed approach is a very efficient means for image segmentation and has been widely used in recent years. The conventional marker-based algorithms are realized using hierarchical queues. A new marker-based watershed algorithm based on the disjoint set data structure is proposed in this paper. It consists of two steps: the flooding step and the resolving step. This algorithm has significantly lower memory requirement as compared with the conventional algorithms while maintaining the computational complexity of O(N) where N is the image size. Experimental results further show that the new algorithm implemented in C language runs much faster than the conventional algorithm based on the hierarchical queues as a result of savings from huge memory allocation and releasing operations.", "total_citations": 59, "citation_graph": {"2006": 1, "2007": 2, "2008": 6, "2009": 5, "2010": 4, "2011": 5, "2012": 4, "2013": 1, "2014": 2, "2015": 4, "2016": 3, "2017": 4, "2018": 3, "2019": 4, "2020": 3, "2021": 2, "2022": 4, "2023": 2}}, {"title": "Blind image quality assessment with active inference", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:EXDW3tg14iEC", "authors": ["Jupo Ma", "Jinjian Wu", "Leida Li", "Weisheng Dong", "Xuemei Xie", "Guangming Shi", "Weisi Lin"], "publication_date": "2021/3/11", "journal": "IEEE Transactions on Image Processing", "description": "Blind image quality assessment (BIQA) is a useful but challenging task. It is a promising idea to design BIQA methods by mimicking the working mechanism of human visual system (HVS). The internal generative mechanism (IGM) indicates that the HVS actively infers the primary content (i.e., meaningful information) of an image for better understanding. Inspired by that, this paper presents a novel BIQA metric by mimicking the active inference process of IGM. Firstly, an active inference module based on the generative adversarial network (GAN) is established to predict the primary content, in which the semantic similarity and the structural dissimilarity (i.e., semantic consistency and structural completeness) are both considered during the optimization. Then, the image quality is measured on the basis of its primary content. Generally, the image quality is highly related to three aspects, i.e., the scene information (content \u2026", "total_citations": 58, "citation_graph": {"2021": 9, "2022": 23, "2023": 26}}, {"title": "Studying personality through the content of posted and liked images on Twitter", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:Dh4RK7yvr34C", "authors": ["Sharath Chandra Guntuku", "Weisi Lin", "Jordan Carpenter", "Wee Keong Ng", "Lyle H Ungar", "Daniel Preo\u0163iuc-Pietro"], "publication_date": "2017/6/25", "book": "Proceedings of the 2017 ACM on web science conference", "description": "Interacting with images through social media has become widespread due to ubiquitous Internet access and multimedia enabled devices. Through images, users generally present their daily activities, preferences or interests. This study aims to identify the way and extent to which personality differences, measured using the Big Five model, are related to online image posting and liking. In two experiments, the larger consisting of ~1.5 million Twitter images both posted and liked by ~4,000 users, we extract interpretable semantic concepts using large-scale image content analysis and analyze differences specific of each personality trait. Predictive results show that image content can predict personality traits, and that there can be significant performance gain by fusing the signal from both posted and liked images.", "total_citations": 57, "citation_graph": {"2017": 2, "2018": 8, "2019": 9, "2020": 7, "2021": 12, "2022": 12, "2023": 7}}, {"title": "Screen image quality assessment incorporating structural degradation measurement", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:5N-NJrZHaHcC", "authors": ["Ke Gu", "Shiqi Wang", "Guangtao \u0396hai", "Siwei Ma", "Weisi Lin"], "publication_date": "2015/5/24", "conference": "2015 IEEE International Symposium on Circuits and Systems (ISCAS)", "description": "Screen content is typically composed of computer generated text and graphics. The contents shown on the screen exhibit various unnatural properties, such as sharp edges and thin lines with few color variations. In this paper we design a novel structure-induced quality metric (SIQM) for assessing the screen image quality. The proposed SIQM works by weighting the benchmark structural similarity index (SSIM) with the structural degradation measurement that is computed using SSIM as well. Experimental results conducted on the newly released subjective quality database concerning screen images show that on one hand the proposed technique is superior to existing quality measures, and on the other hand our model is able to optimize screen video coding and thus introduce remarkable visual quality improvement.", "total_citations": 56, "citation_graph": {"2016": 3, "2017": 7, "2018": 7, "2019": 10, "2020": 5, "2021": 9, "2022": 10, "2023": 3}}, {"title": "Reducing location map in prediction-based difference expansion for reversible image data embedding", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:1DsIQWDZLl8C", "authors": ["Minglei Liu", "Hock Soon Seah", "Ce Zhu", "Weisi Lin", "Feng Tian"], "publication_date": "2011/10/5", "journal": "Signal Processing", "description": "In this paper, we present a reversible data embedding scheme based on an adaptive edge-directed prediction for images. It is known that the difference expansion is an efficient data embedding method. Since the expansion on a large difference will cause a significant embedding distortion, a location map is usually employed to select small differences for expansion and to avoid overflow/underflow problems caused by expansion. However, location map bits lower payload capacity for data embedding. To reduce the location map, our proposed scheme aims to predict small prediction errors for expansion by using an edge detector. Moreover, to generate a small prediction error for each pixel, an adaptive edge-directed prediction is employed which adapts reasonably well between smooth regions and edge areas. Experimental results show that our proposed data embedding scheme for natural images can achieve \u2026", "total_citations": 56, "citation_graph": {"2012": 3, "2013": 3, "2014": 5, "2015": 4, "2016": 0, "2017": 2, "2018": 9, "2019": 9, "2020": 7, "2021": 1, "2022": 7, "2023": 5}}, {"title": "BLIQUE-TMI: Blind quality evaluator for tone-mapped images based on local and global feature analyses", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:BZGggv0hN9sC", "authors": ["Qiuping Jiang", "Feng Shao", "Weisi Lin", "Gangyi Jiang"], "publication_date": "2017/12/15", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "description": "High dynamic range (HDR) image, which has a powerful capacity to represent the wide dynamic range of real-world scenes, has been receiving attention from both academic and industrial communities. Although HDR imaging devices have become prevalent, the display devices for HDR images are still limited. To facilitate the visualization of HDR images in standard low dynamic range displays, many different tone mapping operators (TMOs) have been developed. To create a fair comparison of different TMOs, this paper proposes a BLInd QUality Evaluator to blindly predict the quality of Tone-Mapped Images (BLIQUE-TMI) without accessing the corresponding HDR versions. BLIQUE-TMI measures the quality of TMIs by considering the following aspects: 1) visual information; 2) local structure; and 3) naturalness. To be specific, quality-aware features related to the former two aspects are extracted in a local manner \u2026", "total_citations": 55, "citation_graph": {"2018": 2, "2019": 6, "2020": 12, "2021": 13, "2022": 15, "2023": 7}}, {"title": "Blind image quality assessment for stereoscopic images using binocular guided quality lookup and visual codebook", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:dJ_BR67V0s4C", "authors": ["Feng Shao", "Weisi Lin", "Shanshan Wang", "Gangyi Jiang", "Mei Yu"], "publication_date": "2015/3/9", "journal": "IEEE Transactions on Broadcasting", "description": "The field of assessing three-dimensional (3-D) visual experience is challenging. In this paper, we propose a new blind image quality assessment for stereoscopic images by using binocular guided quality lookup and visual codebook. To be more specific, in the training stage, we construct phase-tuned quality lookup (PTQL) and phase-tuned visual codebook (PTVC) from the binocular energy responses based on stimuli from different spatial frequencies, orientations, and phase shifts. In the test stage, blind quality pooling can be easily achieved by searching the PTQL and PTVC, and the quality score is obtained by averaging the largest values of all patch's quality. Experimental results on three 3-D image quality assessment databases demonstrate that in comparison with the most related existing methods, the devised algorithm achieves high consistency alignment with subjective assessment and low-complexity \u2026", "total_citations": 54, "citation_graph": {"2015": 3, "2016": 13, "2017": 9, "2018": 12, "2019": 2, "2020": 5, "2021": 3, "2022": 1, "2023": 5}}, {"title": "Image quality assessment with degradation on spatial structure", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:Yw6v6SrDvuUC", "authors": ["Jinjian Wu", "Weisi Lin", "Guangming Shi"], "publication_date": "2014/2/5", "journal": "IEEE Signal processing letters", "description": "In this letter, we introduce an improved structural degradation based image quality assessment (IQA) method. Most of the existing structural similarity based IQA metrics mainly consider the spatial contrast degradation but have not fully considered the changes on the spatial distribution of structures. Since the human visual system (HVS) is sensitive to degradations on both spatial contrast and spatial distribution, both factors need to be considered for IQA. In order to measure the structural degradation on spatial distribution, the local binary patterns (LBPs) are first employed to extract structural information. And then, the LBP shift between the reference and distorted images is computed, because noise distorts structural patterns. Finally, the spatial contrast degradation on each pair of LBP shifts is calculated for quality assessment. Experimental results on three large benchmark databases confirm that the proposed IQA \u2026", "total_citations": 54, "citation_graph": {"2013": 2, "2014": 2, "2015": 7, "2016": 10, "2017": 9, "2018": 7, "2019": 5, "2020": 1, "2021": 4, "2022": 5, "2023": 2}}, {"title": "A visual attention model combining top-down and bottom-up mechanisms for salient object detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:qjMakFHDy7sC", "authors": ["Yuming Fang", "Weisi Lin", "Chiew Tong Lau", "Bu-Sung Lee"], "publication_date": "2011/5/22", "conference": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "description": "Selective attention in the human visual system is performed as the way that humans focus on the most important parts when observing a visual scene. Many bottom-up computational models of visual attention have been devised to get the saliency map for an image, which are data-driven or task-independent. However, studies show that the task-driven or top-down mechanism also plays an important role during the formation of visual attention, especially with the cases of object detection and location. In this paper, we proposed a new computational visual attention model by combining bottom-up and top-down mechanisms for man-made object detection in scenes. This model shows that the statistical characteristics of orientation features can be used as top-down clues to help for determining the location for salient objects in natural scenes. Experiments confirm the effectiveness of this visual attention model.", "total_citations": 53, "citation_graph": {"2012": 3, "2013": 6, "2014": 9, "2015": 4, "2016": 7, "2017": 3, "2018": 4, "2019": 3, "2020": 6, "2021": 3, "2022": 1, "2023": 3}}, {"title": "Guided image contrast enhancement based on retrieved images in cloud", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:IX653JsL2_EC", "authors": ["Shiqi Wang", "Ke Gu", "Siwei Ma", "Weisi Lin", "Xianming Liu", "Wen Gao"], "publication_date": "2015/12/17", "journal": "IEEE Transactions on Multimedia", "description": "We propose a guided image contrast enhancement framework based on cloud images, in which the context- sensitive and context-free contrast is jointly improved via solving a multi-criteria optimization problem. In particular, the context-sensitive contrast is improved by performing advanced unsharp masking on the input and edge-preserving filtered images, while the context-free contrast enhancement is achieved by the sigmoid transfer mapping. To automatically determine the contrast enhancement level, the parameters in the optimization process are estimated by taking advantages of the retrieved images with similar content. For the purpose of automatically avoiding the involvement of low-quality retrieved images as the guidance, a recently developed no-reference image quality metric is adopted to rank the retrieved images from the cloud. The image complexity from the free-energy-based brain theory and the \u2026", "total_citations": 52, "citation_graph": {"2016": 10, "2017": 6, "2018": 11, "2019": 9, "2020": 8, "2021": 3, "2022": 4, "2023": 1}}]}