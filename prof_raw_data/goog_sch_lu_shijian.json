{"goog_sch_url": "https://scholar.google.com/citations?hl=en&user=uYmK-A0AAAAJ", "name": "Lu Shijian", "interests": ["Image and video analytics", "computer vision", "machine learning"], "co_authors_url": [{"name": "Fangneng Zhan", "url": "https://scholar.google.com/citations?user=8zbcfzAAAAAJ&hl=en", "aff": "Max Planck Institute for Informatics"}, {"name": "Jiaxing Huang", "url": "https://scholar.google.com/citations?user=czirNcwAAAAJ&hl=en", "aff": "Nanyang Technological University"}, {"name": "Xiao Aoran", "url": "https://scholar.google.com/citations?user=yGKsEpAAAAAJ&hl=en", "aff": "School of Computer Science and Engineering, NTU, Singapore"}, {"name": "Yingchen Yu", "url": "https://scholar.google.com/citations?user=0cet0X8AAAAJ&hl=en", "aff": "Nanyang Technological University"}, {"name": "Gongjie Zhang (\u5f20\u529f\u6770)", "url": "https://scholar.google.com/citations?user=sRBTPp4AAAAJ&hl=en", "aff": "Black Sesame Technologies; Nanyang Technological University"}, {"name": "cui kaiwen", "url": "https://scholar.google.com/citations?user=-9KXqLsAAAAJ&hl=en", "aff": "Ph.D. in Nanyang Technological University"}, {"name": "Rongliang WU", "url": "https://scholar.google.com/citations?user=SZkh3iAAAAAJ&hl=en", "aff": "Nanyang Technological University"}, {"name": "Zhipeng Luo", "url": "https://scholar.google.com/citations?user=mw-qVgcAAAAJ&hl=en", "aff": "Nanyang Technological University"}, {"name": "Chuhui Xue", "url": "https://scholar.google.com/citations?user=KJU5YRYAAAAJ&hl=en", "aff": "Research Scientist at ByteDance Singapore"}, {"name": "Jiahui Zhang", "url": "https://scholar.google.com/citations?user=DXpYbWkAAAAJ&hl=en", "aff": "Nanyang Technological University"}, {"name": "Jingyi Zhang", "url": "https://scholar.google.com/citations?user=1qu-Y7sAAAAJ&hl=en", "aff": "Research associate, Nanyang Technological University"}, {"name": "Yun Xing", "url": "https://scholar.google.com/citations?user=uOAYTXoAAAAJ&hl=en", "aff": "School of Computer Science and Engineering, Nanyang Technological University"}, {"name": "Kunhao Liu", "url": "https://scholar.google.com/citations?user=fAc8WqwAAAAJ&hl=en", "aff": "Nanyang Technological University"}], "citation_table": {"columns": ["All", "Since 2018"], "Citations": [14607, 11391], "h-index": [61, 56], "i10-index": [170, 143]}, "citation_graph": {"2009": 76, "2010": 98, "2011": 149, "2012": 210, "2013": 253, "2014": 425, "2015": 543, "2016": 565, "2017": 648, "2018": 784, "2019": 1071, "2020": 1184, "2021": 2282, "2022": 3105, "2023": 2931}, "articles": [{"title": "ICDAR 2015 Competition on Robust Reading", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&pagesize=100&citation_for_view=uYmK-A0AAAAJ:Fu2w8maKXqMC", "authors": ["Dimosthenis Karatzas", "Lluis Gomez-Bigorda", "Anguelos Nicolaou", "Suman Ghosh", "Andrew Bagdanov", "Masakazu Iwamura", "Jiri Matas", "Lukas Neumann", "Vijay Ramaseshan Chandrasekhar", "Shijian Lu", "Faisal Shafait", "Seiichi Uchida", "Ernest Valveny"], "publication_date": "2015", "conference": "12th International Conference on Document Analysis and Recognition", "description": "Results of the ICDAR 2015 Robust Reading Competition are presented. A new Challenge 4 on Incidental Scene Text has been added to the Challenges on Born-Digital Images, Focused Scene Images and Video Text. Challenge 4 is run on a newly acquired dataset of 1,670 images evaluating Text Localisation, Word Recognition and End-to-End pipelines. In addition, the dataset for Challenge 3 on Video Text has been substantially updated with more video sequences and more accurate ground truth data. Finally, tasks assessing End-to-End system performance have been introduced to all Challenges. The competition took place in the first quarter of 2015, and received a total of 44 submissions. Only the tasks newly introduced in 2015 are reported on. The datasets, the ground truth specification and the evaluation protocols are presented together with the results and a brief summary of the participating methods.", "total_citations": 1467, "citation_graph": {"2016": 48, "2017": 73, "2018": 96, "2019": 182, "2020": 209, "2021": 306, "2022": 283, "2023": 236}}, {"title": "Suppressing uncertainties for large-scale facial expression recognition", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&pagesize=100&citation_for_view=uYmK-A0AAAAJ:MhiOAD_qIWkC", "authors": ["Kai Wang", "Xiaojiang Peng", "Jianfei Yang", "Shijian Lu", "Yu Qiao"], "publication_date": "2020", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "description": "Annotating a qualitative large-scale facial expression dataset is extremely difficult due to the uncertainties caused by ambiguous facial expressions, low-quality facial images, and the subjectiveness of annotators. These uncertainties suspend the progress of large-scale Facial Expression Recognition (FER) in data-driven deep learning era. To address this problelm, this paper proposes to suppress the uncertainties by a simple yet efficient Self-Cure Network (SCN). Specifically, SCN suppresses the uncertainty from two different aspects: 1) a self-attention mechanism over FER dataset to weight each sample in training with a ranking regularization, and 2) a careful relabeling mechanism to modify the labels of these samples in the lowest-ranked group. Experiments on synthetic FER datasets and our collected WebEmotion dataset validate the effectiveness of our method. Results on public benchmarks demonstrate that our SCN outperforms current state-of-the-art methods with 88.14% on RAF-DB, 60.23% on AffectNet, and 89.35% on FERPlus.", "total_citations": 468, "citation_graph": {"2020": 13, "2021": 112, "2022": 173, "2023": 170}}, {"title": "Robust document image binarization technique for degraded document images", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&pagesize=100&citation_for_view=uYmK-A0AAAAJ:fPk4N6BV_jEC", "authors": ["Bolan Su", "Shijian Lu", "Chew Lim Tan"], "publication_date": "2012/12/3", "journal": "IEEE transactions on image processing", "description": "Segmentation of text from badly degraded document images is a very challenging task due to the high inter/intra-variation between the document background and the foreground text of different document images. In this paper, we propose a novel document image binarization technique that addresses these issues by using adaptive image contrast. The adaptive image contrast is a combination of the local image contrast and the local image gradient that is tolerant to text and background variation caused by different types of document degradations. In the proposed technique, an adaptive contrast map is first constructed for an input degraded document image. The contrast map is then binarized and combined with Canny's edge map to identify the text stroke edge pixels. The document text is further segmented by a local threshold that is estimated based on the intensities of detected text stroke edge pixels within a \u2026", "total_citations": 378, "citation_graph": {"2011": 1, "2012": 2, "2013": 9, "2014": 34, "2015": 62, "2016": 52, "2017": 37, "2018": 43, "2019": 42, "2020": 23, "2021": 29, "2022": 19, "2023": 19}}, {"title": "ESIR: End-to-End Scene Text Recognition via Iterative Image Rectification", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&pagesize=100&citation_for_view=uYmK-A0AAAAJ:CaZNVDsoPx4C", "authors": ["Fangneng Zhan", "Shijian Lu"], "publication_date": "2019/6/20", "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "description": "Automated recognition of texts in scenes has been a research challenge for years, largely due to the arbitrary text appearance variation in perspective distortion, text line curvature, text styles and different types of imaging artifacts. The recent deep networks are capable of learning robust representations with respect to imaging artifacts and text style changes, but still face various problems while dealing with scene texts with perspective and curvature distortions. This paper presents an end-to-end trainable scene text recognition system (ESIR) that iteratively removes perspective distortion and text line curvature as driven by better scene text recognition performance. An innovative rectification network is developed, where a line-fitting transformation is designed to estimate the pose of text lines in scenes. Additionally, an iterative rectification framework is developed which corrects scene text distortions iteratively towards a fronto-parallel view. The ESIR is also robust to parameter initialization and easy to train, where the training needs only scene text images and word-level annotations as required by most scene text recognition systems. Extensive experiments over a number of public datasets show that the proposed ESIR is capable of rectifying scene text distortions accurately, achieving superior recognition performance for both normal scene text images and those suffering from perspective and curvature distortions.", "total_citations": 377, "citation_graph": {"2019": 18, "2020": 60, "2021": 96, "2022": 113, "2023": 89}}, {"title": "Binarization of historical document images using the local maximum and minimum", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&pagesize=100&citation_for_view=uYmK-A0AAAAJ:9yKSN-GCB0IC", "authors": ["Bolan Su", "Shijian Lu", "Chew Lim Tan"], "publication_date": "2010/6/9", "book": "Proceedings of the 9th IAPR International Workshop on Document Analysis Systems", "description": "This paper presents a new document image binarization technique that segments the text from badly degraded historical document images. The proposed technique makes use of the image contrast that is defined by the local image maximum and minimum. Compared with the image gradient, the image contrast evaluated by the local maximum and minimum has a nice property that it is more tolerant to the uneven illumination and other types of document degradation such as smear. Given a historical document image, the proposed technique first constructs a contrast image and then detects the high contrast image pixels which usually lie around the text stroke boundary. The document text is then segmented by using local thresholds that are estimated from the detected high contrast pixels within a local neighborhood window. The proposed technique has been tested over the dataset that is used in the recent \u2026", "total_citations": 354, "citation_graph": {"2010": 3, "2011": 20, "2012": 21, "2013": 28, "2014": 60, "2015": 49, "2016": 36, "2017": 16, "2018": 23, "2019": 24, "2020": 19, "2021": 21, "2022": 16, "2023": 9}}, {"title": "CAD-Net: A Context-Aware Detection Network for Objects in Remote Sensing Imagery", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&pagesize=100&citation_for_view=uYmK-A0AAAAJ:ubry08Y2EpUC", "authors": ["Gongjie Zhang", "Shijian Lu", "Wei Zhang"], "publication_date": "2019/3/3", "journal": "IEEE Transactions on Geoscience and Remote Sensing", "description": "Accurate and robust detection of multi-class objects in optical remote sensing images is essential to many real-world applications, such as urban planning, traffic control, searching, and rescuing. However, the state-of-the-art object detection techniques designed for images captured using ground-level sensors usually experience a sharp performance drop when directly applied to remote sensing images, largely due to the object appearance differences in remote sensing images in terms of sparse texture, low contrast, arbitrary orientations, and large-scale variations. This paper presents a novel object detection network [(context-aware detection network (CAD-Net)] that exploits attention-modulated features as well as global and local contexts to address the new challenges in detecting objects from remote sensing images. The proposed CAD-Net learns global and local contexts of objects by capturing their \u2026", "total_citations": 314, "citation_graph": {"2019": 2, "2020": 29, "2021": 85, "2022": 112, "2023": 86}}, {"title": "Document image binarization using background estimation and stroke edges", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&pagesize=100&citation_for_view=uYmK-A0AAAAJ:zYLM7Y9cAGgC", "authors": ["Shijian Lu", "Bolan Su", "Chew Lim Tan"], "publication_date": "2010/12", "journal": "International Journal on Document Analysis and Recognition (IJDAR)", "description": "Document images often suffer from different types of degradation that renders the document image binarization a challenging task. This paper presents a document image binarization technique that segments the text from badly degraded document images accurately. The proposed technique is based on the observations that the text documents usually have a document background of the uniform color and texture and the document text within it has a different intensity level compared with the surrounding document background. Given a document image, the proposed technique first estimates a document background surface through an iterative polynomial smoothing procedure. Different types of document degradation are then compensated by using the estimated document background surface. The text stroke edge is further detected from the compensated document image by using L1-norm image \u2026", "total_citations": 314, "citation_graph": {"2009": 2, "2010": 1, "2011": 8, "2012": 19, "2013": 30, "2014": 47, "2015": 44, "2016": 24, "2017": 20, "2018": 21, "2019": 27, "2020": 18, "2021": 15, "2022": 14, "2023": 14}}, {"title": "Beyond Pixels: A Comprehensive Survey from Bottom-up to Semantic Image Segmentation and Cosegmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&pagesize=100&citation_for_view=uYmK-A0AAAAJ:eq2jaN3J8jMC", "authors": ["Hongyuan Zhu", "Fanman Meng", "Jianfei Cai", "Shijian Lu"], "publication_date": "2015/2/3", "journal": "Journal of Visual Communications and Image Representation", "description": "Image segmentation refers to the process to divide an image into meaningful non-overlapping regions according to human perception, which has become a classic topic since the early ages of computer vision. A lot of research has been conducted and has resulted in many applications. While many segmentation algorithms exist, there are only a few sparse and outdated summarizations available. Thus, in this paper, we aim to provide a comprehensive review of the recent progress in the field. Covering 190 publications, we give an overview of broad segmentation topics including not only the classic unsupervised methods, but also the recent weakly-/semi-supervised methods and the fully-supervised methods. In addition, we review the existing influential datasets and evaluation metrics. We also suggest some design choices and research directions for future research in image segmentation.", "total_citations": 288, "citation_graph": {"2015": 3, "2016": 20, "2017": 31, "2018": 51, "2019": 44, "2020": 53, "2021": 36, "2022": 26, "2023": 17}}, {"title": "Ad-cluster: Augmented discriminative clustering for domain adaptive person re-identification", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&pagesize=100&citation_for_view=uYmK-A0AAAAJ:7H_MAutzIkAC", "authors": ["Yunpeng Zhai", "Shijian Lu", "Qixiang Ye", "Xuebo Shan", "Jie Chen", "Rongrong Ji", "Yonghong Tian"], "publication_date": "2020", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "description": "Domain adaptive person re-identification (re-ID) is a challenging task, especially when person identities in target domains are unknown. Existing methods attempt to address this challenge by transferring image styles or aligning feature distributions across domains, whereas the rich unlabeled samples in target domains are not sufficiently exploited. This paper presents a novel augmented discriminative clustering (AD-Cluster) technique that estimates and augments person clusters in target domains and enforces the discrimination ability of re-ID models with the augmented clusters. AD-Cluster is trained by iterative density-based clustering, adaptive sample augmentation, and discriminative feature learning. It learns an image generator and a feature encoder which aim to maximize the intra-cluster diversity in the sample space and minimize the intra-cluster distance in the feature space in an adversarial min-max manner. Finally, AD-Cluster increases the diversity of sample clusters and improves the discrimination capability of re-ID models greatly. Extensive experiments over Market-1501 and DukeMTMC-reID show that AD-Cluster outperforms the state-of-the-art with large margins.", "total_citations": 282, "citation_graph": {"2020": 11, "2021": 90, "2022": 112, "2023": 69}}, {"title": "Spatial Fusion GAN for Image Synthesis", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&pagesize=100&citation_for_view=uYmK-A0AAAAJ:otzGkya1bYkC", "authors": ["Fangneng Zhan", "Hongyuan Zhu", "Shijian Lu"], "publication_date": "2019/6/20", "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "description": "Recent advances in generative adversarial networks (GANs) have shown great potentials in realistic image synthesis whereas most existing works address synthesis realism in either appearance space or geometry space but few in both. This paper presents an innovative Spatial Fusion GAN (SF-GAN) that combines a geometry synthesizer and an appearance synthesizer to achieve synthesis realism in both geometry and appearance spaces. The geometry synthesizer learns contextual geometries of background images and transforms and places foreground objects into the background images unanimously. The appearance synthesizer adjust the color, brightness and styles of the foreground objects and embeds them into background images harmoniously, where a guided filter is incorporated for detail preserving. The two synthesizers are inter-connected as mutual references which can be trained end-to-end with little supervision. The SF-GAN has been evaluated in two tasks:(1) realistic scene text image synthesis for training better recognition models;(2) glass and hat wearing for realistic matching glasses and hats with real portraits. Qualitative and quantitative comparisons with the state-of-the-art demonstrate the superiority of the proposed SF-GAN.", "total_citations": 275, "citation_graph": {"2019": 11, "2020": 28, "2021": 64, "2022": 103, "2023": 68}}]}