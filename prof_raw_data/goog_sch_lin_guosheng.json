{"goog_sch_url": "https://scholar.google.com/citations?hl=en&user=ZudEhvcAAAAJ", "name": "Lin Guosheng", "interests": ["Computer Vision", "Machine Learning"], "co_authors_url": [{"name": "Chunhua Shen", "url": "https://scholar.google.com/citations?hl=en&user=Ljk2BvIAAAAJ", "aff": "Zhejiang University"}, {"name": "Fayao Liu", "url": "https://scholar.google.com/citations?hl=en&user=AxY1-SIAAAAJ", "aff": "Institute for Infocomm Research, A*STAR"}, {"name": "Ian D Reid", "url": "https://scholar.google.com/citations?hl=en&user=ATkNLcQAAAAJ", "aff": "Mohammad Bin Zayed University of Artificial Intelligence and University of Adelaide"}, {"name": "Anton van den Hengel", "url": "https://scholar.google.com/citations?hl=en&user=nMGZ2ZQAAAAJ", "aff": "Director of Applied Science at Amazon and Director of the Centre for Augmented Reasoning\u00a0\u2026"}, {"name": "Qingyao Wu", "url": "https://scholar.google.com/citations?hl=en&user=n6e_2IgAAAAJ", "aff": "School of Software Engineering, South China University of Technology"}, {"name": "Rui Yao", "url": "https://scholar.google.com/citations?hl=en&user=AveCc5QAAAAJ", "aff": "Ph.D"}, {"name": "Anton Milan", "url": "https://scholar.google.com/citations?hl=en&user=LGx06n8AAAAJ", "aff": "Amazon"}, {"name": "Jianfei Cai", "url": "https://scholar.google.com/citations?hl=en&user=N6czCoUAAAAJ", "aff": "Professor of Data Science & AI, Monash University"}, {"name": "Weide Liu", "url": "https://scholar.google.com/citations?hl=en&user=QzsQSD0AAAAJ", "aff": "Nanyang Technological University"}, {"name": "Yukun Su", "url": "https://scholar.google.com/citations?hl=en&user=O00rbxoAAAAJ", "aff": "Nanyang Technological University"}, {"name": "Weisi Lin", "url": "https://scholar.google.com/citations?hl=en&user=D_S41X4AAAAJ", "aff": "Professor, Nanyang Technological Unversity"}, {"name": "Tzu-Yi HUNG", "url": "https://scholar.google.com/citations?hl=en&user=W_GZKeYAAAAJ", "aff": "Delta Research Center, Singapore"}, {"name": "Ruibo Li", "url": "https://scholar.google.com/citations?hl=en&user=qtGY5T4AAAAJ", "aff": "PhD student at the Nanyang Technological University"}, {"name": "Fengmao Lv", "url": "https://scholar.google.com/citations?hl=en&user=tJStjDQAAAAJ", "aff": "School of Computing and Artificial Intelligence, Southwest Jiaotong University"}, {"name": "Zhonghua Wu", "url": "https://scholar.google.com/citations?hl=en&user=wMDgLCYAAAAJ", "aff": "Nanyang Technological University"}, {"name": "WANG Hao", "url": "https://scholar.google.com/citations?hl=en&user=856zi9EAAAAJ", "aff": "Nanyang Technological University"}, {"name": "Zichuan Liu", "url": "https://scholar.google.com/citations?hl=en&user=-H18WY8AAAAJ", "aff": "Nanyang Technological University"}, {"name": "David Suter", "url": "https://scholar.google.com/citations?hl=en&user=moJRxjoAAAAJ", "aff": "Prof. Edith Cowan University"}, {"name": "Professor Javen Qinfeng Shi", "url": "https://scholar.google.com/citations?hl=en&user=h6O9vYkAAAAJ", "aff": "Director for Causal AI Group, University of Adelaide"}, {"name": "Sheng Yang", "url": "https://scholar.google.com/citations?hl=en&user=EgoKticAAAAJ", "aff": "Nanyang Technological University"}, {"name": "Chunyan Miao", "url": "https://scholar.google.com/citations?hl=en&user=fmXGRJgAAAAJ", "aff": "Nanyang Technological University"}, {"name": "Tong Shen", "url": "https://scholar.google.com/citations?hl=en&user=kBnw1ogAAAAJ", "aff": "The University of Adelaide"}, {"name": "Jiacheng Wei", "url": "https://scholar.google.com/citations?hl=en&user=Dnp03EEAAAAJ", "aff": "Ph.D. Candidate, Nanyang Technological University"}, {"name": "Yujun Cai", "url": "https://scholar.google.com/citations?hl=en&user=TE7lbQwAAAAJ", "aff": "Nanyang Technological University"}, {"name": "Chi Zhang", "url": "https://scholar.google.com/citations?hl=en&user=J4s398EAAAAJ", "aff": "Tencent"}, {"name": "Steven CH Hoi", "url": "https://scholar.google.com/citations?hl=en&user=JoLjflYAAAAJ", "aff": "Managing Director of Salesforce Research Asia; IEEE Fellow; Professor at SMU"}, {"name": "Jiashi Feng", "url": "https://scholar.google.com/citations?hl=en&user=Q8iay0gAAAAJ", "aff": "ByteDance Inc."}, {"name": "Alex Kot", "url": "https://scholar.google.com/citations?hl=en&user=UGZXLxIAAAAJ", "aff": "Nanyang Technological University"}, {"name": "Yazhou Yao\uff08\u59da\u4e9a\u6d32\uff09", "url": "https://scholar.google.com/citations?hl=en&user=_3Ucwv4AAAAJ", "aff": "Nanjing University of Science and Technology"}, {"name": "Nan Song", "url": "https://scholar.google.com/citations?hl=en&user=LcAmIHMAAAAJ", "aff": "Nanyang Technological University,"}, {"name": "Lihua Xie", "url": "https://scholar.google.com/citations?hl=en&user=Fmrv3J8AAAAJ", "aff": "Professor of Electrical Engineering, Nanyang Technological University"}, {"name": "Haonan Luo", "url": "https://scholar.google.com/citations?hl=en&user=NZRs4FkAAAAJ", "aff": "Nanjing university of science and technology"}, {"name": "Yap Kim Hui", "url": "https://scholar.google.com/citations?hl=en&user=nr86m98AAAAJ", "aff": "Associate Professor, Nanyang Technological University, Singapore"}, {"name": "Hao Chen", "url": "https://scholar.google.com/citations?hl=en&user=6qVQ7ZMAAAAJ", "aff": "Southeast University"}, {"name": "Qingyi Tao", "url": "https://scholar.google.com/citations?hl=en&user=fMXnSGMAAAAJ", "aff": "Nanyang Technological University"}, {"name": "Jianxin Wu (\u5434\u5efa\u946b)", "url": "https://scholar.google.com/citations?hl=en&user=0JRtCV4AAAAJ", "aff": "Nanjing University"}, {"name": "Xi Li\uff08\u674e\u73ba\uff09", "url": "https://scholar.google.com/citations?hl=en&user=TYNPJQMAAAAJ", "aff": "College of Computer Science, Zhejiang University, China;"}, {"name": "Anthony Dick", "url": "https://scholar.google.com/citations?hl=en&user=Y6wo5UwAAAAJ", "aff": "Computer Science, University of Adelaide"}, {"name": "Henghui Ding", "url": "https://scholar.google.com/citations?hl=en&user=WI_flSwAAAAJ", "aff": "Nanyang Technological University"}, {"name": "Huaxin Xiao", "url": "https://scholar.google.com/citations?hl=en&user=SnEN2NsAAAAJ", "aff": "National University of Defense Technology"}, {"name": "Lingqiao Liu", "url": "https://scholar.google.com/citations?hl=en&user=Y2xu62UAAAAJ", "aff": "Senior Lecturer of the University of Adelaide"}, {"name": "Damith C. Ranasinghe", "url": "https://scholar.google.com/citations?hl=en&user=oGyoeV0AAAAJ", "aff": "The University of Adelaide, University of Cambridge"}, {"name": "Niko Suenderhauf", "url": "https://scholar.google.com/citations?hl=en&user=WnKjfFEAAAAJ", "aff": "Professor at Queensland University of Technology (QUT) Centre for Robotics"}, {"name": "Michael Milford", "url": "https://scholar.google.com/citations?hl=en&user=TDSmCKgAAAAJ", "aff": "QUT Professor | Joint Director, QUT Robotics Centre | ARC Laureate Fellow | Microsoft\u00a0\u2026"}, {"name": "Peter Corke", "url": "https://scholar.google.com/citations?hl=en&user=wnePPc4AAAAJ", "aff": "Queensland University of Technology (QUT)"}, {"name": "Vijay Kumar BG", "url": "https://scholar.google.com/citations?hl=en&user=sKd9ylYAAAAJ", "aff": "Researcher at NEC Labs, America"}, {"name": "Yao Li", "url": "https://scholar.google.com/citations?hl=en&user=q58NtyoAAAAJ", "aff": "Microsoft"}, {"name": "Bohan Zhuang", "url": "https://scholar.google.com/citations?hl=en&user=DFuDBBwAAAAJ", "aff": "Faculty in Computer Science; Co-founder"}], "citation_table": {"columns": ["All", "Since 2018"], "Citations": [13211, 11846], "h-index": [42, 42], "i10-index": [85, 83]}, "citation_graph": {"2014": 33, "2015": 162, "2016": 361, "2017": 680, "2018": 1100, "2019": 1556, "2020": 1797, "2021": 2206, "2022": 2699, "2023": 2463}, "articles": [{"title": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:kNdYIx-mwKoC", "authors": ["Guosheng Lin", "Anton Milan", "Chunhua Shen", "Ian Reid"], "publication_date": "2017", "conference": "IEEE Conference on Computer Vision and Pattern Recognition", "description": "Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense classification problems such as semantic segmentation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular, we achieve an intersection-over-union score of 83.4 on the challenging PASCAL VOC 2012 dataset, which is the best reported result to date.", "total_citations": 3299, "citation_graph": {"2017": 65, "2018": 282, "2019": 530, "2020": 639, "2021": 629, "2022": 647, "2023": 485}}, {"title": "Learning depth from single monocular images using deep convolutional neural fields", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:WF5omc3nYNoC", "authors": ["Fayao Liu", "Chunhua Shen", "Guosheng Lin", "Ian Reid"], "publication_date": "2015/12/3", "journal": "IEEE transactions on pattern analysis and machine intelligence", "description": "In this article, we tackle the problem of depth estimation from single monocular images. Compared with depth estimation using multiple images such as stereo depth perception, depth from monocular images is much more challenging. Prior work typically focuses on exploiting geometric priors or additional sources of information, most using hand-crafted features. Recently, there is mounting evidence that features from deep convolutional neural networks (CNN) set new records for various vision applications. On the other hand, considering the continuous characteristic of the depth values, depth estimation can be naturally formulated as a continuous conditional random field (CRF) learning problem. Therefore, here we present a deep convolutional neural field model for estimating depths from single monocular images, aiming to jointly explore the capacity of deep CNN and continuous CRF. In particular, we propose a \u2026", "total_citations": 1324, "citation_graph": {"2015": 11, "2016": 22, "2017": 75, "2018": 192, "2019": 238, "2020": 228, "2021": 228, "2022": 207, "2023": 109}}, {"title": "Efficient piecewise training of deep structured models for semantic segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:ufrVoPGSRksC", "authors": ["Guosheng Lin", "Chunhua Shen", "Reid Ian", "Anton van dan Hengel"], "publication_date": "2016", "conference": "IEEE Conference on Computer Vision and Pattern Recognition", "description": "Recent advances in semantic image segmentation have mostly been achieved by training deep convolutional neural networks (CNNs). We show how to improve semantic segmentation through the use of contextual information; specifically, we explore'patch-patch'context between image regions, and'patch-background'context. For learning from the patch-patch context, we formulate Conditional Random Fields (CRFs) with CNN-based pairwise potential functions to capture semantic correlations between neighboring patches. Efficient piecewise training of the proposed deep structured model is then applied to avoid repeated expensive CRF inference for back propagation. For capturing the patch-background context, we show that a network design with traditional multi-scale image input and sliding pyramid pooling is effective for improving performance. Our experimental results set new state-of-the-art performance on a number of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow. In particular, we achieve an intersection-over-union score of 78.0 on the challenging PASCAL VOC 2012 dataset.", "total_citations": 1127, "citation_graph": {"2015": 23, "2016": 83, "2017": 160, "2018": 161, "2019": 164, "2020": 158, "2021": 143, "2022": 123, "2023": 78}}, {"title": "Deep convolutional neural fields for depth estimation from a single image", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:Tyk-4Ss8FVUC", "authors": ["Fayao Liu", "Chunhua Shen", "Guosheng Lin"], "publication_date": "2015", "conference": "Proceedings of the IEEE conference on computer vision and pattern recognition", "description": "We consider the problem of depth estimation from a single monocular image in this work. It is a challenging task as no reliable depth cues are available, eg, stereo correspondences, motions etc. Previous efforts have been focusing on exploiting geometric priors or additional sources of information, with all using hand-crafted features. Recently, there is mounting evidence that features from deep convolutional neural networks (CNN) are setting new records for various vision applications. On the other hand, considering the continuous characteristic of the depth values, depth estimations can be naturally formulated into a continuous conditional random field (CRF) learning problem. Therefore, we in this paper present a deep convolutional neural field model for estimating depths from a single image, aiming to jointly explore the capacity of deep CNN and continuous CRF. Specifically, we propose a deep structured learning scheme which learns the unary and pairwise potentials of continuous CRF in a unified deep CNN framework. The proposed method can be used for depth estimations of general scenes with no geometric priors nor any extra information injected. In our case, the integral of the partition function can be analytically calculated, thus we can exactly solve the log-likelihood optimization. Moreover, solving the MAP problem for predicting depths of a new image is highly efficient as closed-form solutions exist. We experimentally demonstrate that the proposed method outperforms state-of-the-art depth estimation methods on both indoor and outdoor scene datasets.", "total_citations": 1061, "citation_graph": {"2015": 20, "2016": 76, "2017": 101, "2018": 168, "2019": 169, "2020": 173, "2021": 139, "2022": 124, "2023": 67}}, {"title": "Deepemd: Few-shot image classification with differentiable earth mover's distance and structured classifiers", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:M3NEmzRMIkIC", "authors": ["Chi Zhang", "Yujun Cai", "Guosheng Lin", "Chunhua Shen"], "publication_date": "2020", "conference": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition", "description": "In this paper, we address the few-shot classification task from a new perspective of optimal matching between image regions. We adopt the Earth Mover's Distance (EMD) as a metric to compute a structural distance between dense image representations to determine image relevance. The EMD generates the optimal matching flows between structural elements that have the minimum matching cost, which is used to represent the image distance for classification. To generate the important weights of elements in the EMD formulation, we design a cross-reference mechanism, which can effectively minimize the impact caused by the cluttered background and large intra-class appearance variations. To handle k-shot classification, we propose to learn a structured fully connected layer that can directly classify dense image representations with the EMD. Based on the implicit function theorem, the EMD can be inserted as a layer into the network for end-to-end training. We conduct comprehensive experiments to validate our algorithm and we set new state-of-the-art performance on four popular few-shot classification benchmarks, namely miniImageNet, tieredImageNet, Fewshot-CIFAR100 (FC100) and Caltech-UCSD Birds-200-2011 (CUB).", "total_citations": 631, "citation_graph": {"2020": 18, "2021": 144, "2022": 238, "2023": 229}}, {"title": "Canet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:Y5dfb0dijaUC", "authors": ["Chi Zhang", "Guosheng Lin", "Fayao Liu", "Rui Yao", "Chunhua Shen"], "publication_date": "2019", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "description": "Recent progress in semantic segmentation is driven by deep Convolutional Neural Networks and large-scale labeled image datasets. However, data labeling for pixel-wise segmentation is tedious and costly. Moreover, a trained model can only make predictions within a set of pre-defined classes. In this paper, we present CANet, a class-agnostic segmentation network that performs few-shot segmentation on new classes with only a few annotated images available. Our network consists of a two-branch dense comparison module which performs multi-level feature comparison between the support image and the query image, and an iterative optimization module which iteratively refines the predicted results. Furthermore, we introduce an attention mechanism to effectively fuse information from multiple support examples under the setting of k-shot learning. Experiments on PASCAL VOC 2012 show that our method achieves a mean Intersection-over-Union score of 55.4% for 1-shot segmentation and 57.1% for 5-shot segmentation, outperforming state-of-the-art methods by a large margin of 14.6% and 13.2%, respectively.", "total_citations": 468, "citation_graph": {"2019": 4, "2020": 49, "2021": 112, "2022": 155, "2023": 147}}, {"title": "Fast supervised hashing with decision trees for high-dimensional data", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:eMMeJKvmdy0C", "authors": ["Guosheng Lin", "Chunhua Shen", "Qinfeng Shi", "Anton Van den Hengel", "David Suter"], "publication_date": "2014", "conference": "Proceedings of the IEEE conference on computer vision and pattern recognition", "description": "Supervised hashing aims to map the original features to compact binary codes that are able to preserve label based similarity in the Hamming space. Non-linear hash functions have demonstrated their advantage over linear ones due to their powerful generalization capability. In the literature, kernel functions are typically used to achieve non-linearity in hashing, which achieve encouraging retrieval performance at the price of slow evaluation and training time. Here we propose to use boosted decision trees for achieving non-linearity in hashing, which are fast to train and evaluate, hence more suitable for hashing with high dimensional data. In our approach, we first propose sub-modular formulations for the hashing binary code inference problem and an efficient GraphCut based block search method for solving large-scale inference. Then we learn hash functions by training boosted decision trees to fit the binary codes. Experiments demonstrate that our proposed method significantly outperforms most state-of-the-art methods in retrieval precision and training time. Especially for high-dimensional data, our method is orders of magnitude faster than many methods in terms of training time.", "total_citations": 466, "citation_graph": {"2014": 5, "2015": 34, "2016": 52, "2017": 89, "2018": 62, "2019": 83, "2020": 43, "2021": 38, "2022": 37, "2023": 16}}, {"title": "Pyramid graph networks with connection attentions for region-based one-shot semantic segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:r0BpntZqJG4C", "authors": ["Chi Zhang", "Guosheng Lin", "Fayao Liu", "Jiushuang Guo", "Qingyao Wu", "Rui Yao"], "publication_date": "2019", "conference": "Proceedings of the IEEE/CVF International Conference on Computer Vision", "description": "One-shot image segmentation aims to undertake the segmentation task of a novel class with only one training image available. The difficulty lies in that image segmentation has structured data representations, which yields a many-to-many message passing problem. Previous methods often simplify it to a one-to-many problem by squeezing support data to a global descriptor. However, a mixed global representation drops the data structure and information of individual elements. In this paper, we propose to model structured segmentation data with graphs and apply attentive graph reasoning to propagate label information from support data to query data. The graph attention mechanism could establish the element-to-element correspondence across structured data by learning attention weights between connected graph nodes. To capture correspondence at different semantic levels, we further propose a pyramid-like structure that models different sizes of image regions as graph nodes and undertakes graph reasoning at different levels. Experiments on PASCAL VOC 2012 dataset demonstrate that our proposed network significantly outperforms the baseline method and leads to new state-of-the-art performance on 1-shot and 5-shot segmentation benchmarks.", "total_citations": 254, "citation_graph": {"2019": 1, "2020": 25, "2021": 72, "2022": 85, "2023": 71}}, {"title": "CRF learning with CNN features for image segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:YsMSGLbcyi4C", "authors": ["Fayao Liu", "Guosheng Lin", "Chunhua Shen"], "publication_date": "2015/10/1", "journal": "Pattern Recognition", "description": "Conditional Random Rields (CRF) have been widely applied in image segmentations. While most studies rely on hand-crafted features, we here propose to exploit a pre-trained large convolutional neural network (CNN) to generate deep features for CRF learning. The deep CNN is trained on the ImageNet dataset and transferred to image segmentations here for constructing potentials of superpixels. Then the CRF parameters are learnt using a structured support vector machine (SSVM). To fully exploit context information in inference, we construct spatially related co-occurrence pairwise potentials and incorporate them into the energy function. This prefers labelling of object pairs that frequently co-occur in a certain spatial layout and at the same time avoids implausible labellings during the inference. Extensive experiments on binary and multi-class segmentation benchmarks demonstrate the promise of the \u2026", "total_citations": 230, "citation_graph": {"2015": 4, "2016": 21, "2017": 29, "2018": 39, "2019": 36, "2020": 44, "2021": 19, "2022": 24, "2023": 12}}, {"title": "A general two-step approach to learning-based hashing", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:d1gkVwhDpl0C", "authors": ["Guosheng Lin", "Chunhua Shen", "David Suter", "Anton Van Den Hengel"], "publication_date": "2013", "conference": "Proceedings of the IEEE international conference on computer vision", "description": "Most existing approaches to hashing apply a single form of hash function, and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to respond to the data, and can result in complex optimization problems that are difficult to solve. Here we propose a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. This framework allows a number of existing approaches to hashing to be placed in context, and simplifies the development of new problemspecific hashing methods. Our framework decomposes the hashing learning problem into two steps: hash bit learning and hash function learning based on the learned bits. The first step can typically be formulated as binary quadratic problems, and the second step can be accomplished by training standard binary classifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate that the proposed framework is effective, flexible and outperforms the state-of-the-art.", "total_citations": 203, "citation_graph": {"2014": 12, "2015": 21, "2016": 32, "2017": 41, "2018": 24, "2019": 26, "2020": 19, "2021": 13, "2022": 9, "2023": 3}}, {"title": "Crnet: Cross-reference networks for few-shot segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:JV2RwH3_ST0C", "authors": ["Weide Liu", "Chi Zhang", "Guosheng Lin", "Fayao Liu"], "publication_date": "2020", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "description": "Over the past few years, state-of-the-art image segmentation algorithms are based on deep convolutional neural networks. To render a deep network with the ability to understand a concept, humans need to collect a large amount of pixel-level annotated data to train the models, which is time-consuming and tedious. Recently, few-shot segmentation is proposed to solve this problem. Few-shot segmentation aims to learn a segmentation model that can be generalized to novel classes with only a few training images. In this paper, we propose a cross-reference network (CRNet) for few-shot segmentation. Unlike previous works which only predict the mask in the query image, our proposed model concurrently makes predictions for both the support image and the query image. With a cross-reference mechanism, our network can better find the co-occurrent objects in two images, thus helping the few-shot segmentation task. We also develop a mask refinement module to recurrently refine the prediction of the foreground regions. For the k-shot learning, we propose to finetune parts of networks to take advantage of multiple labeled support images. Experiments on the PASCAL VOC 2012 dataset show that our network achieves state-of-the-art performance.", "total_citations": 184, "citation_graph": {"2020": 7, "2021": 47, "2022": 68, "2023": 60}}, {"title": "Few-shot incremental learning with continually evolved classifiers", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:4OULZ7Gr8RgC", "authors": ["Chi Zhang", "Nan Song", "Guosheng Lin", "Yun Zheng", "Pan Pan", "Yinghui Xu"], "publication_date": "2021", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "description": "Few-shot class-incremental learning (FSCIL) aims to design machine learning algorithms that can continually learn new concepts from a few data points, without forgetting knowledge of old classes. The difficulty lies in that limited data from new classes not only lead to significant overfitting issues but also exacerbate the notorious catastrophic forgetting problems. Moreover, as training data come in sequence in FSCIL, the learned classifier can only provide discriminative information in individual sessions, while FSCIL requires all classes to be involved for evaluation. In this paper, we address the FSCIL problem from two aspects. First, we adopt a simple but effective decoupled learning strategy of representations and classifiers that only the classifiers are updated in each incremental session, which avoids knowledge forgetting in the representations. By doing so, we demonstrate that a pre-trained backbone plus a non-parametric class mean classifier can beat state-of-the-art methods. Second, to make the classifiers learned on individual sessions applicable to all classes, we propose a Continually Evolved Classifier (CEC) that employs a graph model to propagate context information between classifiers for adaptation. To enable the learning of CEC, we design a pseudo incremental learning paradigm that episodically constructs a pseudo incremental learning task to optimize the graph parameters by sampling data from the base dataset. Experiments on three popular benchmark datasets, including CIFAR100, miniImageNet, and Caltech-USCD Birds-200-2011 (CUB200), show that our method significantly outperforms the baselines and sets new \u2026", "total_citations": 160, "citation_graph": {"2021": 14, "2022": 54, "2023": 92}}, {"title": "Video object segmentation and tracking: A survey", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:4JMBOYKVnBMC", "authors": ["Rui Yao", "Guosheng Lin", "Shixiong Xia", "Jiaqi Zhao", "Yong Zhou"], "publication_date": "2020/5/23", "description": "Object segmentation and object tracking are fundamental research areas in the computer vision community. These two topics are difficult to handle some common challenges, such as occlusion, deformation, motion blur, scale variation, and more. The former contains heterogeneous object, interacting object, edge ambiguity, and shape complexity; the latter suffers from difficulties in handling fast motion, out-of-view, and real-time processing. Combining the two problems of Video Object Segmentation and Tracking (VOST) can overcome their respective difficulties and improve their performance. VOST can be widely applied to many practical applications such as video summarization, high definition video compression, human computer interaction, and autonomous vehicles. This survey aims to provide a comprehensive review of the state-of-the-art VOST methods, classify these methods into different categories, and \u2026", "total_citations": 159, "citation_graph": {"2019": 6, "2020": 18, "2021": 38, "2022": 51, "2023": 45}}, {"title": "Learning hash functions using column generation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:qjMakFHDy7sC", "authors": ["Xi Li", "Guosheng Lin", "Chunhua Shen", "Anton van den Hengel", "Anthony Dick"], "publication_date": "2013/3/2", "journal": "ICML2013", "description": "Fast nearest neighbor searching is becoming an increasingly important tool in solving many large-scale problems. Recently a number of approaches to learning data-dependent hash functions have been developed. In this work, we propose a column generation based method for learning data-dependent hash functions on the basis of proximity comparison information. Given a set of triplets that encode the pairwise proximity comparison information, our method learns hash functions that preserve the relative comparison relationships in the data as well as possible within the large-margin learning framework. The learning procedure is implemented using column generation and hence is named CGHash. At each iteration of the column generation procedure, the best hash function is selected. Unlike most other hashing methods, our method generalizes to new data points naturally; and has a training objective which is convex, thus ensuring that the global optimum can be identified. Experiments demonstrate that the proposed method learns compact binary codes and that its retrieval performance compares favorably with state-of-the-art methods when tested on a few benchmark datasets.", "total_citations": 158, "citation_graph": {"2013": 5, "2014": 11, "2015": 28, "2016": 22, "2017": 20, "2018": 22, "2019": 14, "2020": 20, "2021": 6, "2022": 4, "2023": 2}}, {"title": "Exploring context with deep structured models for semantic segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:hqOjcs7Dif8C", "authors": ["Guosheng Lin", "Chunhua Shen", "Anton van den Hengel", "Ian Reid"], "publication_date": "2017", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "description": "We propose an approach for exploiting contextual information in semantic image segmentation, and particularly investigate the use of patch-patch context and patch-background context in deep CNNs. We formulate deep structured models by combining CNNs and Conditional Random Fields (CRFs) for learning the patch-patch context between image regions. Specifically, we formulate CNN-based pairwise potential functions to capture semantic correlations between neighboring patches. Efficient piecewise training of the proposed deep structured model is then applied in order to avoid repeated expensive CRF inference during the course of back propagation. For capturing the patch-background context, we show that a network design with traditional multi-scale image inputs and sliding pyramid pooling is very effective for improving performance. We perform comprehensive evaluation of the proposed method. We \u2026", "total_citations": 153, "citation_graph": {"2016": 5, "2017": 19, "2018": 29, "2019": 27, "2020": 25, "2021": 21, "2022": 12, "2023": 10}}, {"title": "Fast training of triplet-based deep binary embedding networks", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:5nxA0vEk-isC", "authors": ["Bohan Zhuang", "Guosheng Lin", "Chunhua Shen", "Ian Reid"], "publication_date": "2016", "conference": "Proceedings of the IEEE conference on computer vision and pattern recognition", "description": "In this paper, we aim to learn a mapping (or embedding) from images to a compact binary space in which Hamming distances correspond to a ranking measure for the image retrieval task. We make use of a triplet loss because this has been shown to be most effective for ranking problems. How-ever, training in previous works can be prohibitively expensive due to the fact that optimization is directly performed on the triplet space, where the number of possible triplets for training is cubic in the number of training examples. To address this issue, we propose to formulate high-order binary codes learning as a multi-label classification problem by explicitly separating learning into two interleaved stages. To solve the first stage, we design a large-scale high-order binary codes inference algorithm to reduce the high-order objective to a standard binary quadratic problem such that graph cuts can be used to efficiently infer the binary codes which serve as the labels of each training datum. In the second stage we propose to map the original image to compact binary codes via carefully designed deep convolutional neural networks (CNNs) and the hash-ing function fitting can be solved by training binary CNN classifiers. An incremental/interleaved optimization strategy is proffered to ensure that these two steps are interactive with each other during training for better accuracy. We conduct experiments on several benchmark datasets, which demonstrate both improved training time (by as much as two orders of magnitude) as well as producing state-of-the-art hashing for various retrieval tasks.", "total_citations": 136, "citation_graph": {"2015": 1, "2016": 3, "2017": 17, "2018": 25, "2019": 28, "2020": 21, "2021": 18, "2022": 6, "2023": 12}}, {"title": "Monet: Deep motion exploitation for video object segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:7PzlFSSx8tAC", "authors": ["Huaxin Xiao", "Jiashi Feng", "Guosheng Lin", "Yu Liu", "Maojun Zhang"], "publication_date": "2018", "conference": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "description": "In this paper, we propose a novel MoNet model to deeply exploit motion cues for boosting video object segmentation performance from two aspects, ie, frame representation learning and segmentation refinement. Concretely, MoNet exploits computed motion cue (ie, optical flow) to reinforce the representation of the target frame by aligning and integrating representations from its neighbors. The new representation provides valuable temporal contexts for segmentation and improves robustness to various common contaminating factors, eg, motion blur, appearance variation and deformation of video objects. Moreover, MoNet exploits motion inconsistency and transforms such motion cue into foreground/background prior to eliminate distraction from confusing instances and noisy regions. By introducing a distance transform layer, MoNet can effectively separate motion-inconstant instances/regions and thoroughly refine segmentation results. Integrating the proposed two motion exploitation components with a standard segmentation network, MoNet provides new state-of-the-art performance on three competitive benchmark datasets.", "total_citations": 128, "citation_graph": {"2018": 2, "2019": 25, "2020": 23, "2021": 27, "2022": 26, "2023": 25}}, {"title": "Crowd counting via weighted VLAD on a dense attribute feature map", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:MXK_kJrjxJIC", "authors": ["Biyun Sheng", "Chunhua Shen", "Guosheng Lin", "Jun Li", "Wankou Yang", "Changyin Sun"], "publication_date": "2016/12/8", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "description": "Crowd counting is an important task in computer vision, which has many applications in video surveillance. Although the regression-based framework has achieved great improvements for crowd counting, how to improve the discriminative power of image representation is still an open problem. Conventional holistic features used in crowd counting often fail to capture semantic attributes and spatial cues of the image. In this paper, we propose integrating semantic information into learning locality-aware feature (LAF) sets for accurate crowd counting. First, with the help of a convolutional neural network, the original pixel space is mapped onto a dense attribute feature map, where each dimension of the pixelwise feature indicates the probabilistic strength of a certain semantic class. Then, LAF built on the idea of spatial pyramids on neighboring patches is proposed to explore more spatial context and local information \u2026", "total_citations": 120, "citation_graph": {"2017": 1, "2018": 12, "2019": 27, "2020": 25, "2021": 17, "2022": 22, "2023": 15}}, {"title": "A dilated inception network for visual saliency prediction", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:RHpTSmoSYBkC", "authors": ["Sheng Yang", "Guosheng Lin", "Qiuping Jiang", "Weisi Lin"], "publication_date": "2019/10/14", "journal": "IEEE Transactions on Multimedia", "description": "Recently, with the advent of deep convolutional neural networks (DCNN), the improvements in visual saliency prediction research are impressive. One possible direction to approach the next improvement is to fully characterize the multi-scale saliency-influential factors with a computationally-friendly module in DCNN architectures. In this work, we propose an end-to-end dilated inception network (DINet) for visual saliency prediction. It captures multi-scale contextual features effectively with very limited extra parameters. Instead of utilizing parallel standard convolutions with different kernel sizes as the existing inception module, our proposed dilated inception module (DIM) uses parallel dilated convolutions with different dilation rates which can significantly reduce the computation load while enriching the diversity of receptive fields in feature maps. Moreover, the performance of our saliency model is further improved \u2026", "total_citations": 119, "citation_graph": {"2019": 3, "2020": 12, "2021": 38, "2022": 26, "2023": 40}}, {"title": "Learning markov clustering networks for scene text detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:9ZlFYXVOiuMC", "authors": ["Zichuan Liu", "Guosheng Lin", "Sheng Yang", "Jiashi Feng", "Weisi Lin", "Wang Ling Goh"], "publication_date": "2018/5/22", "conference": "Computer Vision and Pattern Recognition (CVPR) 2018", "description": "A novel framework named Markov Clustering Network (MCN) is proposed for fast and robust scene text detection. MCN predicts instance-level bounding boxes by firstly converting an image into a Stochastic Flow Graph (SFG) and then performing Markov Clustering on this graph. Our method can detect text objects with arbitrary size and orientation without prior knowledge of object size. The stochastic flow graph encode objects' local correlation and semantic information. An object is modeled as strongly connected nodes, which allows flexible bottom-up detection for scale-varying and rotated objects. MCN generates bounding boxes without using Non-Maximum Suppression, and it can be fully parallelized on GPUs. The evaluation on public benchmarks shows that our method outperforms the existing methods by a large margin in detecting multioriented text objects. MCN achieves new state-of-art performance on challenging MSRA-TD500 dataset with precision of 0.88, recall of 0.79 and F-score of 0.83. Also, MCN achieves realtime inference with frame rate of 34 FPS, which is speedup when compared with the fastest scene text detection algorithm.", "total_citations": 119, "citation_graph": {"2018": 4, "2019": 25, "2020": 24, "2021": 28, "2022": 22, "2023": 16}}, {"title": "Multi-path region mining for weakly supervised 3d semantic segmentation on point clouds", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:hMod-77fHWUC", "authors": ["Jiacheng Wei", "Guosheng Lin", "Kim-Hui Yap", "Tzu-Yi Hung", "Lihua Xie"], "publication_date": "2020", "conference": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition", "description": "Point clouds provide intrinsic geometric information and surface context for scene understanding. Existing methods for point cloud segmentation require a large amount of fully labeled data. Using advanced depth sensors, collection of large scale 3D dataset is no longer a cumbersome process. However, manually producing point-level label on the large scale dataset is time and labor-intensive. In this paper, we propose a weakly supervised approach to predict point-level results using weak labels on 3D point clouds. We introduce our multi-path region mining module to generate pseudo point-level labels from a classification network trained with weak labels. It mines the localization cues for each class from various aspects of the network feature using different attention modules. Then, we use the point-level pseudo label to train a point cloud segmentation network in a fully supervised manner. To the best of our knowledge, this is the first method that uses cloud-level weak labels on raw 3D space to train a point cloud semantic segmentation network. In our setting, the 3D weak labels only indicate the classes that appeared in our input sample. We discuss both scene-and subcloud-level weakly labels on raw 3D point cloud data and perform in-depth experiments on them. On ScanNet dataset, our result trained with subcloud-level labels is compatible with some fully supervised methods.", "total_citations": 104, "citation_graph": {"2020": 2, "2021": 21, "2022": 46, "2023": 33}}, {"title": "Efficient dense labelling of human activity sequences from wearables using fully convolutional networks", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:YOwf2qJgpHMC", "authors": ["Rui Yao", "Guosheng Lin", "Qinfeng Shi", "Damith C Ranasinghe"], "publication_date": "2018/6/1", "journal": "Pattern Recognition", "description": "Recognising human activities in sequential data from sensors is a challenging research area. A significant problem arises from the need to determine fixed sequence partitions (windows) to overcome the inability of a single sample to provide adequate information about an activity; commonly overcome by using a fixed size sliding window over consecutive samples to extract information\u2014either handcrafted or learned features\u2014and predicting a single label for all the samples in the window. Two key issues arise from this approach: (i) the samples in one window may not always share the same label, a problem more significant for short duration activities such as gestures. We refer to this as the multi-class windows problem. (ii) the inferencing phase is constrained by the window size selected during training while the best window size is difficult to tune in practice.\nWe propose an efficient method for predicting the label of \u2026", "total_citations": 100, "citation_graph": {"2017": 1, "2018": 12, "2019": 14, "2020": 10, "2021": 21, "2022": 27, "2023": 15}}, {"title": "Supervised hashing using graph cuts and boosted decision trees", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:u-x6o8ySG0sC", "authors": ["Guosheng Lin", "Chunhua Shen", "Anton Van den Hengel"], "publication_date": "2015/2/18", "journal": "IEEE transactions on pattern analysis and machine intelligence", "description": "To build large-scale query-by-example image retrieval systems, embedding image features into a binary Hamming space provides great benefits. Supervised hashing aims to map the original features to compact binary codes that are able to preserve label based similarity in the binary Hamming space. Most existing approaches apply a single form of hash function, and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of those methods, and can result in complex optimization problems that are difficult to solve. In this work we proffer a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. The proposed framework allows a number of existing approaches to hashing to be placed in context, and simplifies the development of new problem-specific hashing methods. Our framework decomposes the \u2026", "total_citations": 96, "citation_graph": {"2015": 2, "2016": 10, "2017": 15, "2018": 14, "2019": 12, "2020": 20, "2021": 11, "2022": 7, "2023": 4}}, {"title": "Towards robust curve text detection with conditional spatial expansion", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:tkaPQYYpVKoC", "authors": ["Zichuan Liu", "Guosheng Lin", "Sheng Yang", "Fayao Liu", "Weisi Lin", "Wang Ling Goh"], "publication_date": "2019", "conference": "proceedings of the IEEE/CVF conference on computer vision and pattern recognition", "description": "It is challenging to detect curve texts due to their irregular shapes and varying sizes. In this paper, we first investigate the deficiency of the existing curve detection methods and then propose a novel Conditional Spatial Expansion (CSE) mechanism to improve the performance of curve detection. Instead of regarding the curve text detection as a polygon regression or a segmentation problem, we formulate it as a sequence prediction on the spatial domain. CSE starts with a seed arbitrarily chosen within a text region and progressively merges neighborhood regions based on the extracted local features by a CNN and contextual information of merged regions. The CSE is highly parameterized and can be seamlessly integrated into existing object detection frameworks. Enhanced by the data-dependent CSE mechanism, our curve text detection system provides robust instance-level text region extraction with minimal post-processing. The analysis experiment shows that our CSE can handle texts with various shapes, sizes, and orientations, and can effectively suppress the false-positives coming from text-like textures or unexpected texts included in the same RoI. Compared with the existing curve text detection algorithms, our method is more robust and enjoys a simpler processing flow. It also creates a new state-of-art performance on curve text benchmarks with F-measurement of up to 78.4%.", "total_citations": 92, "citation_graph": {"2019": 4, "2020": 24, "2021": 24, "2022": 24, "2023": 16}}, {"title": "Learning meta-class memory for few-shot semantic segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:uWQEDVKXjbEC", "authors": ["Zhonghua Wu", "Xiangxi Shi", "Guosheng Lin", "Jianfei Cai"], "publication_date": "2021", "conference": "Proceedings of the IEEE/CVF International Conference on Computer Vision", "description": "Currently, the state-of-the-art methods treat few-shot semantic segmentation task as a conditional foreground-background segmentation problem, assuming each class is independent. In this paper, we introduce the concept of meta-class, which is the meta information (eg certain middle-level features) shareable among all classes. To explicitly learn meta-class representations in few-shot segmentation task, we propose a novel Meta-class Memory based few-shot segmentation method (MM-Net), where we introduce a set of learnable memory embeddings to memorize the meta-class information during the base class training and transfer to novel classes during the inference stage. Moreover, for the k-shot scenario, we propose a novel image quality measurement module to select images from the set of support images. A high-quality class prototype could be obtained with the weighted sum of support image features based on the quality measure. Experiments on both PASCAL-5^ i and COCO datasets show that our proposed method is able to achieve state-of-the-art results in both 1-shot and 5-shot settings. Particularly, our proposed MM-Net achieves 37.5% mIoU on the COCO dataset in 1-shot setting, which is 5.1% higher than the previous state-of-the-art.", "total_citations": 90, "citation_graph": {"2021": 2, "2022": 28, "2023": 60}}, {"title": "Decoupled spatial neural attention for weakly supervised semantic segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:_B80troHkn4C", "authors": ["Tianyi Zhang", "Guosheng Lin", "Jianfei Cai", "Tong Shen", "Chunhua Shen", "Alex C Kot"], "publication_date": "2019/5/3", "journal": "IEEE Transactions on Multimedia", "description": "Weakly supervised semantic segmentation receives much research attention since it alleviates the need to obtain a large amount of dense pixel-wise ground-truth annotations for the training images. Compared with other forms of weak supervision, image labels are quite efficient to obtain. In this paper, we focus on the weakly supervised semantic segmentation with image label annotations. Recent progress for this task has been largely dependent on the quality of generated pseudo-annotations. In this paper, inspired by spatial neural-attention for image captioning, we propose a decoupled spatial neural attention network for generating pseudo-annotations. Our decoupled attention structure could simultaneously identify the object regions and localize the discriminative parts, which generates high-quality pseudo-annotations in one forward path. The generated pseudo-annotations lead to the segmentation results \u2026", "total_citations": 86, "citation_graph": {"2018": 2, "2019": 8, "2020": 20, "2021": 25, "2022": 20, "2023": 11}}, {"title": "Sequence searching with deep-learnt depth for condition-and viewpoint-invariant route-based place recognition", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:roLk4NBRz8UC", "authors": ["Michael Milford", "Chunhua Shen", "Stephanie Lowry", "Niko Suenderhauf", "Sareh Shirazi", "Guosheng Lin", "Fayao Liu", "Edward Pepperell", "Cesar Lerma", "Ben Upcroft", "Ian Reid"], "publication_date": "2015", "conference": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops", "description": "Vision-based localization on robots and vehicles remains unsolved when extreme appearance change and viewpoint change are present simultaneously. The current state of the art approaches to this challenge either deal with only one of these two problems; for example FAB-MAP (viewpoint invariance) or SeqSLAM (appearance-invariance), or use extensive training within the test environment, an impractical requirement in many application scenarios. In this paper we significantly improve the viewpoint invariance of the SeqSLAM algorithm by using state-of-the-art deep learning techniques to generate synthetic viewpoints. Our approach is different to other deep learning approaches in that it does not rely on the ability of the CNN network to learn invariant features, but only to produce\" good enough\" depth images from day-time imagery only. We evaluate the system on a new multi-lane day-night car dataset specifically gathered to simultaneously test both appearance and viewpoint change. Results demonstrate that the use of synthetic viewpoints improves the maximum recall achieved at 100% precision by a factor of 2.2 and maximum recall by a factor of 2.7, enabling correct place recognition across multiple road lanes and significantly reducing the time between correct localizations", "total_citations": 86, "citation_graph": {"2015": 3, "2016": 9, "2017": 12, "2018": 7, "2019": 12, "2020": 15, "2021": 11, "2022": 8, "2023": 7}}, {"title": "Cross-domain semantic segmentation via domain-invariant interactive relation transfer", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:BqipwSGYUEgC", "authors": ["Fengmao Lv", "Tao Liang", "Xiang Chen", "Guosheng Lin"], "publication_date": "2020", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "description": "Exploiting photo-realistic synthetic data to train semantic segmentation models has received increasing attention over the past years. However, the domain mismatch between synthetic and real images will cause a significant performance drop when the model trained with synthetic images is directly applied to real-world scenarios. In this paper, we propose a new domain adaptation approach, called Pivot Interaction Transfer (PIT). Our method mainly focuses on constructing pivot information that is common knowledge shared across domains as a bridge to promote the adaptation of semantic segmentation model from synthetic domains to real-world domains. Specifically, we first infer the image-level category information about the target images, which is then utilized to facilitate pixel-level transfer for semantic segmentation, with the assumption that the interactive relation between the image-level category information and the pixel-level semantic information is invariant across domains. To this end, we propose a novel multi-level region expansion mechanism that aligns both the image-level and pixel-level information. Comprehensive experiments on the adaptation from both GTAV and SYNTHIA to Cityscapes clearly demonstrate the superiority of our method.", "total_citations": 81, "citation_graph": {"2019": 1, "2020": 1, "2021": 28, "2022": 29, "2023": 22}}, {"title": "Context decoupling augmentation for weakly supervised semantic segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:u_35RYKgDlwC", "authors": ["Yukun Su", "Ruizhou Sun", "Guosheng Lin", "Qingyao Wu"], "publication_date": "2021", "conference": "Proceedings of the IEEE/CVF international conference on computer vision", "description": "Data augmentation is vital for deep learning neural networks. By providing massive training samples, it helps to improve the generalization ability of the model. Weakly supervised semantic segmentation (WSSS) is a challenging problem that has been deeply studied in recent years, conventional data augmentation approaches for WSSS usually employ geometrical transformations, random cropping, and color jittering. However, merely increasing the same contextual semantic data does not bring much gain to the networks to distinguish the objects, eg, the correct image-level classification of\" aeroplane\" may be not only due to the recognition of the object itself but also its co-occurrence context like\" sky\", which will cause the model to focus less on the object features. To this end, we present a Context Decoupling Augmentation (CDA) method, to change the inherent context in which the objects appear and thus drive the network to remove the dependence between object instances and contextual information. To validate the effectiveness of the proposed method, extensive experiments on PASCAL VOC 2012 dataset with several alternative network architectures demonstrate that CDA can boost various popular WSSS methods to the new state-of-the-art by a large margin.", "total_citations": 78, "citation_graph": {"2021": 4, "2022": 31, "2023": 42}}, {"title": "RGBD salient object detection via disentangled cross-modal fusion", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:J_g5lzvAfSwC", "authors": ["Hao Chen", "Yongjian Deng", "Youfu Li", "Tzu-Yi Hung", "Guosheng Lin"], "publication_date": "2020/8/12", "journal": "IEEE Transactions on Image Processing", "description": "Depth is beneficial for salient object detection (SOD) for its additional saliency cues. Existing RGBD SOD methods focus on tailoring complicated cross-modal fusion topologies, which although achieve encouraging performance, are with a high risk of over-fitting and ambiguous in studying cross-modal complementarity. Different from these conventional approaches combining cross-modal features entirely without differentiating, we concentrate our attention on decoupling the diverse cross-modal complements to simplify the fusion process and enhance the fusion sufficiency. We argue that if cross-modal heterogeneous representations can be disentangled explicitly, the cross-modal fusion process can hold less uncertainty, while enjoying better adaptability. To this end, we design a disentangled cross-modal fusion network to expose structural and content representations from both modalities by cross-modal \u2026", "total_citations": 78, "citation_graph": {"2019": 1, "2020": 1, "2021": 17, "2022": 28, "2023": 31}}, {"title": "Spsequencenet: Semantic segmentation network on 4d point clouds", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:NMxIlDl6LWMC", "authors": ["Hanyu Shi", "Guosheng Lin", "Hao Wang", "Tzu-Yi Hung", "Zhenhua Wang"], "publication_date": "2020", "conference": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition", "description": "Point clouds are useful in many applications like autonomous driving and robotics as they provide natural 3D information of the surrounding environments. While there are extensive research on 3D point clouds, scene understanding on 4D point clouds, a series of consecutive 3D point clouds frames, is an emerging topic and yet under-investigated. With 4D point clouds (3D point cloud videos), robotic systems could enhance their robustness by leveraging the temporal information from previous frames. However, the existing semantic segmentation methods on 4D point clouds suffer from low precision due to the spatial and temporal information loss in their network structures. In this paper, we propose SpSequenceNet to address this problem. The network is designed based on 3D sparse convolution. And we introduce two novel modules, a cross-frame global attention module and a cross-frame local interpolation module, to capture spatial and temporal information in 4D point clouds. We conduct extensive experiments on SemanticKITTI, and achieve the state-of-the-art result of 43.1% on mIoU, which is 1.5% higher than the previous best approach.", "total_citations": 77, "citation_graph": {"2020": 1, "2021": 18, "2022": 34, "2023": 23}}, {"title": "Deeply learning the messages in message passing inference", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:PELIpwtuRlgC", "authors": ["Guosheng Lin", "Chunhua Shen", "Ian Reid", "Anton van den Hengel"], "publication_date": "2015", "journal": "Advances in Neural Information Processing Systems", "description": "Deep structured output learning shows great promise in tasks like semantic image segmentation. We proffer a new, efficient deep structured model learning scheme, in which we show how deep Convolutional Neural Networks (CNNs) can be used to directly estimate the messages in message passing inference for structured prediction with Conditional Random Fields CRFs). With such CNN message estimators, we obviate the need to learn or evaluate potential functions for message calculation. This confers significant efficiency for learning, since otherwise when performing structured learning for a CRF with CNN potentials it is necessary to undertake expensive inference for every stochastic gradient iteration. The network output dimension of message estimators is the same as the number of classes, rather than exponentially growing in the order of the potentials. Hence it is more scalable for cases that a large number of classes are involved. We apply our method to semantic image segmentation and achieve impressive performance, which demonstrates the effectiveness and usefulness of our CNN message learning method.", "total_citations": 77, "citation_graph": {"2015": 4, "2016": 17, "2017": 10, "2018": 8, "2019": 17, "2020": 6, "2021": 7, "2022": 5, "2023": 3}}, {"title": "Bootstrapping the performance of webly supervised semantic segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:QIV2ME_5wuYC", "authors": ["Tong Shen", "Guosheng Lin", "Chunhua Shen", "Ian Reid"], "publication_date": "2018", "conference": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "description": "Fully supervised methods for semantic segmentation require pixel-level class masks to train, the creation of which are expensive in terms of manual labour and time. In this work, we focus on weak supervision, developing a method for training a high-quality pixel-level classifier for semantic segmentation, using only image-level class labels as the provided ground-truth. Our method is formulated as a two-stage approach in which we first aim to create accurate pixel-level masks for the training images via a bootstrapping process, and then use these now-accurately segmented images as a proxy ground-truth in a more standard supervised setting. The key driver for our work is that in the target dataset we typically have reliable ground-truth image-level labels, while data crawled from the web may have unreliable labels, but can be filtered to comprise only easy images to segment, therefore having reliable boundaries. These two forms of information are complementary and we use this observation to build a novel bi-directional transfer learning. This framework transfers knowledge between two domains, target domain and web domain, bootstrapping the performance of weakly supervised semantic segmentation. Conducting experiments on the popular benchmark dataset PASCAL VOC 2012 based on both a VGG16 network and on ResNet50, we reach state-of-the-art performance with scores of 60.2% IoU and 63.9% IoU respectively.", "total_citations": 74, "citation_graph": {"2019": 12, "2020": 19, "2021": 17, "2022": 17, "2023": 9}}, {"title": "M2e-try on net: Fashion from model to everyone", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:hC7cP41nSMkC", "authors": ["Zhonghua Wu", "Guosheng Lin", "Qingyi Tao", "Jianfei Cai"], "publication_date": "2019/10/15", "book": "Proceedings of the 27th ACM international conference on multimedia", "description": "Most existing virtual try-on applications require clean clothes images. Instead, we present a novel virtual Try-On network, M2E-Try On Net, which transfers the clothes from a model image to a person image without the need of any clean product images. To obtain a realistic image of person wearing the desired model clothes, we aim to solve the following challenges: 1) non-rigid nature of clothes - we need to align poses between the model and the user; 2) richness in textures of fashion items - preserving the fine details and characteristics of the clothes is critical for photo-realistic transfer; 3) variation of identity appearances - it is required to fit the desired model clothes to the person identity seamlessly. To tackle these challenges, we introduce three key components, including the pose alignment network (PAN), the texture refinement network (TRN) and the fitting network (FTN). Since it is unlikely to gather image pairs \u2026", "total_citations": 73, "citation_graph": {"2019": 9, "2020": 15, "2021": 25, "2022": 11, "2023": 12}}, {"title": "Splitting vs. merging: Mining object regions with discrepancy and intersection loss for weakly supervised semantic segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:ns9cj8rnVeAC", "authors": ["Tianyi Zhang", "Guosheng Lin", "Weide Liu", "Jianfei Cai", "Alex Kot"], "publication_date": "2020", "conference": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXII 16", "description": "In this paper we focus on the task of weakly-supervised semantic segmentation supervised with image-level labels. Since the pixel-level annotation is not available in the training process, we rely on region mining models to estimate the pseudo-masks from the image-level labels. Thus, in order to improve the final segmentation results, we aim to train a region-mining model which could accurately and completely highlight the target object regions for generating high-quality pseudo-masks. However, the region mining models are likely to only highlight the most discriminative regions instead of the entire objects. In this paper, we aim to tackle this problem from a novel perspective of optimization process. We propose a Splitting vs. Merging optimization strategy, which is mainly composed of the Discrepancy loss and the Intersection loss. The proposed Discrepancy loss aims at mining out regions of different \u2026", "total_citations": 68, "citation_graph": {"2019": 1, "2020": 2, "2021": 17, "2022": 29, "2023": 19}}, {"title": "Semantic segmentation from limited training data", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:aqlVkmm33-oC", "authors": ["A Milan", "T Pham", "K Vijay", "D Morrison", "AW Tow", "L Liu", "J Erskine", "R Grinover", "A Gurman", "T Hunn", "N Kelly-Boxall", "D Lee", "M McTaggart", "G Rallos", "A Razjigaev", "T Rowntree", "T Shen", "R Smith", "S Wade-McCue", "Z Zhuang", "C Lehnert", "G Lin", "I Reid", "P Corke", "J Leitner"], "publication_date": "2017/9/22", "conference": "IEEE International Conference on Robotics and Automation (ICRA) 2018", "description": "We present our approach for robotic perception in cluttered scenes that led to winning the recent Amazon Robotics Challenge (ARC) 2017. Next to small objects with shiny and transparent surfaces, the biggest challenge of the 2017 competition was the introduction of unseen categories. In contrast to traditional approaches which require large collections of annotated data and many hours of training, the task here was to obtain a robust perception pipeline with only few minutes of data acquisition and training time. To that end, we present two strategies that we explored. One is a deep metric learning approach that works in three separate steps: semantic-agnostic boundary detection, patch classification and pixel-wise voting. The other is a fully-supervised semantic segmentation approach with efficient dataset collection. We conduct an extensive analysis of the two methods on our ARC 2017 dataset. Interestingly \u2026", "total_citations": 61, "citation_graph": {"2017": 3, "2018": 9, "2019": 17, "2020": 10, "2021": 11, "2022": 7, "2023": 3}}, {"title": "RefineNet: Multi-path refinement networks for dense prediction", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:_Re3VWB3Y0AC", "authors": ["Guosheng Lin", "Fayao Liu", "Anton Milan", "Chunhua Shen", "Ian Reid"], "publication_date": "2020", "journal": "IEEE transactions on pattern analysis and machine intelligence", "description": "Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense prediction problems such as semantic segmentation and depth estimation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments on semantic segmentation which is a dense classification problem and achieve good performance on seven public datasets. We further apply our method for depth estimation and demonstrate the effectiveness of our method on dense regression problems.", "total_citations": 58, "citation_graph": {"2019": 2, "2020": 6, "2021": 13, "2022": 22, "2023": 15}}, {"title": "Deepemd: Differentiable earth mover's distance for few-shot learning", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:rO6llkc54NcC", "authors": ["Chi Zhang", "Yujun Cai", "Guosheng Lin", "Chunhua Shen"], "publication_date": "2022/10/26", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "description": "In this work, we develop methods for few-shot image classification from a new perspective of optimal matching between image regions. We employ the Earth Mover's Distance (EMD) as a metric to compute a structural distance between dense image representations to determine image relevance. The EMD generates the optimal matching flows between structural elements that have the minimum matching cost, which is used to calculate the image distance for classification. To generate the important weights of elements in the EMD formulation, we design a cross-reference mechanism, which can effectively alleviate the adverse impact caused by the cluttered background and large intra-class appearance variations. To implement -shot classification, we propose to learn a structured fully connected layer that can directly classify dense image representations with the EMD. Based on the implicit function theorem, the \u2026", "total_citations": 55, "citation_graph": {"2021": 12, "2022": 15, "2023": 27}}, {"title": "Self-supervised 3d skeleton action representation learning with motion consistency and continuity", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:p2g8aNsByqUC", "authors": ["Yukun Su", "Guosheng Lin", "Qingyao Wu"], "publication_date": "2021", "conference": "Proceedings of the IEEE/CVF international conference on computer vision", "description": "Recently, self-supervised learning (SSL) has been proved very effective and it can help boost the performance in learning representations from unlabeled data in the image domain. Yet, very little is explored about its usefulness in 3D skeleton-based action recognition understanding. Directly applying existing SSL techniques for 3D skeleton learning, however, suffers from trivial solutions and imprecise representations. To tackle these drawbacks, we consider perceiving the consistency and continuity of motion at different playback speeds are two critical issues. To this end, we propose a novel SSL method to learn the 3D skeleton representation in an efficacious way. Specifically, by constructing a positive clip (speed-changed) and a negative clip (motion-broken) of the sampled action sequence, we encourage the positive pairs closer while pushing the negative pairs to force the network to learn the intrinsic dynamic motion consistency information. Moreover, to enhance the learning features, skeleton interpolation is further exploited to model the continuity of human skeleton data. To validate the effectiveness of the proposed method, extensive experiments are conducted on Kinetics, NTU60, NTU120, and PKUMMD datasets with several alternative network architectures. Experimental evaluations demonstrate the superiority of our approach and through which, we can gain significant performance improvement without using extra labeled data.", "total_citations": 51, "citation_graph": {"2021": 2, "2022": 20, "2023": 28}}, {"title": "Progressive modality reinforcement for human multimodal emotion recognition from unaligned multimodal sequences", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:D03iK_w7-QYC", "authors": ["Fengmao Lv", "Xiang Chen", "Yanyong Huang", "Lixin Duan", "Guosheng Lin"], "publication_date": "2021", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "description": "Human multimodal emotion recognition involves time-series data of different modalities, such as natural language, visual motions, and acoustic behaviors. Due to the variable sampling rates for sequences from different modalities, the collected multimodal streams are usually unaligned. The asynchrony across modalities increases the difficulty on conducting efficient multimodal fusion. Hence, this work mainly focuses on multimodal fusion from unaligned multimodal sequences. To this end, we propose the Progressive Modality Reinforcement (PMR) approach based on the recent advances of crossmodal transformer. Our approach introduces a message hub to exchange information with each modality. The message hub sends common messages to each modality and reinforces their features via crossmodal attention. In turn, it also collects the reinforced features from each modality and uses them to generate a reinforced common message. By repeating the cycle process, the common message and the modalities' features can progressively complement each other. Finally, the reinforced features are used to make predictions for human emotion. Comprehensive experiments on different human multimodal emotion recognition benchmarks clearly demonstrate the superiority of our approach.", "total_citations": 47, "citation_graph": {"2021": 3, "2022": 18, "2023": 24}}, {"title": "Human interaction learning on 3d skeleton point clouds for video violence recognition", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:vV6vV6tmYwMC", "authors": ["Yukun Su", "Guosheng Lin", "Jinhui Zhu", "Qingyao Wu"], "publication_date": "2020", "conference": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part IV 16", "description": "This paper introduces a new method for recognizing violent behavior by learning contextual relationships between related people from human skeleton points. Unlike previous work, we first formulate 3D skeleton point clouds from human skeleton sequences extracted from videos and then perform interaction learning on these 3D skeleton point clouds. A novel Skeleton Points Interaction Learning (SPIL) module, is proposed to model the interactions between skeleton points. Specifically, by constructing a specific weight distribution strategy between local regional points, SPIL aims to selectively focus on the most relevant parts of them based on their features and spatial-temporal position information. In order to capture diverse types of relation information, a multi-head mechanism is designed to aggregate different features from independent heads to jointly handle different types of relationships between \u2026", "total_citations": 45, "citation_graph": {"2019": 1, "2020": 0, "2021": 9, "2022": 13, "2023": 22}}, {"title": "HCRF-Flow: Scene flow from point clouds with continuous high-order CRFs and position-aware flow embedding", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:a0OBvERweLwC", "authors": ["Ruibo Li", "Guosheng Lin", "Tong He", "Fayao Liu", "Chunhua Shen"], "publication_date": "2021", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "description": "Scene flow in 3D point clouds plays an important role in understanding dynamic environments. Although significant advances have been made by deep neural networks, the performance is far from satisfactory as only per-point translational motion is considered, neglecting the constraints of the rigid motion in local regions. To address the issue, we propose to introduce the motion consistency to force the smoothness among neighboring points. In addition, constraints on the rigidity of the local transformation are also added by sharing unique rigid motion parameters for all points within each local region. To this end, a high-order CRFs based relation module (Con-HCRFs) is deployed to explore both point-wise smoothness and region-wise rigidity. To empower the CRFs to have a discriminative unary term, we also introduce a position-aware flow estimation module to be incorporated into the Con-HCRFs. Comprehensive experiments on FlyingThings3D and KITTI show that our proposed framework (HCRF-Flow) achieves state-of-the-art performance and significantly outperforms previous approaches substantially.", "total_citations": 42, "citation_graph": {"2021": 1, "2022": 18, "2023": 23}}, {"title": "A unified transformer framework for group-based segmentation: Co-segmentation, co-saliency detection and video salient object detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:tzM49s52ZIMC", "authors": ["Yukun Su", "Jingliang Deng", "Ruizhou Sun", "Guosheng Lin", "Hanjing Su", "Qingyao Wu"], "publication_date": "2023/4/5", "journal": "IEEE Transactions on Multimedia", "description": "Humans tend to mine objects by learning from a group of images or several frames of video since we live in a dynamic world. In the computer vision area, many researchers focus on co-segmentation (CoS), co-saliency detection (CoSD) and video salient object detection (VSOD) to discover the co-occurrent objects. However, previous approaches design different networks for these similar tasks separately, and they are difficult to apply to each other. Besides, they fail to take full advantage of the cues among inter- and intra-feature within a group of images. In this paper, we introduce a unified framework to tackle these issues from a unified view, term as UFGS (Unified Framework for Group-based Segmentation). Specifically, we first introduce a transformer block, which views the image feature as a patch token and then captures their long-range dependencies through the self-attention mechanism. This can help the \u2026", "total_citations": 40, "citation_graph": {"2022": 9, "2023": 30}}, {"title": "Guided co-segmentation network for fast video object segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:O3NaXMp0MMsC", "authors": ["Weide Liu", "Guosheng Lin", "Tianyi Zhang", "Zichuan Liu"], "publication_date": "2020/7/20", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "description": "Semi-supervised video object segmentation is a task of propagating instance masks given in the first frame to the entire video. It is a challenging task since it usually suffers from heavy occlusions, large deformation, and large variations of objects. To alleviate these problems, many existing works apply time-consuming techniques such as fine-tuning, post-processing, or extracting optical flow, which makes them intractable for online segmentation. In our work, we focus on online semi-supervised video object segmentation. We propose a GCSeg (Guided Co-Segmentation) Network which is mainly composed of a Reference Module and a Co-segmentation Module, to simultaneously incorporate the short-term, middle-term, and long-term temporal inter-frame relationships. Moreover, we propose an Adaptive Search Strategy to reduce the risk of propagating inaccurate segmentation results in subsequent frames. Our \u2026", "total_citations": 40, "citation_graph": {"2020": 2, "2021": 6, "2022": 13, "2023": 18}}, {"title": "On lightweight privacy-preserving collaborative learning for internet-of-things objects", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:JoZmwDi-zQgC", "authors": ["Linshan Jiang", "Rui Tan", "Xin Lou", "Guosheng Lin"], "publication_date": "2019/4/15", "book": "Proceedings of the international conference on internet of things design and implementation", "description": "The Internet of Things (IoT) will be a main data generation infrastructure for achieving better system intelligence. This paper considers the design and implementation of a practical privacy-preserving collaborative learning scheme, in which a curious learning coordinator trains a better machine learning model based on the data samples contributed by a number of IoT objects, while the confidentiality of the raw forms of the training data is protected against the coordinator. Existing distributed machine learning and data encryption approaches incur significant computation and communication overhead, rendering them ill-suited for resource-constrained IoT objects. We study an approach that applies independent Gaussian random projection at each IoT object to obfuscate data and trains a deep neural network at the coordinator based on the projected data from the IoT objects. This approach introduces light \u2026", "total_citations": 40, "citation_graph": {"2019": 2, "2020": 13, "2021": 10, "2022": 11, "2023": 4}}, {"title": "Self-point-flow: Self-supervised scene flow estimation from point clouds with optimal transport and random walk", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:yD5IFk8b50cC", "authors": ["Ruibo Li", "Guosheng Lin", "Lihua Xie"], "publication_date": "2021", "conference": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition", "description": "Due to the scarcity of annotated scene flow data, self-supervised scene flow learning in point clouds has attracted increasing attention. In the self-supervised manner, establishing correspondences between two point clouds to approximate scene flow is an effective approach. Previous methods often obtain correspondences by applying point-wise matching that only takes the distance on 3D point coordinates into account, introducing two critical issues:(1) it overlooks other discriminative measures, such as color and surface normal, which often bring fruitful clues for accurate matching; and (2) it often generates sub-par performance, as the matching is operated in an unconstrained situation, where multiple points can be ended up with the same corresponding point. To address the issues, we formulate this matching task as an optimal transport problem. The output optimal assignment matrix can be utilized to guide the generation of pseudo ground truth. In this optimal transport, we design the transport cost by considering multiple descriptors and encourage one-to-one matching by mass equality constraints. Also, constructing a graph on the points, a random walk module is introduced to encourage the local consistency of the pseudo labels. Comprehensive experiments on FlyingThings3D and KITTI show that our method achieves state-of-the-art performance among self-supervised learning methods. Our self-supervised method even performs on par with some supervised learning approaches, although we do not need any ground truth flow for training.", "total_citations": 37, "citation_graph": {"2021": 1, "2022": 15, "2023": 21}}, {"title": "Semantics-aware visual object tracking", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:ZeXyd9-uunAC", "authors": ["Rui Yao", "Guosheng Lin", "Chunhua Shen", "Yanning Zhang", "Qinfeng Shi"], "publication_date": "2018/6/18", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "description": "In this paper, we propose a semantics-aware visual object tracking method, which introduces semantics into the tracking procedure and extends the model of an object with explicit semantics prior to enhancing the robustness of three key aspects of the tracking framework, i.e., appearance model, search scheme, and scale adaptation. We first present a semantic object proposal generation method for video sequences to generate high-quality category-oriented object proposals. Then, a hybrid semantics-aware tracking algorithm with semantic compatibility is proposed. This algorithm takes full advantages of globally sparse semantic object proposal prediction and locally dense prediction with a template model and semantic distractor-aware color appearance model. Furthermore, we propose to exploit semantics to localize object accurately via an energy minimization framework-based scale adaptation method, which \u2026", "total_citations": 35, "citation_graph": {"2018": 2, "2019": 2, "2020": 10, "2021": 7, "2022": 8, "2023": 6}}, {"title": "Cycle-consistent inverse GAN for text-to-image synthesis", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:b0M2c_1WBrUC", "authors": ["Hao Wang", "Guosheng Lin", "Steven CH Hoi", "Chunyan Miao"], "publication_date": "2021/10/17", "book": "Proceedings of the 29th ACM International Conference on Multimedia", "description": "This paper investigates an open research task of text-to-image synthesis for automatically generating or manipulating images from text descriptions. Prevailing methods mainly take the textual descriptions as the conditional input for the GAN generation, and need to train different models for the text-guided image generation and manipulation tasks. In this paper, we propose a novel unified framework of Cycle-consistent Inverse GAN (CI-GAN) for both text-to-image generation and text-guided image manipulation tasks. Specifically, we first train a GAN model without text input, aiming to generate images with high diversity and quality. Then we learn a GAN inversion model to convert the images back to the GAN latent space and obtain the inverted latent codes for each image, where we introduce the cycle-consistency training to learn more robust and consistent inverted latent codes. We further uncover the semantics of \u2026", "total_citations": 34, "citation_graph": {"2021": 1, "2022": 18, "2023": 15}}, {"title": "Weakly supervised segmentation with maximum bipartite graph matching", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:ldfaerwXgEUC", "authors": ["Weide Liu", "Chi Zhang", "Guosheng Lin", "Tzu-Yi Hung", "Chunyan Miao"], "publication_date": "2020/10/12", "book": "Proceedings of the 28th ACM International Conference on Multimedia", "description": "In the weakly supervised segmentation task with only image-level labels, a common step in many existing algorithms is first to locate the image regions corresponding to each existing class with the Class Activation Maps (CAMs), and then generate the pseudo ground truth masks based on the CAMs to train a segmentation network in the fully supervised manner. The quality of the CAMs has a crucial impact on the performance of the segmentation model. We propose to improve the CAMs from a novel graph perspective. We model paired images containing common classes with a bipartite graph and use the maximum matching algorithm to locate corresponding areas in two images. The matching areas are then used to refine the predicted object regions in the CAMs. The experiments on Pascal VOC 2012 dataset show that our network can effectively boost the performance of the baseline model and achieves new \u2026", "total_citations": 33, "citation_graph": {"2021": 11, "2022": 12, "2023": 10}}, {"title": "Progressive self-guided loss for salient object detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:3s1wT3WcHBgC", "authors": ["Sheng Yang", "Weisi Lin", "Guosheng Lin", "Qiuping Jiang", "Zichuan Liu"], "publication_date": "2021/10/4", "journal": "IEEE Transactions on Image Processing", "description": "We present a simple yet effective progressive self-guided loss function to facilitate deep learning-based salient object detection (SOD) in images. The saliency maps produced by the most relevant works still suffer from incomplete predictions due to the internal complexity of salient objects. Our proposed progressive self-guided loss simulates a morphological closing operation on the model predictions for progressively creating auxiliary training supervisions to step-wisely guide the training process. We demonstrate that this new loss function can guide the SOD model to highlight more complete salient objects step-by-step and meanwhile help to uncover the spatial dependencies of the salient object pixels in a region growing manner. Moreover, a new feature aggregation module is proposed to capture multi-scale features and aggregate them adaptively by a branch-wise attention mechanism. Benefiting from this \u2026", "total_citations": 32, "citation_graph": {"2022": 16, "2023": 16}}, {"title": "Sequential person recognition in photo albums with a recurrent network", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:dTyEYWd-f8wC", "authors": ["Yao Li", "Guosheng Lin", "Bohan Zhuang", "Lingqiao Liu", "Chunhua Shen", "Anton van den Hengel"], "publication_date": "2017", "conference": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "description": "Recognizing the identities of people in everyday photos is still a very challenging problem for machine vision, due to issues such as non-frontal faces, changes in clothing, location, lighting. Recent studies have shown that rich relational information between people in the same photo can help in recognizing their identities. In this work, we propose to model the relational information between people as a sequence prediction task. At the core of our work is a novel recurrent network architecture, in which relational information between instances' labels and appearance are modeled jointly. In addition to relational cues, scene context is incorporated in our sequence prediction model with no additional cost. In this sense, our approach is a unified framework for modeling both contextual cues and visual appearance of person instances. Our model is trained end-to-end with a sequence of annotated instances in a photo as inputs, and a sequence of corresponding labels as targets. We demonstrate that this simple but elegant formulation achieves state-of-the-art performance on the newly released People In Photo Albums (PIPA) dataset.", "total_citations": 32, "citation_graph": {"2017": 5, "2018": 5, "2019": 9, "2020": 7, "2021": 2, "2022": 2, "2023": 1}}, {"title": "Few-shot segmentation with optimal transport matching and message flow", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:nb7KW1ujOQ8C", "authors": ["Weide Liu", "Chi Zhang", "Henghui Ding", "Tzu-Yi Hung", "Guosheng Lin"], "publication_date": "2022", "journal": "IEEE Transactions on Multimedia", "description": "We tackle the challenging task of few-shot segmentation in this work. It is essential for few-shot semantic segmentation to fully utilize the support information. Previous methods typically adopt masked average pooling over the support feature to extract the support clues as a global vector, usually dominated by the salient part and lost certain essential clues. In this work, we argue that every support pixel's information is desired to be transferred to all query pixels and propose a Correspondence Matching Network (CMNet) with an Optimal Transport Matching module to mine out the correspondence between the query and support images. Besides, it is critical to fully utilize both local and global information from the annotated support images. To this end, we propose a Message Flow module to propagate the message along the inner-flow inside the same image and cross-flow between support and query images, which greatly helps enhance the local feature representations. Experiments on PASCAL VOC 2012, MS COCO, and FSS-1000 datasets show that our network achieves new state-of-the-art few-shot segmentation performance.", "total_citations": 31, "citation_graph": {"2021": 4, "2022": 8, "2023": 18}}, {"title": "Meta navigator: Search for a good adaptation policy for few-shot learning", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:dshw04ExmUIC", "authors": ["Chi Zhang", "Henghui Ding", "Guosheng Lin", "Ruibo Li", "Changhu Wang", "Chunhua Shen"], "publication_date": "2021", "conference": "Proceedings of the IEEE/CVF International Conference on Computer Vision", "description": "Few-shot learning aims to adapt knowledge learned from previous tasks to novel tasks with only a limited amount of labeled data. Research literature on few-shot learning exhibits great diversity, while different algorithms often excel at different few-shot learning scenarios. It is therefore tricky to decide which learning strategies to use under different task conditions. Inspired by the recent success in Automated Machine Learning literature (AutoML), in this paper, we present Meta Navigator, a framework that attempts to solve the aforementioned limitation in few-shot learning by seeking a higher-level strategy and proffer to automate the selection from various few-shot learning designs. The goal of our work is to search for good parameter adaptation policies that are applied to different stages in the network for few-shot classification. We present a search space that covers many popular few-shot learning algorithms in the literature and develop a differentiable searching and decoding algorithm based on meta-learning that supports gradient-based optimization. We demonstrate the effectiveness of our searching-based method on multiple benchmark datasets. Extensive experiments show that our approach significantly outperforms baselines and demonstrates performance advantages over many state-of-the-art methods. Code and models will be made publicly available.", "total_citations": 31, "citation_graph": {"2021": 1, "2022": 13, "2023": 17}}, {"title": "Optimizing ranking measures for compact binary code learning", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:2osOgNQ5qMEC", "authors": ["Guosheng Lin", "Chunhua Shen", "Jianxin Wu"], "publication_date": "2014", "conference": "Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part III 13", "description": "Hashing has proven a valuable tool for large-scale information retrieval. Despite much success, existing hashing methods optimize over simple objectives such as the reconstruction error or graph Laplacian related loss functions, instead of the performance evaluation criteria of interest\u2014multivariate performance measures such as the AUC and NDCG. Here we present a general framework (termed StructHash) that allows one to directly optimize multivariate performance measures. The resulting optimization problem can involve exponentially or infinitely many variables and constraints, which is more challenging than standard structured output learning. To solve the StructHash optimization problem, we use a combination of column generation and cutting-plane techniques. We demonstrate the generality of StructHash by applying it to ranking prediction and image retrieval, and show that it outperforms a few \u2026", "total_citations": 30, "citation_graph": {"2015": 7, "2016": 4, "2017": 6, "2018": 1, "2019": 4, "2020": 4, "2021": 0, "2022": 1, "2023": 1}}, {"title": "Segeqa: Video segmentation based visual attention for embodied question answering", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:iH-uZ7U-co4C", "authors": ["Haonan Luo", "Guosheng Lin", "Zichuan Liu", "Fayao Liu", "Zhenmin Tang", "Yazhou Yao"], "publication_date": "2019/10/27", "conference": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "description": "Embodied Question Answering (EQA) is a newly defined research area where an agent is required to answer the user's questions by exploring the real world environment. It has attracted increasing research interests due to its broad applications in automatic driving system, in-home robots, and personal assistants. Most of the existing methods perform poorly in terms of answering and navigation accuracy due to the absence of local details and vulnerability to the ambiguity caused by complicated vision conditions. To tackle these problems, we propose a segmentation based visual attention mechanism for Embodied Question Answering. Firstly, We extract the local semantic features by introducing a novel high-speed video segmentation framework. Then by the guide of extracted semantic features, a bottom-up visual attention mechanism is proposed for the Visual Question Answering (VQA) sub-task. Further, a \u2026", "total_citations": 27, "citation_graph": {"2020": 14, "2021": 8, "2022": 3, "2023": 2}}, {"title": "Iterative refinement for multi-source visual domain adaptation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:35N4QoGY0k4C", "authors": ["Hanrui Wu", "Yuguang Yan", "Guosheng Lin", "Min Yang", "Michael K Ng", "Qingyao Wu"], "publication_date": "2020/8/6", "journal": "IEEE Transactions on Knowledge and Data Engineering", "description": "One of the main challenges in multi-source domain adaptation is how to reduce the domain discrepancy between each source domain and a target domain, and then evaluate the domain relevance to determine how much knowledge should be transferred from different source domains to the target domain. However, most prior approaches barely consider both discrepancies and relevance among domains. In this paper, we propose an algorithm, called Iterative Refinement based on Feature Selection and the Wasserstein distance (IRFSW), to solve semi-supervised domain adaptation with multiple sources. Specifically, IRFSW aims to explore both the discrepancies and relevance among domains in an iterative learning procedure, which gradually refines the learning performance until the algorithm stops. In each iteration, for each source domain and the target domain, we develop a sparse model to select features \u2026", "total_citations": 26, "citation_graph": {"2021": 6, "2022": 9, "2023": 11}}, {"title": "Structure-aware generation network for recipe generation from images", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:70eg2SAEIzsC", "authors": ["Hao Wang", "Guosheng Lin", "Steven CH Hoi", "Chunyan Miao"], "publication_date": "2020", "conference": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXVII 16", "description": "Sharing food has become very popular with the development of social media. For many real-world applications, people are keen to know the underlying recipes of a food item. In this paper, we are interested in automatically generating cooking instructions for food. We investigate an open research task of generating cooking instructions based on only food images and ingredients, which is similar to the image captioning task. However, compared with image captioning datasets, the target recipes are long-length paragraphs and do not have annotations on structure information. To address the above limitations, we propose a novel framework of Structure-aware Generation Network (SGN) to tackle the food recipe generation task. Our approach brings together several novel ideas in a systematic framework: (1) exploiting an unsupervised learning approach to obtain the sentence-level tree structure labels \u2026", "total_citations": 26, "citation_graph": {"2020": 2, "2021": 9, "2022": 9, "2023": 5}}, {"title": "RigidFlow: Self-Supervised Scene Flow Learning on Point Clouds by Local Rigidity Prior", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:LPZeul_q3PIC", "authors": ["Ruibo Li", "Chi Zhang", "Guosheng Lin", "Zhe Wang", "Chunhua Shen"], "publication_date": "2022", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "description": "In this work, we focus on scene flow learning on point clouds in a self-supervised manner. A real-world scene can be well modeled as a collection of rigidly moving parts, therefore its scene flow can be represented as a combination of rigid motion of each part. Inspired by this observation, we propose to generate pseudo scene flow for self-supervised learning based on piecewise rigid motion estimation, in which the source point cloud is decomposed into a set of local regions and each region is treated as rigid. By rigidly aligning each region with its potential counterpart in the target point cloud, we obtain a region-specific rigid transformation to represent the flow, which together constitutes the pseudo scene flow labels of the entire scene to enable network training. Compared with most existing approaches relying on point-wise similarities for point matching, our method explicitly enforces region-wise rigid alignments, yielding locally rigid pseudo scene flow labels. We demonstrate the effectiveness of our self-supervised learning method on FlyingThings3D and KITTI datasets. Comprehensive experiments show that our method achieves new state-of-the-art performance in self-supervised scene flow learning, without any ground truth scene flow for supervision, even outperforming some supervised counterparts.", "total_citations": 24, "citation_graph": {"2022": 3, "2023": 21}}, {"title": "Structured Learning of Tree Potentials in CRF for Image Segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:_kc_bZDykSQC", "authors": ["Fayao Liu", "Guosheng Lin", "Ruizhi Qiao", "Chunhua Shen"], "publication_date": "2017", "journal": "IEEE Transactions on Neural Networks and Learning Systems", "description": "We propose a new approach to image segmentation, which exploits the advantages of both conditional random fields (CRFs) and decision trees. In the literature, the potential functions of CRFs are mostly defined as a linear combination of some predefined parametric models, and then, methods, such as structured support vector machines, are applied to learn those linear coefficients. We instead formulate the unary and pairwise potentials as nonparametric forests-ensembles of decision trees, and learn the ensemble parameters and the trees in a unified optimization problem within the large-margin framework. In this fashion, we easily achieve nonlinear learning of potential functions on both unary and pairwise terms in CRFs. Moreover, we learn classwise decision trees for each object that appears in the image. Experimental results on several public segmentation data sets demonstrate the power of the learned \u2026", "total_citations": 23, "citation_graph": {"2017": 2, "2018": 4, "2019": 4, "2020": 3, "2021": 5, "2022": 3, "2023": 2}}, {"title": "Trrnet: Tiered relation reasoning for compositional visual question answering", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:lSLTfruPkqcC", "authors": ["Xiaofeng Yang", "Guosheng Lin", "Fengmao Lv", "Fayao Liu"], "publication_date": "2020", "conference": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXI 16", "description": "Compositional visual question answering requires reasoning over both semantic and geometry object relations. We propose a novel tiered reasoning method that dynamically selects object level candidates based on language representations and generates robust pairwise relations within the selected candidate objects. The proposed tiered relation reasoning method can be compatible with the majority of the existing visual reasoning frameworks, leading to significant performance improvement with very little extra computational cost. Moreover, we propose a policy network that decides the appropriate reasoning steps based on question complexity and current reasoning status. In experiments, our model achieves state-of-the-art performance on two VQA datasets.", "total_citations": 22, "citation_graph": {"2020": 1, "2021": 3, "2022": 10, "2023": 8}}, {"title": "CNN-Based RGB-D Salient Object Detection: Learn, Select, and Fuse", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:dfsIfKJdRG4C", "authors": ["Hao Chen", "Youfu Li", "Yongjian Deng", "Guosheng Lin"], "publication_date": "2021/7", "journal": "International Journal of Computer Vision", "description": "The goal of this work is to present a systematic solution for RGB-D salient object detection, which addresses the following three aspects with a unified framework: modal-specific representation learning, complementary cue selection, and cross-modal complement fusion. To learn discriminative modal-specific features, we propose a hierarchical cross-modal distillation scheme, in which we use the progressive predictions from the well-learned source modality to supervise learning feature hierarchies and inference in the new modality. To better select complementary cues, we formulate a residual function to incorporate complements from the paired modality adaptively. Furthermore, a top-down fusion structure is constructed for sufficient cross-modal cross-level interactions. The experimental results demonstrate the effectiveness of the proposed cross-modal distillation scheme in learning from a new modality, the \u2026", "total_citations": 21, "citation_graph": {"2021": 7, "2022": 2, "2023": 11}}, {"title": "Attention is not Enough: Mitigating the Distribution Discrepancy in Asynchronous Multimodal Sequence Fusion", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:XiSMed-E-HIC", "authors": ["Tao Liang", "Guosheng Lin", "Lei Feng", "Yan Zhang", "Fengmao Lv"], "publication_date": "2021", "conference": "Proceedings of the IEEE/CVF International Conference on Computer Vision", "description": "Videos flow as the mixture of language, acoustic, and vision modalities. A thorough video understanding needs to fuse time-series data of different modalities for prediction. Due to the variable receiving frequency for sequences from each modality, there usually exists inherent asynchrony across the collected multimodal streams. Towards an efficient multimodal fusion from asynchronous multimodal streams, we need to model the correlations between elements from different modalities. The recent Multimodal Transformer (MulT) approach extends the self-attention mechanism of the original Transformer network to learn the crossmodal dependencies between elements. However, the direct replication of self-attention will suffer from the distribution mismatch across different modality features. As a result, the learnt crossmodal dependencies can be unreliable. Motivated by this observation, this work proposes the Modality-Invariant Crossmodal Attention (MICA) approach towards learning crossmodal interactions over modality-invariant space in which the distribution mismatch between different modalities is well bridged. To this end, both the marginal distribution and the elements with high-confidence correlations are aligned over the common space of the query and key vectors which are computed from different modalities. Experiments on three standard benchmarks of multimodal video understanding clearly validate the superiority of our approach.", "total_citations": 21, "citation_graph": {"2022": 9, "2023": 11}}, {"title": "Discriminative Training of Deep Fully-connected Continuous CRF with Task-specific Loss", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:0EnyYjriUFMC", "authors": ["Fayao Liu", "Guosheng Lin", "Chunhua Shen"], "publication_date": "2017", "journal": "IEEE Transactions on Image Processing", "description": "Recent works on deep conditional random fields (CRFs) have set new records on many vision tasks involving structured predictions. Here, we propose a fully connected deep continuous CRF model with task-specific losses for both discrete and continuous labeling problems. We exemplify the usefulness of the proposed model on multi-class semantic labeling (discrete) and the robust depth estimation (continuous) problems. In our framework, we model both the unary and the pairwise potential functions as deep convolutional neural networks (CNNs), which are jointly learned in an end-to-end fashion. The proposed method possesses the main advantage of continuously valued CRFs, which is a closed-form solution for the maximum a posteriori (MAP) inference. To better take into account the quality of the predicted estimates during the cause of learning, instead of using the commonly employed maximum likelihood \u2026", "total_citations": 21, "citation_graph": {"2016": 1, "2017": 2, "2018": 3, "2019": 3, "2020": 2, "2021": 1, "2022": 2, "2023": 7}}, {"title": "Modeling the uncertainty for self-supervised 3d skeleton action representation learning", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:Tiz5es2fbqcC", "authors": ["Yukun Su", "Guosheng Lin", "Ruizhou Sun", "Yun Hao", "Qingyao Wu"], "publication_date": "2021/10/17", "book": "Proceedings of the 29th ACM International Conference on Multimedia", "description": "Self-supervised learning (SSL) has been proved very effective in learning representations from unlabeled data in language and vision domains. Yet, very few instrumental self-supervised approaches exist for 3D skeleton action understanding, and directly applying the existing SSL methods from other domains for skeleton action learning may suffer from misalignment of representations and some limitations. In this paper, we consider that a good representation learning encoder can distinguish the underlying features of different actions, which can make the similar motions closer while pushing the dissimilar motions away. There exists, however, some uncertainties in the skeleton actions due to the inherent ambiguity of 3D skeleton pose in different viewpoints or the sampling algorithm in contrastive learning, thus, it is ill-posed to differentiate the action features in the deterministic embedding space. To address these \u2026", "total_citations": 20, "citation_graph": {"2021": 2, "2022": 8, "2023": 9}}, {"title": "Graph edit distance reward: Learning to edit scene graph", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:RGFaLdJalmkC", "authors": ["Lichang Chen", "Guosheng Lin", "Shijie Wang", "Qingyao Wu"], "publication_date": "2020", "conference": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XIX 16", "description": "Scene Graph, as a vital tool to bridge the gap between language domain and image domain, has been widely adopted in the cross-modality task like VQA. In this paper, we propose a new method to edit the scene graph according to the user instructions, which has never been explored. To be specific, in order to learn editing scene graphs as the semantics given by texts, we propose a Graph Edit Distance Reward, which is based on the Policy Gradient and Graph Matching algorithm, to optimize neural symbolic model. In the context of text-editing image retrieval, we validate the effectiveness of our method in CSS and CRIR dataset. Besides, CRIR is a new synthetic dataset generated by us, which we will publish soon for future use.", "total_citations": 19, "citation_graph": {"2021": 6, "2022": 9, "2023": 4}}, {"title": "StructBoost: Boosting Methods for Predicting Structured Output Variables", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:9yKSN-GCB0IC", "authors": ["Chunhua Shen", "Guosheng Lin", "Anton van den Hengel"], "publication_date": "2013/2/14", "journal": "IEEE transactions on pattern analysis and machine intelligence", "description": "Boosting is a method for learning a single accurate predictor by linearly combining a set of less accurate weak learners. Recently, structured learning has found many applications in computer vision. Inspired by structured support vector machines (SSVM), here we propose a new boosting algorithm for structured output prediction, which we refer to as StructBoost. StructBoost supports nonlinear structured learning by combining a set of weak structured learners. As SSVM generalizes SVM, our StructBoost generalizes standard boosting approaches such as AdaBoost, or LPBoost to structured learning. The resulting optimization problem of StructBoost is more challenging than SSVM in the sense that it may involve exponentially many variables and constraints. In contrast, for SSVM one usually has an exponential number of constraints and a cutting-plane method is used. In order to efficiently solve StructBoost, we \u2026", "total_citations": 18, "citation_graph": {"2014": 3, "2015": 4, "2016": 3, "2017": 4, "2018": 2, "2019": 1, "2020": 0, "2021": 1}}, {"title": "Cyclesegnet: Object co-segmentation with cycle refinement and region correspondence", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:ZHo1McVdvXMC", "authors": ["Chi Zhang", "Guankai Li", "Guosheng Lin", "Qingyao Wu", "Rui Yao"], "publication_date": "2021/6/14", "journal": "IEEE Transactions on Image Processing", "description": "Image co-segmentation is an active computer vision task that aims to segment the common objects from a set of images. Recently, researchers design various learning-based algorithms to undertake the co-segmentation task. The main difficulty in this task is how to effectively transfer information between images to make conditional predictions. In this paper, we present CycleSegNet, a novel framework for the co-segmentation task. Our network design has two key components: a region correspondence module which is the basic operation for exchanging information between local image regions, and a cycle refinement module, which utilizes ConvLSTMs to progressively update image representations and exchange information in a cycle and iterative manner. Extensive experiments demonstrate that our proposed method significantly outperforms the state-of-the-art methods on four popular benchmark datasets \u2026", "total_citations": 16, "citation_graph": {"2021": 2, "2022": 8, "2023": 6}}, {"title": "3D Pose Transfer with Correspondence Learning and Mesh Refinement", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:UxriW0iASnsC", "authors": ["Chaoyue Song", "Jiacheng Wei", "Ruibo Li", "Fayao Liu", "Guosheng Lin"], "publication_date": "2021/12/6", "journal": "Advances in Neural Information Processing Systems", "description": "3D pose transfer is one of the most challenging 3D generation tasks. It aims to transfer the pose of a source mesh to a target mesh and keep the identity (eg, body shape) of the target mesh. Some previous works require key point annotations to build reliable correspondence between the source and target meshes, while other methods do not consider any shape correspondence between sources and targets, which leads to limited generation quality. In this work, we propose a correspondence-refinement network to achieve the 3D pose transfer for both human and animal meshes. The correspondence between source and target meshes is first established by solving an optimal transport problem. Then, we warp the source mesh according to the dense correspondence and obtain a coarse warped mesh. The warped mesh will be better refined with our proposed Elastic Instance Normalization, which is a conditional normalization layer and can help to generate high-quality meshes. Extensive experimental results show that the proposed architecture can effectively transfer the poses from source to target meshes and produce better results with satisfied visual performance than state-of-the-art methods.", "total_citations": 14, "citation_graph": {"2022": 5, "2023": 9}}, {"title": "Keypoint based weakly supervised human parsing", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:IWHjjKOFINEC", "authors": ["Zhonghua Wu", "Guosheng Lin", "Jianfei Cai"], "publication_date": "2019/11/1", "journal": "Image and Vision Computing", "description": "Fully convolutional networks (FCN) have achieved great success in human parsing in recent years. In conventional human parsing tasks, pixel-level labeling is required for guiding the training, which usually involves enormous human labeling efforts. To ease the labeling efforts, we propose a novel weakly supervised human parsing method which only requires simple object keypoint annotations for learning. We develop an iterative learning method to generate pseudo part segmentation masks from keypoint labels. With these pseudo masks, we train a FCN network to output pixel-level human parsing predictions. Furthermore, we develop a correlation network to perform joint prediction of part and object segmentation masks and improve the segmentation performance. The experiment results show that our weakly supervised method is able to achieve very competitive human parsing results. Despite that our method \u2026", "total_citations": 14, "citation_graph": {"2019": 2, "2020": 3, "2021": 5, "2022": 4}}, {"title": "Weakly Supervised Semantic Segmentation Based on Web Image Co-segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:L8Ckcad2t8MC", "authors": ["Tong Shen", "Guosheng Lin", "Lingqiao Liu", "Chunhua Shen", "Ian Reid"], "publication_date": "2017/5/25", "conference": "The British Machine Vision Conference (BMVC)", "description": "Training a Fully Convolutional Network (FCN) for semantic segmentation requires a large number of masks with pixel level labelling, which involves a large amount of human labour and time for annotation. In contrast, web images and their image-level labels are much easier and cheaper to obtain. In this work, we propose a novel method for weakly supervised semantic segmentation with only image-level labels. The method utilizes the internet to retrieve a large number of images and uses a large scale co-segmentation framework to generate masks for the retrieved images. We first retrieve images from search engines, e.g. Flickr and Google, using semantic class names as queries, e.g. class names in the dataset PASCAL VOC 2012. We then use high quality masks produced by co-segmentation on the retrieved images as well as the target dataset images with image level labels to train segmentation networks. We obtain an IoU score of 56.9 on test set of PASCAL VOC 2012, which reaches the state-of-the-art performance.", "total_citations": 14, "citation_graph": {"2018": 1, "2019": 3, "2020": 5, "2021": 2, "2022": 2, "2023": 1}}, {"title": "Self-supervised object localization with joint graph partition", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:geHnlv5EZngC", "authors": ["Yukun Su", "Guosheng Lin", "Yun Hao", "Yiwen Cao", "Wenjun Wang", "Qingyao Wu"], "publication_date": "2022/6/28", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "description": "Object localization aims to generate a tight bounding box for the target object, which is a challenging problem that has been deeply studied in recent years. Since collecting bounding-box labels is time-consuming and laborious, many researchers focus on weakly supervised object localization (WSOL). As the recent appealing self-supervised learning technique shows its powerful function in visual tasks, in this paper, we take the early attempt to explore unsupervised object localization by self-supervision. Specifically, we adopt different geometric transformations to image and utilize their parameters as pseudo labels for self-supervised learning. Then, the class-agnostic activation map (CAAM) is used to highlight the target object potential regions. However, such attention maps merely focus on the most discriminative part of the objects, which will affect the quality of the predicted bounding box. Based on the motivation that the activation maps of different transformations of the same image should be equivariant, we further design a siamese network that encodes the paired images and propose a joint graph cluster partition mechanism in an unsupervised manner to enhance the object co-occurrent regions. To validate the effectiveness of the proposed method, extensive experiments are conducted on CUB-200-2011, Stanford Cars and FGVC-Aircraft datasets. Experimental results show that our method outperforms state-of-the-art methods using the same level of supervision, even outperforms some weakly-supervised methods.", "total_citations": 13, "citation_graph": {"2022": 3, "2023": 10}}, {"title": "Weakly Supervised Segmentation on Outdoor 4D Point Clouds With Temporal Matching and Spatial Graph Propagation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:5Ul4iDaHHb8C", "authors": ["Hanyu Shi", "Jiacheng Wei", "Ruibo Li", "Fayao Liu", "Guosheng Lin"], "publication_date": "2022", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "description": "Existing point cloud segmentation methods require a large amount of annotated data, especially for the outdoor point cloud scene. Due to the complexity of the outdoor 3D scenes, manual annotations on the outdoor point cloud scene are time-consuming and expensive. In this paper, we study how to achieve scene understanding with limited annotated data. Treating 100 consecutive frames as a sequence, we divide the whole dataset into a series of sequences and annotate only 0.1% points in the first frame of each sequence to reduce the annotation requirements. This leads to a total annotation budget of 0.001%. We propose a novel temporal-spatial framework for effective weakly supervised learning to generate high-quality pseudo labels from these limited annotated data. Specifically, the framework contains two modules: an matching module in temporal dimension to propagate pseudo labels across different frames, and a graph propagation module in spatial dimension to propagate the information of pseudo labels to the entire point clouds in each frame. With only 0.001% annotations for training, experimental results on both SemanticKITTI and SemanticPOSS shows our weakly supervised two-stage framework is comparable to some existing fully supervised methods. We also evaluate our framework with 0.005% initial annotations on SemanticKITTI, and achieve a result close to fully supervised backbone model.", "total_citations": 13, "citation_graph": {"2022": 2, "2023": 11}}, {"title": "On lightweight privacy-preserving collaborative learning for internet of things by independent random projections", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:HoB7MX3m0LUC", "authors": ["Linshan Jiang", "Rui Tan", "Xin Lou", "Guosheng Lin"], "publication_date": "2021/3/27", "journal": "ACM Transactions on Internet of Things", "description": "The Internet of Things (IoT) will be a main data generation infrastructure for achieving better system intelligence. This article considers the design and implementation of a practical privacy-preserving collaborative learning scheme, in which a curious learning coordinator trains a better machine learning model based on the data samples contributed by a number of IoT objects, while the confidentiality of the raw forms of the training data is protected against the coordinator. Existing distributed machine learning and data encryption approaches incur significant computation and communication overhead, rendering them ill-suited for resource-constrained IoT objects. We study an approach that applies independent random projection at each IoT object to obfuscate data and trains a deep neural network at the coordinator based on the projected data from the IoT objects. This approach introduces light computation \u2026", "total_citations": 13, "citation_graph": {"2022": 6, "2023": 7}}, {"title": "Efficient few-shot object detection via knowledge inheritance", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:sSrBHYA8nusC", "authors": ["Ze Yang", "Chi Zhang", "Ruibo Li", "Yi Xu", "Guosheng Lin"], "publication_date": "2022/12/14", "journal": "IEEE Transactions on Image Processing", "description": "Few-shot object detection (FSOD), which aims at learning a generic detector that can adapt to unseen tasks with scarce training samples, has witnessed consistent improvement recently. However, most existing methods ignore the efficiency issues, e.g., high computational complexity and slow adaptation speed. Notably, efficiency has become an increasingly important evaluation metric for few-shot techniques due to an emerging trend toward embedded AI. To this end, we present an efficient pretrain-transfer framework (PTF) baseline with no computational increment, which achieves comparable results with previous state-of-the-art (SOTA) methods. Upon this baseline, we devise an initializer named knowledge inheritance (KI) to reliably initialize the novel weights for the box classifier, which effectively facilitates the knowledge transfer process and boosts the adaptation speed. Within the KI initializer, we propose an \u2026", "total_citations": 12, "citation_graph": {"2022": 1, "2023": 11}}, {"title": "Dual adaptive transformations for weakly supervised point cloud segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:VOx2b1Wkg3QC", "authors": ["Zhonghua Wu", "Yicheng Wu", "Guosheng Lin", "Jianfei Cai", "Chen Qian"], "publication_date": "2022/10/23", "book": "European Conference on Computer Vision", "description": "Weakly supervised point cloud segmentation, i.e. semantically segmenting a point cloud with only a few labeled points in the whole 3D scene, is highly desirable due to the heavy burden of collecting abundant dense annotations for the model training. However, existing methods remain challenging to accurately segment 3D point clouds since limited annotated data may lead to insufficient guidance for label propagation to unlabeled data. Considering the smoothness-based methods have achieved promising progress, in this paper, we advocate applying the consistency constraint under various perturbations to effectively regularize unlabeled 3D points. Specifically, we propose a novel DAT (Dual Adaptive Transformations) model for weakly supervised point cloud segmentation, where the dual adaptive transformations are performed via an adversarial strategy at both point-level and region-level, aiming at enforcing \u2026", "total_citations": 12, "citation_graph": {"2022": 2, "2023": 10}}, {"title": "Dense Semantics-Assisted Networks for Video Action Recognition", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:EUQCXRtRnyEC", "authors": ["Haonan Luo", "Guosheng Lin", "Yazhou Yao", "Zhenmin Tang", "Qingyao Wu", "Xiansheng Hua"], "publication_date": "2021/7/28", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "description": "Most existing action recognition approaches directly leverage the video-level features to recognize human actions from videos. Although these methods have made remarkable progress, the accuracy is still unsatisfied. When the test video involves complex backgrounds and activities, existing methods usually suffer from a significant drop in accuracy. Human action is inherently a high-level concept. Merely applying a video classification model without a detailed semantic understanding of the video content, e.g., objects, scene context, object motions, object interactions, is inadequate to tackle the challenges for action recognition. Fine-level semantic understanding of videos generates elementary semantic concepts from the raw video data, such as the semantics of objects and background regions. It can be employed to bridge the gap between the raw video data and the high-level concept of human actions. In this \u2026", "total_citations": 12, "citation_graph": {"2022": 8, "2023": 4}}, {"title": "Few-shot fine-grained classification with spatial attentive comparison", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:fPk4N6BV_jEC", "authors": ["Xiaoqian Ruan", "Guosheng Lin", "Cheng Long", "Shengli Lu"], "publication_date": "2021/4/22", "journal": "Knowledge-Based Systems", "description": "The main goal of this paper is to propose a novel model, named Spatial Attentive Comparison Network (SACN), which is used to address a problem, termed few-shot fine-grained recognition (FSFG). FSFG is to recognize fine-grained examples with only a few samples, which is challenging for deep neural networks. SACN is made up of three modules, namely feature extraction module, selective-comparison similarity module (SCSM), and classification module: feature extraction module extracts the distinctive information into feature maps, SCSM is used to fuse the features of support set with those of the query set based on selective comparison. Considering the noisy background and tiny differences between different categories, we apply SCSM to fuse these features by arranging different weights pixel by pixel, and all these weights are learned automatically. Moreover, we apply pyramid structure to enrich the \u2026", "total_citations": 12, "citation_graph": {"2021": 2, "2022": 6, "2023": 4}}, {"title": "Weakly-supervised cross-domain road scene segmentation via multi-level curriculum adaptation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:pqnbT2bcN3wC", "authors": ["Fengmao Lv", "Guosheng Lin", "Peng Liu", "Guowu Yang", "Sinno Jialin Pan", "Lixin Duan"], "publication_date": "2020/11/24", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "description": "Semantic segmentation, which aims to acquire pixel-level understanding about images, is among the key components in computer vision. To train a good segmentation model for real-world images, it usually requires a huge amount of time and labor effort to obtain sufficient pixel-level annotations of real-world images beforehand. To get rid of such a nontrivial burden, one can use simulators to automatically generate synthetic images that inherently contain full pixel-level annotations and use them to train a segmentation model for the real-world images. However, training with synthetic images usually cannot lead to good performance due to the domain difference between the synthetic images (i.e., source domain) and the real-world images (i.e., target domain). To deal with this issue, a number of unsupervised domain adaptation (UDA) approaches have been proposed, where no labeled real-world images are available \u2026", "total_citations": 12, "citation_graph": {"2021": 1, "2022": 6, "2023": 5}}, {"title": "Exploring bottom-up and top-down cues with attentive learning for webly supervised object detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:blknAaTinKkC", "authors": ["Zhonghua Wu", "Qingyi Tao", "Guosheng Lin", "Jianfei Cai"], "publication_date": "2020", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "description": "Fully supervised object detection has achieved great success in recent years. However, abundant bounding boxes annotations are needed for training a detector for novel classes. To reduce the human labeling effort, we propose a novel webly supervised object detection (WebSOD) method for novel classes which only requires the web images without further annotations. Our proposed method combines bottom-up and top-down cues for novel class detection. Within our approach, we introduce a bottom-up mechanism based on the well-trained fully supervised object detector (ie Faster RCNN) as an object region estimator for web images by recognizing the common objectiveness shared by base and novel classes. With the estimated regions on the web images, we then utilize the top-down attention cues as the guidance for region classification. Furthermore, we propose a residual feature refinement (RFR) block to tackle the domain mismatch between web domain and the target domain. We demonstrate our proposed method on PASCAL VOC dataset with three different novel/base splits. Without any target-domain novel-class images and annotations, our proposed webly supervised object detection model is able to achieve promising performance for novel classes. Moreover, we also conduct transfer learning experiments on large scale ILSVRC 2013 detection dataset and achieve state-of-the-art performance.", "total_citations": 12, "citation_graph": {"2020": 1, "2021": 5, "2022": 5, "2023": 1}}, {"title": "CRCNet: Few-Shot Segmentation with Cross-Reference and Region\u2013Global Conditional Networks", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:kRWSkSYxWN8C", "authors": ["Weide Liu", "Chi Zhang", "Guosheng Lin", "Fayao Liu"], "publication_date": "2022/12", "journal": "International Journal of Computer Vision", "description": "Few-shot segmentation aims to learn a segmentation model that can be generalized to novel classes with only a few training images. In this paper, we propose a Cross-Reference and Local\u2013Global Conditional Networks (CRCNet) for few-shot segmentation. Unlike previous works that only predict the query image\u2019s mask, our proposed model concurrently makes predictions for both the support image and the query image. Our network can better find the co-occurrent objects in the two images with a cross-reference mechanism, thus helping the few-shot segmentation task. To further improve feature comparison, we develop a local-global conditional module to capture both global and local relations. We also develop a mask refinement module to refine the prediction of the foreground regions recurrently. Experiments on the PASCAL VOC 2012, MS COCO, and FSS-1000 datasets show that our network achieves new \u2026", "total_citations": 11, "citation_graph": {"2022": 1, "2023": 9}}, {"title": "Cross-image region mining with region prototypical network for weakly supervised segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:P5F9QuxV20EC", "authors": ["Weide Liu", "Xiangfei Kong", "Tzu-Yi Hung", "Guosheng Lin"], "publication_date": "2021/12/31", "journal": "IEEE Transactions on Multimedia", "description": "Weakly supervised image segmentation trained with image-level labels usually suffers from inaccurate coverage of object areas during the generation of the pseudo groundtruth. This is because the object activation maps are trained with the classification objective and lack the ability to generalize. To improve the generality of the object activation maps, we propose a region prototypical network ( RPNet ) to explore the cross-image object diversity of the training set. Similar object parts across images are identified via region feature comparison. Object confidence is propagated between regions to discover new object areas while background regions are suppressed. Experiments show that the proposed method generates more complete and accurate pseudo object masks while achieving state-of-the-art performance on PASCAL VOC 2012 and MS COCO. In addition, we investigate the robustness of the proposed \u2026", "total_citations": 11, "citation_graph": {"2022": 2, "2023": 8}}, {"title": "Structured learning of binary codes with column generation for optimizing ranking measures", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:8k81kl-MbHgC", "authors": ["Guosheng Lin", "Fayao Liu", "Chunhua Shen", "Jianxin Wu", "Heng Tao Shen"], "publication_date": "2017/6", "journal": "International Journal of Computer Vision", "description": "Hashing methods aim to learn a set of hash functions which map the original features to compact binary codes with similarity preserving in the Hamming space. Hashing has proven a valuable tool for large-scale information retrieval. We propose a column generation based binary code learning framework for data-dependent hash function learning. Given a set of triplets that encode the pairwise similarity comparison information, our column generation based method learns hash functions that preserve the relative comparison relations within the large-margin learning framework. Our method iteratively learns the best hash functions during the column generation procedure. Existing hashing methods optimize over simple objectives such as the reconstruction error or graph Laplacian related loss functions, instead of the performance evaluation criteria of interest\u2014multivariate performance measures such as the \u2026", "total_citations": 11, "citation_graph": {"2017": 2, "2018": 4, "2019": 3, "2020": 1, "2021": 1}}, {"title": "Learning Multi-level Region Consistency with Dense Multi-label Networks for Semantic Segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:ULOm3_A8WrAC", "authors": ["Tong Shen", "Guosheng Lin", "Chunhua Shen", "Ian Reid"], "publication_date": "2017", "conference": "International Joint Conference on Artificial Intelligence (IJCAI)", "description": "Semantic image segmentation is a fundamental task in image understanding. Per-pixel semantic labelling of an image benefits greatly from the ability to consider region consistency both locally and globally. However, many Fully Convolutional Network based methods do not impose such consistency, which may give rise to noisy and implausible predictions. We address this issue by proposing a dense multi-label network module that is able to encourage the region consistency at different levels. This simple but effective module can be easily integrated into any semantic segmentation systems. With comprehensive experiments, we show that the dense multi-label can successfully remove the implausible labels and clear the confusion so as to boost the performance of semantic segmentation systems.", "total_citations": 11, "citation_graph": {"2017": 1, "2018": 4, "2019": 4, "2020": 0, "2021": 0, "2022": 2}}, {"title": "Learning structural representations for recipe generation and food retrieval", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:u9iWguZQMMsC", "authors": ["Hao Wang", "Guosheng Lin", "Steven CH Hoi", "Chunyan Miao"], "publication_date": "2022/6/10", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "description": "Food is significant to human daily life. In this paper, we are interested in learning structural representations for lengthy recipes, that can benefit the recipe generation and food cross-modal retrieval tasks. Different from the common vision-language data, here the food images contain mixed ingredients and target recipes are lengthy paragraphs, where we do not have annotations on structure information. To address the above limitations, we propose a novel method to unsupervisedly learn the sentence-level tree structures for the cooking recipes. Our approach brings together several novel ideas in a systematic framework: (1) exploiting an unsupervised learning approach to obtain the sentence-level tree structure labels before training; (2) generating trees of target recipes from images with the supervision of tree structure labels learned from (1); and (3) integrating the learned tree structures into the recipe generation \u2026", "total_citations": 10, "citation_graph": {"2022": 3, "2023": 7}}, {"title": "Mv-ton: Memory-based video virtual try-on network", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:CHSYGLWDkRkC", "authors": ["Xiaojing Zhong", "Zhonghua Wu", "Taizhe Tan", "Guosheng Lin", "Qingyao Wu"], "publication_date": "2021/10/17", "book": "Proceedings of the 29th ACM International Conference on Multimedia", "description": "With the development of Generative Adversarial Network, image-based virtual try-on methods have made great progress. However, limited work has explored the task of video-based virtual try-on while it is important in real-world applications. Most existing video-based virtual try-on methods usually require clothing templates and they can only generate blurred and low-resolution results. To address these challenges, we propose a Memory-based Video virtual Try-On Network (MV-TON), which seamlessly transfers desired clothes to a target person without using any clothing templates and generates high-resolution realistic videos. Specifically, MV-TON consists of two modules: 1) a try-on module that transfers the desired clothes from model images to frame images by pose alignment and region-wise replacing of pixels; 2) a memory refinement module that learns to embed the existing generated frames into the latent \u2026", "total_citations": 10, "citation_graph": {"2021": 1, "2022": 5, "2023": 4}}, {"title": "Dynamically transformed instance normalization network for generalizable person re-identification", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:5awf1xo2G04C", "authors": ["Bingliang Jiao", "Lingqiao Liu", "Liying Gao", "Guosheng Lin", "Lu Yang", "Shizhou Zhang", "Peng Wang", "Yanning Zhang"], "publication_date": "2022/10/23", "book": "European Conference on Computer Vision", "description": "Existing person re-identification methods often suffer significant performance degradation on unseen domains, which fuels interest in domain generalizable person re-identification (DG-PReID). As an effective technology to alleviate domain variance, the Instance Normalization (IN) has been widely employed in many existing works. However, IN also suffers from the limitation of eliminating discriminative patterns that might be useful for a particular domain or instance. In this work, we propose a new normalization scheme called Dynamically Transformed Instance Normalization (DTIN) to alleviate the drawback of IN. Our idea is to employ dynamic convolution to allow the unnormalized feature to control the transformation of the normalized features into new representations. In this way, we can ensure the network has sufficient flexibility to strike the right balance between eliminating irrelevant domain-specific features \u2026", "total_citations": 9, "citation_graph": {"2022": 1, "2023": 8}}, {"title": "SymmNeRF: Learning to Explore Symmetry Prior for Single-View View Synthesis", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:1qzjygNMrQYC", "authors": ["Xingyi Li", "Chaoyi Hong", "Yiran Wang", "Zhiguo Cao", "Ke Xian", "Guosheng Lin"], "publication_date": "2022", "conference": "Proceedings of the Asian Conference on Computer Vision", "description": "We study the problem of novel view synthesis of objects from a single image. Existing methods have demonstrated the potential in single-view view synthesis. However, they still fail to recover the fine appearance details, especially in self-occluded areas. This is because a single view only provides limited information. We observe that manmade objects usually exhibit symmetric appearances, which introduce additional prior knowledge. Motivated by this, we investigate the potential performance gains of explicitly embedding symmetry into the scene representation. In this paper, we propose SymmNeRF, a neural radiance field (NeRF) based framework that combines local and global conditioning under the introduction of symmetry priors. In particular, SymmNeRF takes the pixel-aligned image features and the corresponding symmetric features as extra inputs to the NeRF, whose parameters are generated by a hypernetwork. As the parameters are conditioned on the image-encoded latent codes, SymmNeRF is thus scene-independent and can generalize to new scenes. Experiments on synthetic and real-world datasets show that SymmNeRF synthesizes novel views with more details regardless of the pose transformation, and demonstrates good generalization when applied to unseen objects. Code is available at: https://github. com/xingyi-li/SymmNeRF.", "total_citations": 9, "citation_graph": {"2023": 9}}, {"title": "CT-Net: Complementary Transfering Network for Garment Transfer with Arbitrary Geometric Changes", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:cFHS6HbyZ2cC", "authors": ["Fan Yang", "Guosheng Lin"], "publication_date": "2021", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "description": "Garment transfer shows great potential in realistic applications with the goal of transfering outfits across different people images. However, garment transfer between images with heavy misalignments or severe occlusions still remains as a challenge. In this work, we propose Complementary Transfering Network (CT-Net) to adaptively model different levels of geometric changes and transfer outfits between different people. In specific, CT-Net consists of three modules: i) A complementary warping module first estimates two complementary warpings to transfer the desired clothes in different granularities. ii) A layout prediction module is proposed to predict the target layout, which guides the preservation or generation of the body parts in the synthesized images. iii) A dynamic fusion module adaptively combines the advantages of the complementary warpings to render the garment transfer results. Extensive experiments conducted on DeepFashion dataset demonstrate that our network synthesizes high-quality garment transfer images and significantly outperforms the state-of-art methods both qualitatively and quantitatively. Our source code will be available online.", "total_citations": 9, "citation_graph": {"2022": 6, "2023": 3}}, {"title": "Tackling background ambiguities in multi-class few-shot point cloud semantic segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:D_sINldO8mEC", "authors": ["Lvlong Lai", "Jian Chen", "Chi Zhang", "Zehong Zhang", "Guosheng Lin", "Qingyao Wu"], "publication_date": "2022/10/11", "journal": "Knowledge-Based Systems", "description": "Few-shot point cloud semantic segmentation learns to segment novel classes with scarce labeled samples. Within an episode, a novel target class is defined by a few support samples with corresponding binary masks, where only the points of this class are labeled as foreground and others are regarded as background. In the tasks involving multiple target classes, since the meanings of background are diverse for different target classes, background ambiguities appear: Some points labeled as background in one support sample may be of other target classes. It will result in incorrect guidance and damage model\u2019s segmentation performance. However, previous methods in the literature do not consider this problem. In this paper, we propose a simple yet effective approach to tackle background ambiguities, which adopts the entropy of predictions on query samples to the training objective function as an additional \u2026", "total_citations": 8, "citation_graph": {"2023": 8}}, {"title": "Guided by Meta-Set: A Data-Driven Method for Fine-Grained Visual Recognition", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:fQNAKQ3IYiAC", "authors": ["Chuanyi Zhang", "Guosheng Lin", "Qiong Wang", "Fumin Shen", "Yazhou Yao", "Zhenmin Tang"], "publication_date": "2022/6/9", "journal": "IEEE Transactions on Multimedia", "description": "The lack of sufficient training data has been one obstacle to fine-grained visual classification research because labeling subcategories generally requires specialist knowledge. As one optional approach to alleviating the data-hunger problem, leveraging web images as training data is drawing increasing attention. Nevertheless, web images potentially have false labels, which can misguide the training process. Although several works have been proposed to deal with label noise, it still can be difficult for the network to tackle complex real-world noisy labels without any prior knowledge. In the literature, we propose to leverage a small and clean meta-set to provide reliable prior knowledge for tackling noisy web images. Specifically, our method trains a network with two peer predicting heads, which learn from noisy web images (web head) and meta ones (meta head), respectively. The meta head produces pseudo soft \u2026", "total_citations": 8, "citation_graph": {"2022": 5, "2023": 3}}, {"title": "Feature flow: In-network feature flow estimation for video object detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:2P1L_qKh6hAC", "authors": ["Ruibing Jin", "Guosheng Lin", "Changyun Wen", "Jianliang Wang", "Fayao Liu"], "publication_date": "2022/2/1", "journal": "Pattern Recognition", "description": "Optical flow, which expresses pixel displacement, is widely used in many computer vision tasks to provide pixel-level motion information. However, with the remarkable progress of the convolutional neural network, recent state-of-the-art approaches are proposed to solve problems directly on feature-level. Since the displacement of feature vector is not consistent with the pixel displacement, a common approach is to forward optical flow to a neural network and fine-tune this network on the task dataset. With this method, they expect the fine-tuned network to produce tensors encoding feature-level motion information. In this paper, we rethink about this de facto paradigm and analyze its drawbacks in the video object detection task. To mitigate these issues, we propose a novel network (IFF-Net) with an In-network Feature Flow estimation module (IFF module) for video object detection. Without resorting to pre-training on \u2026", "total_citations": 8, "citation_graph": {"2022": 5, "2023": 3}}, {"title": "Expanding Large Pre-Trained Unimodal Models With Multimodal Information Injection for Image-Text Multimodal Classification", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:8AbLer7MMksC", "authors": ["Tao Liang", "Guosheng Lin", "Mingyang Wan", "Tianrui Li", "Guojun Ma", "Fengmao Lv"], "publication_date": "2022", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "description": "Fine-tuning pre-trained models for downstream tasks is mainstream in deep learning. However, the pre-trained models are limited to be fine-tuned by data from a specific modality. For example, as a visual model, DenseNet cannot directly take the textual data as its input. Hence, although the large pre-trained models such as DenseNet or BERT have a great potential for the downstream recognition tasks, they have weaknesses in leveraging multimodal information, which is a new trend of deep learning. This work focuses on fine-tuning pre-trained unimodal models with multimodal inputs of image-text pairs and expanding them for image-text multimodal recognition. To this end, we propose the Multimodal Information Injection Plug-in (MI2P) which is attached to different layers of the unimodal models (eg, DenseNet and BERT). The proposed MI2P unit provides the path to integrate the information of other modalities into the unimodal models. Specifically, MI2P performs cross-modal feature transformation by learning the fine-grained correlations between the visual and textual features. Through the proposed MI2P unit, we can inject the language information into the vision backbone by attending the word-wise textual features to different visual channels, as well as inject the visual information into the language backbone by attending the channel-wise visual features to different textual words. Armed with the MI2P attachments, the pre-trained unimodal models can be expanded to process multimodal data without the need to change the network structures.", "total_citations": 8, "citation_graph": {"2022": 1, "2023": 7}}, {"title": "Bottom-Up Scene Text Detection with Markov Clustering Networks", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:maZDTaKrznsC", "authors": ["Zichuan Liu", "Guosheng Lin", "Wang Ling Goh"], "publication_date": "2020", "journal": "International Journal of Computer Vision", "description": "A novel detection framework named Markov Clustering Network (MCN) is proposed for fast and robust scene text detection. Different from the traditional top-down scene text detection approaches that inherit from the classic object detection, MCN detects scene text objects in a bottom-up manner. MCN predicts instance-level bounding boxes by firstly converting an image into a stochastic flow graph where Markov Clustering is performed based on the predicted stochastic flows. The stochastic flows encode the local correlation and semantic information of scene text objects. An object is modeled as strongly connected nodes by flows, which allows flexible and bottom-up detection for scale-varying and rotated text objects without prior knowledge of object size. The flow prediction is supported by the advanced Convolutional Neural Networks architectures and Position-aware spatial attention mechanism, which provides \u2026", "total_citations": 8, "citation_graph": {"2021": 1, "2022": 4, "2023": 3}}, {"title": "Local fusion networks with chained residual pooling for video action recognition", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:-f6ydRqryjwC", "authors": ["Feixiang He", "Fayao Liu", "Rui Yao", "Guosheng Lin"], "publication_date": "2019/1/1", "journal": "Image and Vision Computing", "description": "Action recognition is an important yet challenging problem. We here present a novel method, multistage local fusion networks with residual connections, to boost the performance of video action recognition. In realistic videos, an action instance may have a long time span and some frames may suffer from deteriorated object appearance due to motion blur or video defocus. Our method enhances the per-frame representation by capturing information from neighboring frames. We propose a local fusion block which considers neighboring frames to capture appearance and local motion information for generating per-frame representation. Our local fusion is performed in a multistage manner allowing feature fusion from varying neighborhood sizes in the temporal dimension. We employ residual connections in the fusion blocks to enable effective gradient propagation through the whole network allowing effective end-to \u2026", "total_citations": 8, "citation_graph": {"2020": 2, "2021": 5, "2022": 1}}, {"title": "Learning regional purity for instance segmentation on 3d point clouds", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:XiVPGOgt02cC", "authors": ["Shichao Dong", "Guosheng Lin", "Tzu-Yi Hung"], "publication_date": "2022/10/23", "book": "European Conference on Computer Vision", "description": "3D instance segmentation is a fundamental task for scene understanding, with a variety of applications in robotics and AR/VR. Many proposal-free methods have been proposed recently for this task, with remarkable results and high efficiency. However, these methods heavily rely on instance centroid regression and do not explicitly detect object boundaries, thus may mistakenly group nearby objects into the same clusters in some scenarios. In this paper, we define a novel concept of \u201cregional purity\u201d as the percentage of neighboring points belonging to the same instance within a fixed-radius 3D space. Intuitively, it indicates the likelihood of a point belonging to the boundary area. To evaluate the feasibility of predicting regional purity, we design a strategy to build a random scene toy dataset based on existing training data. Besides, using toy data is a \u201cfree\u201d way of data augmentation on learning regional purity, which \u2026", "total_citations": 7, "citation_graph": {"2023": 7}}, {"title": "Online Active Proposal Set Generation for weakly supervised object detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:zA6iFVUQeVQC", "authors": ["Ruibing Jin", "Guosheng Lin", "Changyun Wen"], "publication_date": "2022/2/15", "journal": "Knowledge-Based Systems", "description": "To reduce the manpower consumption on box-level annotations, many weakly supervised object detection methods which only require image-level annotations, have been proposed recently. The training process in these methods is formulated into two steps. They firstly train a neural network under weak supervision to generate pseudo ground truths (PGTs). Then, these PGTs are used to train another network under full supervision. Compared with fully supervised methods, the training process in weakly supervised methods becomes more complex and time-consuming. Furthermore, overwhelming negative proposals are involved at the first step. This is neglected by most methods, which makes the training network biased towards to negative proposals and thus degrades the quality of the PGTs, limiting the training network performance at the second step. Online proposal sampling is an intuitive solution to these \u2026", "total_citations": 7, "citation_graph": {"2022": 4, "2023": 3}}, {"title": "Depth and Video Segmentation Based Visual Attention for Embodied Question Answering", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:08ZZubdj9fEC", "authors": ["Haonan Luo", "Guosheng Lin", "Yazhou Yao", "Fayao Liu", "Zichuan Liu", "Zhenming Tang"], "publication_date": "2022/1/4", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "description": "Embodied Question Answering (EQA) is a newly defined research area where an agent is required to answer the user's questions by exploring the real-world environment. It has attracted increasing research interests due to its broad applications in personal assistants and in-home robots. Most of the existing methods perform poorly in terms of answering and navigation accuracy due to the absence of fine-level semantic information, stability to the ambiguity, and 3D spatial information of the virtual environment. To tackle these problems, we propose a depth and segmentation based visual attention mechanism for Embodied Question Answering. First, we extract local semantic features by introducing a novel high-speed video segmentation framework. Then guided by the extracted semantic features, a depth and segmentation based visual attention mechanism is proposed for the Visual Question Answering (VQA) sub-task \u2026", "total_citations": 7, "citation_graph": {"2022": 1, "2023": 6}}, {"title": "Point discriminative learning for unsupervised representation learning on 3D point clouds", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:WqliGbK-hY8C", "authors": ["Fayao Liu", "Guosheng Lin", "Chuan-Sheng Foo", "Chaitanya K Joshi", "Jie Lin"], "publication_date": "2021/8/4", "journal": "arXiv preprint arXiv:2108.02104", "description": "Unsupervised learning has witnessed tremendous success in natural language understanding and 2D image domain recently. How to leverage the power of unsupervised learning for 3D point cloud analysis remains open. Most existing methods simply adapt techniques used in 2D domain to 3D domain, while not fully exploiting the specificity of 3D data. In this work we propose a point discriminative learning method for unsupervised representation learning on 3D point clouds, which is specially designed for point cloud data and can learn local and global shape features. We achieve this by imposing a novel point discrimination loss on the middle level and global level features produced by the backbone network. This point discrimination loss enforces the features to be consistent with points belonging to the corresponding local shape region and inconsistent with randomly sampled noisy points. Our method is simple in design, which works by adding an extra adaptation module and a point consistency module for unsupervised training of the backbone encoder. Once trained, these two modules can be discarded during supervised training of the classifier or decoder for downstream tasks. We conduct extensive experiments on 3D object classification, 3D semantic and part segmentation in various settings and achieve new state-of-the-art results. We also perform a detailed analysis of our method and visually demonstrate that the reconstructed local shapes from our learned unsupervised features are highly consistent with the ground-truth shapes.", "total_citations": 7, "citation_graph": {"2022": 3, "2023": 4}}, {"title": "Integratedpifu: Integrated pixel aligned implicit function for single-view human reconstruction", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:9vf0nzSNQJEC", "authors": ["Kennard Yanting Chan", "Guosheng Lin", "Haiyu Zhao", "Weisi Lin"], "publication_date": "2022/10/23", "book": "European conference on computer vision", "description": "We propose IntegratedPIFu, a new pixel-aligned implicit model that builds on the foundation set by PIFuHD. IntegratedPIFu shows how depth and human parsing information can be predicted and capitalized upon in a pixel-aligned implicit model. In addition, IntegratedPIFu introduces depth-oriented sampling, a novel training scheme that improve any pixel-aligned implicit model\u2019s ability to reconstruct important human features without noisy artefacts. Lastly, IntegratedPIFu presents a new architecture that, despite using less model parameters than PIFuHD, is able to improves the structural correctness of reconstructed meshes. Our results show that IntegratedPIFu significantly outperforms existing state-of-the-arts methods on single-view human reconstruction. We provide the code in our supplementary materials. Our code is available at https://github.com/kcyt/IntegratedPIFu.", "total_citations": 6, "citation_graph": {"2022": 3, "2023": 3}}, {"title": "Unsupervised 3d pose transfer with cross consistency and dual reconstruction", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&pagesize=100&citation_for_view=ZudEhvcAAAAJ:tKAzc9rXhukC", "authors": ["Chaoyue Song", "Jiacheng Wei", "Ruibo Li", "Fayao Liu", "Guosheng Lin"], "publication_date": "2023/3/20", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "description": "The goal of 3D pose transfer is to transfer the pose from the source mesh to the target mesh while preserving the identity information (e.g., face, body shape) of the target mesh. Deep learning-based methods improved the efficiency and performance of 3D pose transfer. However, most of them are trained under the supervision of the ground truth, whose availability is limited in real-world scenarios. In this work, we present X-DualNet , a simple yet effective approach that enables unsupervised 3D pose transfer. In X-DualNet , we introduce a generator which contains correspondence learning and pose transfer modules to achieve 3D pose transfer. We learn the shape correspondence by solving an optimal transport problem without any key point annotations and generate high-quality meshes with our elastic instance normalization (ElaIN) in the pose transfer module. With as the basic component, we propose a \u2026", "total_citations": 5, "citation_graph": {"2023": 5}}, {"title": "Few-shot Open-set Recognition Using Background as Unknowns", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:eJXPG6dFmWUC", "authors": ["Nan Song", "Chi Zhang", "Guosheng Lin"], "publication_date": "2022/10/10", "book": "Proceedings of the 30th ACM International Conference on Multimedia", "description": "In this paper, we propose to solve the problem from two novel aspects. First, instead of learning the decision boundaries between seen classes, as is done in standard close-set classification, we reserve space for unseen classes, such that images located in these areas are recognized as the unseen classes. Second, to effectively learn such decision boundaries, we propose to utilize the background features from seen classes. As these background regions do not significantly contribute to the decision of close-set classification, it is natural to use them as pseudo unseen classes for classifier learning. Our extensive experiments show that our proposed method not only outperforms multiple baselines but also sets new state-of-the-art results on three popular benchmarks, namely tieredImageNet, miniImageNet, and Caltech-USCD Birds-200-2011 (CUB).", "total_citations": 5, "citation_graph": {"2023": 5}}, {"title": "Cross-modal graph with meta concepts for video captioning", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:1sJd4Hv_s6UC", "authors": ["Hao Wang", "Guosheng Lin", "Steven CH Hoi", "Chunyan Miao"], "publication_date": "2022/7/28", "journal": "IEEE Transactions on Image Processing", "description": "Video captioning targets interpreting the complex visual contents as text descriptions, which requires the model to fully understand video scenes including objects and their interactions. Prevailing methods adopt off-the-shelf object detection networks to give object proposals and use the attention mechanism to model the relations between objects. They often miss some undefined semantic concepts of the pretrained model and fail to identify exact predicate relationships between objects. In this paper, we investigate an open research task of generating text descriptions for the given videos, and propose Cross-Modal Graph (CMG) with meta concepts for video captioning. Specifically, to cover the useful semantic concepts in video captions, we weakly learn the corresponding visual regions for text descriptions, where the associated visual regions and textual words are named cross-modal meta concepts. We further \u2026", "total_citations": 5, "citation_graph": {"2023": 5}}, {"title": "Robust-EQA: Robust Learning for Embodied Question Answering With Noisy Labels", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:dQ2og3OwTAUC", "authors": ["Haonan Luo", "Guosheng Lin", "Fumin Shen", "Xingguo Huang", "Yazhou Yao", "Hengtao Shen"], "publication_date": "2023/3/13", "journal": "IEEE Transactions on Neural Networks and Learning Systems", "description": "Embodied question answering (EQA) is a recently emerged research field in which an agent is asked to answer the user\u2019s questions by exploring the environment and collecting visual information. Plenty of researchers turn their attention to the EQA field due to its broad potential application areas, such as in-home robots, self-driven mobile, and personal assistants. High-level visual tasks, such as EQA, are susceptible to noisy inputs, because they have complex reasoning processes. Before the profits of the EQA field can be applied to practical applications, good robustness against label noise needs to be equipped. To tackle this problem, we propose a novel label noise-robust learning algorithm for the EQA task. First, a joint training co-regularization noise-robust learning method is proposed for noisy filtering of the visual question answering (VQA) module, which trains two parallel network branches by one loss \u2026", "total_citations": 4, "citation_graph": {"2023": 4}}, {"title": "Self-Training Vision Language BERTs with a Unified Conditional Model", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:vRqMK49ujn8C", "authors": ["Xiaofeng Yang", "Fengmao Lv", "Fayao Liu", "Guosheng Lin"], "publication_date": "2023/1/10", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "description": "Natural language BERTs are trained with language corpus in a self-supervised manner. Unlike natural language BERTs, vision language BERTs need paired data to train, which restricts the scale of VL-BERT pretraining. We propose a self-training approach that allows training VL-BERTs from unlabeled image data. The proposed method starts with our unified conditional model \u2013 a vision language BERT model that can perform zero-shot conditional generation. Given different conditions, the unified conditional model can generate captions, dense captions, and even questions. We use the labeled image data to train a teacher model and use the trained model to generate pseudo captions on unlabeled image data. We then combine the labeled data and pseudo labeled data to train a student model. The process is iterated by putting the student model as a new teacher. By using the proposed self-training approach \u2026", "total_citations": 4, "citation_graph": {"2023": 4}}, {"title": "Neural Vector Fields: Implicit Representation by Explicit Learning", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:PR6Y55bgFSsC", "authors": ["Xianghui Yang", "Guosheng Lin", "Zhenghao Chen", "Luping Zhou"], "publication_date": "2023", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "description": "Deep neural networks (DNNs) are widely applied for nowadays 3D surface reconstruction tasks and such methods can be further divided into two categories, which respectively warp templates explicitly by moving vertices or represent 3D surfaces implicitly as signed or unsigned distance functions. Taking advantage of both advanced explicit learning process and powerful representation ability of implicit functions, we propose a novel 3D representation method, Neural Vector Fields (NVF). It not only adopts the explicit learning process to manipulate meshes directly, but also leverages the implicit representation of unsigned distance functions (UDFs) to break the barriers in resolution and topology. Specifically, our method first predicts the displacements from queries towards the surface and models the shapes as Vector Fields. Rather than relying on network differentiation to obtain direction fields as most existing UDF-based methods, the produced vector fields encode the distance and direction fields both and mitigate the ambiguity at\" ridge\" points, such that the calculation of direction fields is straightforward and differentiation-free. The differentiation-free characteristic enables us to further learn a shape codebook via Vector Quantization, which encodes the cross-object priors, accelerates the training procedure, and boosts model generalization on cross-category reconstruction. The extensive experiments on surface reconstruction benchmarks indicate that our method outperforms those state-of-the-art methods in different evaluation scenarios including watertight vs non-watertight shapes, category-specific vs category-agnostic reconstruction \u2026", "total_citations": 4, "citation_graph": {"2023": 4}}, {"title": "IT3D: Improved Text-to-3D Generation with Explicit View Synthesis", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:j8SEvjWlNXcC", "authors": ["Yiwen Chen", "Chi Zhang", "Xiaofeng Yang", "Zhongang Cai", "Gang Yu", "Lei Yang", "Guosheng Lin"], "publication_date": "2023/8/22", "journal": "arXiv preprint arXiv:2308.11473", "description": "Recent strides in Text-to-3D techniques have been propelled by distilling knowledge from powerful large text-to-image diffusion models (LDMs). Nonetheless, existing Text-to-3D approaches often grapple with challenges such as over-saturation, inadequate detailing, and unrealistic outputs. This study presents a novel strategy that leverages explicitly synthesized multi-view images to address these issues. Our approach involves the utilization of image-to-image pipelines, empowered by LDMs, to generate posed high-quality images based on the renderings of coarse 3D models. Although the generated images mostly alleviate the aforementioned issues, challenges such as view inconsistency and significant content variance persist due to the inherent generative nature of large diffusion models, posing extensive difficulties in leveraging these images effectively. To overcome this hurdle, we advocate integrating a discriminator alongside a novel Diffusion-GAN dual training strategy to guide the training of 3D models. For the incorporated discriminator, the synthesized multi-view images are considered real data, while the renderings of the optimized 3D models function as fake data. We conduct a comprehensive set of experiments that demonstrate the effectiveness of our method over baseline approaches.", "total_citations": 3, "citation_graph": {"2023": 3}}, {"title": "3D Cinemagraphy from a Single Image", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:ZuybSZzF8UAC", "authors": ["Xingyi Li", "Zhiguo Cao", "Huiqiang Sun", "Jianming Zhang", "Ke Xian", "Guosheng Lin"], "publication_date": "2023", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "description": "We present 3D Cinemagraphy, a new technique that marries 2D image animation with 3D photography. Given a single still image as input, our goal is to generate a video that contains both visual content animation and camera motion. We empirically find that naively combining existing 2D image animation and 3D photography methods leads to obvious artifacts or inconsistent animation. Our key insight is that representing and animating the scene in 3D space offers a natural solution to this task. To this end, we first convert the input image into feature-based layered depth images using predicted depth values, followed by unprojecting them to a feature point cloud. To animate the scene, we perform motion estimation and lift the 2D motion into the 3D scene flow. Finally, to resolve the problem of hole emergence as points move forward, we propose to bidirectionally displace the point cloud as per the scene flow and synthesize novel views by separately projecting them into target image planes and blending the results. Extensive experiments demonstrate the effectiveness of our method. A user study is also conducted to validate the compelling rendering results of our method.", "total_citations": 3, "citation_graph": {"2023": 3}}, {"title": "General Object Pose Transformation Network from Unpaired Data", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:bnK-pcrLprsC", "authors": ["Yukun Su", "Guosheng Lin", "Ruizhou Sun", "Qingyao Wu"], "publication_date": "2022/10/23", "book": "European Conference on Computer Vision", "description": "Object pose transformation is a challenging task. Yet, most existing pose transformation networks only focus on synthesizing humans. These methods either rely on the keypoints information or rely on the manual annotations of the paired target pose images for training. However, collecting such paired data is laboring and the cue of keypoints is inapplicable to general objects. In this paper, we address a problem of novel general object pose transformation from unpaired data. Given a source image of an object that provides appearance information and a desired pose image as reference in the absence of paired examples, we produce a depiction of the object in that specified pose, retaining the appearance of both the object and background. Specifically, to preserve the source information, we propose an adversarial network with patial-tructural (SS) block and exture-tyle-olor (TSC) block after the correlation \u2026", "total_citations": 3, "citation_graph": {"2023": 3}}, {"title": "Task-in-all domain adaptation for semantic segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:isC4tDSrTZIC", "authors": ["Tianyi Zhang", "Jingyi Yang", "Chuanxia Zheng", "Guosheng Lin", "Jianfei Cai", "Alex C Kot"], "publication_date": "2019/12/1", "conference": "2019 IEEE Visual Communications and Image Processing (VCIP)", "description": "In this work we tackle the problem of unsupervised domain adaptation for semantic segmentation. One pipeline is to sequentially train image-translation model and the final task segmentation model. In such pipeline, image translation is aimed to generate the translated source-domain images which are visually similar to the target-domain images and then the final task model is trained using the translated images and its corresponding groundtruth. However, the visually optimal translated-images are not necessarily optimal for the final task of segmenting the target-domain images. Thus we propose a Task-in-all pipeline for unsupervised domain adaptation on semantic segmentation, which incorporates image translation and final segmentation task into an end-to-end training pipeline. Our aim is to generate the translated images which better assists the final task, instead of just being visually similar to the target \u2026", "total_citations": 3, "citation_graph": {"2020": 1, "2021": 0, "2022": 1, "2023": 1}}, {"title": "Domain adaptive semantic segmentation through structure enhancement", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:mB3voiENLucC", "authors": ["Fengmao Lv", "Qing Lian", "Guowu Yang", "Guosheng Lin", "Sinno Jialin Pan", "Lixin Duan"], "publication_date": "2018", "conference": "Proceedings of the European Conference on Computer Vision (ECCV) Workshops", "description": "Although fully convolutional networks have recently achieved great advances in semantic segmentation, the performance leaps heavily rely on supervision with pixel-level annotations which are extremely expensive and time-consuming to collect. Training models on synthetic data is a feasible way to relieve the annotation burden. However, the domain shift between synthetic and real images usually lead to poor generalization performance. In this work, we propose an effective method to adapt the segmentation network trained on synthetic images to real scenarios in an unsupervised fashion. To improve the adaptation performance for semantic segmentation, we enhance the structure information of the target images at both the feature level and the output level. Specifically, we enforce the segmentation network to learn a representation that encodes the target images\u2019 visual cues through image reconstruction, which is beneficial to the structured prediction of the target images. Further more, we implement adversarial training at the output space of the segmentation network to align the structured prediction of the source and target images based on the similar spatial structure they share. To validate the performance of our method, we conduct comprehensive experiments on the\" GTA5 to Cityscapes\" dataset which is a standard domain adaptation benchmark for semantic segmentation. The experimental results clearly demonstrate that our method can effectively bridge the synthetic and real image domains and obtain better adaptation performance compared with the existing state-of-the-art methods.", "total_citations": 3, "citation_graph": {"2020": 2, "2021": 0, "2022": 1}}, {"title": "StyleAvatar3D: Leveraging Image-Text Diffusion Models for High-Fidelity 3D Avatar Generation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:Fu2w8maKXqMC", "authors": ["Chi Zhang", "Yiwen Chen", "Yijun Fu", "Zhenglin Zhou", "Gang Yu", "Billzb Wang", "Bin Fu", "Tao Chen", "Guosheng Lin", "Chunhua Shen"], "publication_date": "2023/5/30", "journal": "arXiv preprint arXiv:2305.19012", "description": "The recent advancements in image-text diffusion models have stimulated research interest in large-scale 3D generative models. Nevertheless, the limited availability of diverse 3D resources presents significant challenges to learning. In this paper, we present a novel method for generating high-quality, stylized 3D avatars that utilizes pre-trained image-text diffusion models for data generation and a Generative Adversarial Network (GAN)-based 3D generation network for training. Our method leverages the comprehensive priors of appearance and geometry offered by image-text diffusion models to generate multi-view images of avatars in various styles. During data generation, we employ poses extracted from existing 3D models to guide the generation of multi-view images. To address the misalignment between poses and images in data, we investigate view-specific prompts and develop a coarse-to-fine discriminator for GAN training. We also delve into attribute-related prompts to increase the diversity of the generated avatars. Additionally, we develop a latent diffusion model within the style space of StyleGAN to enable the generation of avatars based on image inputs. Our approach demonstrates superior performance over current state-of-the-art methods in terms of visual quality and diversity of the produced avatars.", "total_citations": 2, "citation_graph": {"2023": 2}}, {"title": "OR-NeRF: Object Removing from 3D Scenes Guided by Multiview Segmentation with Neural Radiance Fields", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:7T2F9Uy0os0C", "authors": ["Youtan Yin", "Zhoujie Fu", "Fan Yang", "Guosheng Lin"], "publication_date": "2023/5/17", "journal": "arXiv preprint arXiv:2305.10503", "description": "The emergence of Neural Radiance Fields (NeRF) for novel view synthesis has led to increased interest in 3D scene editing. One important task in editing is removing objects from a scene while ensuring visual reasonability and multiview consistency. However, current methods face challenges such as time-consuming object labelling, limited capability to remove specific targets, and compromised rendering quality after removal. This paper proposes a novel object-removing pipeline, named OR-NeRF, that can remove objects from 3D scenes with either point or text prompts on a single view, achieving better performance in less time than previous works. Our method uses a points projection strategy to rapidly spread user annotations to all views, significantly reducing the processing burden. This algorithm allows us to leverage the recent 2D segmentation model Segment-Anything (SAM) to predict masks with improved precision and efficiency. Additionally, we obtain colour and depth priors through 2D inpainting methods. Finally, our algorithm employs depth supervision and perceptual loss for scene reconstruction to maintain consistency in geometry and appearance after object removal. Experimental results demonstrate that our method achieves better editing quality with less time than previous works, considering both quality and quantity.", "total_citations": 2, "citation_graph": {"2023": 2}}, {"title": "Improving tail-class representation with centroid contrastive learning", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:ZfRJV9d4-WMC", "authors": ["Anthony Meng Huat Tiong", "Junnan Li", "Guosheng Lin", "Boyang Li", "Caiming Xiong", "Steven CH Hoi"], "publication_date": "2023/4/1", "journal": "Pattern Recognition Letters", "description": "In vision domain, large-scale natural datasets typically exhibit long-tailed distribution which has large class imbalance between head and tail classes. This distribution poses difficulty in learning good representations for tail classes. Recent developments have shown good long-tailed model can be learnt by decoupling the training into representation learning and classifier balancing. However, these works pay insufficient consideration on the long-tailed effect on representation learning. In this work, we propose interpolative centroid contrastive learning (ICCL) to improve long-tailed representation learning. ICCL interpolates two images from a class-agnostic sampler and a class-aware sampler, and trains the model such that the representation of the interpolative image can be used to retrieve the centroids for both source classes. We demonstrate the effectiveness of our approach on multiple long-tailed image \u2026", "total_citations": 2, "citation_graph": {"2023": 2}}, {"title": "Harmonizing Base and Novel Classes: A Class-Contrastive Approach for Generalized Few-Shot Segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:UHK10RUVsp4C", "authors": ["Weide Liu", "Zhonghua Wu", "Yang Zhao", "Yuming Fang", "Chuan-Sheng Foo", "Jun Cheng", "Guosheng Lin"], "publication_date": "2023/3/24", "journal": "arXiv preprint arXiv:2303.13724", "description": "Current methods for few-shot segmentation (FSSeg) have mainly focused on improving the performance of novel classes while neglecting the performance of base classes. To overcome this limitation, the task of generalized few-shot semantic segmentation (GFSSeg) has been introduced, aiming to predict segmentation masks for both base and novel classes. However, the current prototype-based methods do not explicitly consider the relationship between base and novel classes when updating prototypes, leading to a limited performance in identifying true categories. To address this challenge, we propose a class contrastive loss and a class relationship loss to regulate prototype updates and encourage a large distance between prototypes from different classes, thus distinguishing the classes from each other while maintaining the performance of the base classes. Our proposed approach achieves new state-of-the-art performance for the generalized few-shot segmentation task on PASCAL VOC and MS COCO datasets.", "total_citations": 2, "citation_graph": {"2023": 1}}, {"title": "TAPS3D: Text-Guided 3D Textured Shape Generation from Pseudo Supervision", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:hkOj_22Ku90C", "authors": ["Jiacheng Wei", "Hao Wang", "Jiashi Feng", "Guosheng Lin", "Kim-Hui Yap"], "publication_date": "2023", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "description": "In this paper, we investigate an open research task of generating controllable 3D textured shapes from the given textual descriptions. Previous works either require ground truth caption labeling or extensive optimization time. To resolve these issues, we present a novel framework, TAPS3D, to train a text-guided 3D shape generator with pseudo captions. Specifically, based on rendered 2D images, we retrieve relevant words from the CLIP vocabulary and construct pseudo captions using templates. Our constructed captions provide high-level semantic supervision for generated 3D shapes. Further, in order to produce fine-grained textures and increase geometry diversity, we propose to adopt low-level image regularization to enable fake-rendered images to align with the real ones. During the inference phase, our proposed model can generate 3D textured shapes from the given text without any additional optimization. We conduct extensive experiments to analyze each of our proposed components and show the efficacy of our framework in generating high-fidelity 3D textured and text-relevant shapes.", "total_citations": 2, "citation_graph": {"2023": 2}}, {"title": "Paired Cross-Modal Data Augmentation for Fine-Grained Image-to-Text Retrieval", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:eflP2zaiRacC", "authors": ["Hao Wang", "Guosheng Lin", "Steven Hoi", "Chunyan Miao"], "publication_date": "2022/10/10", "book": "Proceedings of the 30th ACM International Conference on Multimedia", "description": "This paper investigates an open research problem of generating text-image pairs to improve the training of fine-grained image-to-text cross-modal retrieval task, and proposes a novel framework for paired data augmentation by uncovering the hidden semantic information of StyleGAN2 model. Specifically, we first train a StyleGAN2 model on the given dataset. We then project the real images back to the latent space of StyleGAN2 to obtain the latent codes. To make the generated images manipulatable, we further introduce a latent space alignment module to learn the alignment between StyleGAN2 latent codes and the corresponding textual caption features. When we do online paired data augmentation, we first generate augmented text through random token replacement, then pass the augmented text into the latent space alignment module to output the latent codes, which are finally fed to StyleGAN2 to generate \u2026", "total_citations": 2, "citation_graph": {"2023": 2}}, {"title": "Learning language to symbol and language to vision mapping for visual grounding", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:tOudhMTPpwUC", "authors": ["Su He", "Xiaofeng Yang", "Guosheng Lin"], "publication_date": "2022/6/1", "journal": "Image and Vision Computing", "description": "Visual Grounding (VG) is a task of locating a specific object in an image semantically matching a given linguistic expression. The mapping of the linguistic and visual contents and the understanding of diverse linguistic expressions are the two challenges of this task. The performance of visual grounding is consistently improved by deep visual features in the last few years. While deep visual features contain rich information, they could also be noisy, biased and easily over-fitted. In contrast, symbolic features are discrete, easy to map and usually less noisy. In this work, we propose a novel modular network learning to match both the object's symbolic features and conventional visual features with the linguistic information. Moreover, the Residual Attention Parser is designed to alleviate the difficulty of understanding diverse expressions. Our model achieves competitive performance on three popular datasets of VG.", "total_citations": 2, "citation_graph": {"2022": 2}}, {"title": "S-PIFu: Integrating Parametric Human Models with PIFu for Single-view Clothed Human Reconstruction", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:WA5NYHcadZ8C", "authors": ["Kennard Chan", "Guosheng Lin", "Haiyu Zhao", "Weisi Lin"], "publication_date": "2022", "conference": "Advances in Neural Information Processing Systems", "description": "We present three novel strategies to incorporate a parametric body model into a pixel-aligned implicit model for single-view clothed human reconstruction. Firstly, we introduce ray-based sampling, a novel technique that transforms a parametric model into a set of highly informative, pixel-aligned 2D feature maps. Next, we propose a new type of feature based on blendweights. Blendweight-based labels serve as soft human parsing labels and help to improve the structural fidelity of reconstructed meshes. Finally, we show how we can extract and capitalize on body part orientation information from a parametric model to further improve reconstruction quality. Together, these three techniques form our S-PIFu framework, which significantly outperforms state-of-the-arts methods in all metrics. Our code is available at https://github. com/kcyt/SPIFu.", "total_citations": 2, "citation_graph": {"2023": 2}}, {"title": "Contrastive Generative Network with Recursive-Loop for 3D point cloud generalized zero-shot classification", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:-_dYPAW6P2MC", "authors": ["Yun Hao", "Yukun Su", "Guosheng Lin", "Hanjing Su", "Qingyao Wu"], "publication_date": "2023/12/1", "journal": "Pattern Recognition", "description": "Generalized Zero-Shot Learning (GZSL) aims to recognize objects from both seen and unseen categories by transferring semantic knowledge and merely utilizing seen class data for training. Recent feature generation methods in the 2D image domain have made great progress. However, very little is known about its usefulness in 3D point cloud zero-shot learning. This work aims to facilitate research on 3D point cloud generalized zero-shot learning. Different from previous works, we focus on synthesizing the more high-level discriminative point cloud features. To this end, we design a representation enhancement strategy to generate the features. Specifically, we propose a Contrastive Generative Network with Recursive-Loop, termed as CGRL, which can be leveraged to enlarge the inter-class distances and narrow the intra-class gaps. By applying the contrastive representations to the generative model in a \u2026", "total_citations": 1, "citation_graph": {"2023": 1}}, {"title": "Improving Video Violence Recognition with Human Interaction Learning on 3D Skeleton Point Clouds", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:fEOibwPWpKIC", "authors": ["Yukun Su", "Guosheng Lin", "Qingyao Wu"], "publication_date": "2023/8/26", "journal": "arXiv preprint arXiv:2308.13866", "description": "Deep learning has proved to be very effective in video action recognition. Video violence recognition attempts to learn the human multi-dynamic behaviours in more complex scenarios. In this work, we develop a method for video violence recognition from a new perspective of skeleton points. Unlike the previous works, we first formulate 3D skeleton point clouds from human skeleton sequences extracted from videos and then perform interaction learning on these 3D skeleton point clouds. Specifically, we propose two types of Skeleton Points Interaction Learning (SPIL) strategies: (i) Local-SPIL: by constructing a specific weight distribution strategy between local regional points, Local-SPIL aims to selectively focus on the most relevant parts of them based on their features and spatial-temporal position information. In order to capture diverse types of relation information, a multi-head mechanism is designed to aggregate different features from independent heads to jointly handle different types of relationships between points. (ii) Global-SPIL: to better learn and refine the features of the unordered and unstructured skeleton points, Global-SPIL employs the self-attention layer that operates directly on the sampled points, which can help to make the output more permutation-invariant and well-suited for our task. Extensive experimental results validate the effectiveness of our approach and show that our model outperforms the existing networks and achieves new state-of-the-art performance on video violence datasets.", "total_citations": 1, "citation_graph": {"2023": 1}}, {"title": "StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:uJ-U7cs_P_0C", "authors": ["Yanda Li", "Chi Zhang", "Gang Yu", "Zhibin Wang", "Bin Fu", "Guosheng Lin", "Chunhua Shen", "Ling Chen", "Yunchao Wei"], "publication_date": "2023/8/20", "journal": "arXiv preprint arXiv:2308.10253", "description": "The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have sparked significant interest in the development of multimodal Large Language Models (LLMs). A primary research objective of such models is to align visual and textual modalities effectively while comprehending human instructions. Current methodologies often rely on annotations derived from benchmark datasets to construct image-dialogue datasets for training purposes, akin to instruction tuning in LLMs. However, these datasets often exhibit domain bias, potentially constraining the generative capabilities of the models. In an effort to mitigate these limitations, we propose a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning. This approach harnesses the power of generative models, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content. This not only provides greater flexibility compared to existing methodologies but also significantly enhances several model capabilities. Our research includes comprehensive experiments conducted on various datasets using the open-source LLAVA model as a testbed for our proposed pipeline. Our results underscore marked enhancements across more than ten commonly assessed capabilities,", "total_citations": 1, "citation_graph": {"2023": 1}}, {"title": "Neural Video Depth Stabilizer", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:evX43VCCuoAC", "authors": ["Yiran Wang", "Min Shi", "Jiaqi Li", "Zihao Huang", "Zhiguo Cao", "Jianming Zhang", "Ke Xian", "Guosheng Lin"], "publication_date": "2023/7/17", "journal": "arXiv preprint arXiv:2307.08695", "description": "Video depth estimation aims to infer temporally consistent depth. Some methods achieve temporal consistency by finetuning a single-image depth model during test time using geometry and re-projection constraints, which is inefficient and not robust. An alternative approach is to learn how to enforce temporal consistency from data, but this requires well-designed models and sufficient video depth data. To address these challenges, we propose a plug-and-play framework called Neural Video Depth Stabilizer (NVDS) that stabilizes inconsistent depth estimations and can be applied to different single-image depth models without extra effort. We also introduce a large-scale dataset, Video Depth in the Wild (VDW), which consists of 14,203 videos with over two million frames, making it the largest natural-scene video depth dataset to our knowledge. We evaluate our method on the VDW dataset as well as two public benchmarks and demonstrate significant improvements in consistency, accuracy, and efficiency compared to previous approaches. Our work serves as a solid baseline and provides a data foundation for learning-based video depth models. We will release our dataset and code for future research.", "total_citations": 1, "citation_graph": {"2023": 1}}, {"title": "Single-view 3D Mesh Reconstruction for Seen and Unseen Categories", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:kzcrU_BdoSEC", "authors": ["Xianghui Yang", "Guosheng Lin", "Luping Zhou"], "publication_date": "2023/5/30", "journal": "IEEE Transactions on Image Processing", "description": "Single-view 3D object reconstruction is a fundamental and challenging computer vision task that aims at recovering 3D shapes from single-view RGB images. Most existing deep learning based reconstruction methods are trained and evaluated on the same categories, and they cannot work well when handling objects from novel categories that are not seen during training. Focusing on this issue, this paper tackles Single-view 3D Mesh Reconstruction, to study the model generalization on unseen categories and encourage models to reconstruct objects literally. Specifically, we propose an end-to-end two-stage network, GenMesh, to break the category boundaries in reconstruction. Firstly, we factorize the complicated image-to-mesh mapping into two simpler mappings, i.e ., image-to-point mapping and point-to-mesh mapping, while the latter is mainly a geometric problem and less dependent on object categories \u2026", "total_citations": 1, "citation_graph": {"2023": 1}}, {"title": "Effective End-to-End Vision Language Pretraining with Semantic Visual Loss", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:LjlpjdlvIbIC", "authors": ["Xiaofeng Yang", "Fayao Liu", "Guosheng Lin"], "publication_date": "2023/1/18", "journal": "IEEE Transactions on Multimedia", "description": "Current vision language pretraining models are dominated by methods using region visual features extracted from object detectors. Given their good performance, the extract-then-process pipeline significantly restricts the inference speed and therefore limits their real-world use cases. However, training vision language models from raw image pixels is difficult, as the raw image pixels give much less prior knowledge than region features. In this paper, we systematically study how to leverage auxiliary visual pretraining tasks to help training end-to-end vision language models. We introduce three types of visual losses that enable much faster convergence and better finetuning accuracy. Compared with region feature models, our end-to-end models could achieve similar or better performance on down-stream tasks and run more than 10 times faster during inference. Compared with other end-to-end models, our \u2026", "total_citations": 1, "citation_graph": {"2023": 1}}, {"title": "Weakly Supervised Class-Agnostic Motion Prediction for Autonomous Driving", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:_Ybze24A_UAC", "authors": ["Ruibo Li", "Hanyu Shi", "Ziang Fu", "Zhe Wang", "Guosheng Lin"], "publication_date": "2023", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "description": "Understanding the motion behavior of dynamic environments is vital for autonomous driving, leading to increasing attention in class-agnostic motion prediction in LiDAR point clouds. Outdoor scenes can often be decomposed into mobile foregrounds and static backgrounds, which enables us to associate motion understanding with scene parsing. Based on this observation, we study a novel weakly supervised motion prediction paradigm, where fully or partially (1%, 0.1%) annotated foreground/background binary masks rather than expensive motion annotations are used for supervision. To this end, we propose a two-stage weakly supervised approach, where the segmentation model trained with the incomplete binary masks in Stage1 will facilitate the self-supervised learning of the motion prediction network in Stage2 by estimating possible moving foregrounds in advance. Furthermore, for robust self-supervised motion learning, we design a Consistency-aware Chamfer Distance loss by exploiting multi-frame information and explicitly suppressing potential outliers. Comprehensive experiments show that, with fully or partially binary masks as supervision, our weakly supervised models surpass the self-supervised models by a large margin and perform on par with some supervised ones. This further demonstrates that our approach achieves a good compromise between annotation effort and performance.", "total_citations": 1, "citation_graph": {"2023": 1}}, {"title": "Semantic segmentation via domain adaptation with global structure embedding", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:k_IJM867U9cC", "authors": ["Tianyi Zhang", "Guosheng Lin", "Jianfei Cai", "Alex C Kot"], "publication_date": "2019/12/1", "conference": "2019 IEEE Visual Communications and Image Processing (VCIP)", "description": "In this paper we focus on the problem of unsupervised domain adaptation for semantic segmentation. The previous works usually focus on adversarial learning either in pixel-level or feature-level. However, global structure knowledge is often neglected in the adversarial learning due to the possible reasons: First, the result of pixel-level adversarial learning does not necessarily preserve the semantic consistency of the input image. Second, global structure knowledge is not embedded to regularize the feature-level adversarial learning. In this work, we propose a framework for unsupervised domain adaptation in semantic segmentation which effectively incorporates pixel- level, feature-level adversarial learning and self-training strategy. Our framework embeds the global structure knowledge into the adversarial training step to tackle the problem of structure misalignment. Consequently, our proposed framework \u2026", "total_citations": 1, "citation_graph": {"2020": 1}}, {"title": "Fast training of effective multi-class boosting using coordinate descent optimization", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:IjCSPb-OGe4C", "authors": ["Guosheng Lin", "Chunhua Shen", "Anton van den Hengel", "David Suter"], "publication_date": "2013", "conference": "Computer Vision\u2013ACCV 2012: 11th Asian Conference on Computer Vision, Daejeon, Korea, November 5-9, 2012, Revised Selected Papers, Part II 11", "description": "We present a novel column generation based boosting method for multi-class classification. Our multi-class boosting is formulated in a single optimization problem as in [1]. Different from most existing multi-class boosting methods, which use the same set of weak learners for all the classes, we train class specified weak learners (i.e., each class has a different set of weak learners). We show that using separate weak learner sets for each class leads to fast convergence, without introducing additional computational overhead in the training procedure. To further make the training more efficient and scalable, we also propose a fast coordinate descent method for solving the optimization problem at each boosting iteration. The proposed coordinate descent method is conceptually simple and easy to implement in that it is a closed-form solution for each coordinate update. Experimental results on a variety of \u2026", "total_citations": 1, "citation_graph": {"2019": 1}}, {"title": "Mutex Parts Collaborative Network for 3d Point Cloud Zero-Shot Classification", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:ye4kPcJQO24C", "authors": ["Yiwen Cao", "Yukun Su", "Guosheng Lin", "Qingyao Wu"], "journal": "Available at SSRN 4332138", "description": "Zero-shot learning (ZSL) aims to recognize objects from unseen categories by transferring semantic knowledge and merely utilizing seen class data for training. Although ZSL has made significant progress in the 2D image domain in recent years, very little has been explored about its usefulness in 3D point cloud domain yet. Directly applying existing ZSL techniques for 3D point cloud, however, suffers from the features misalignment caused by data structural differences and the ambiguity caused by poor 3D representations with their semantic counterparts. To this end, we make an early attempt to explore the practicality in the domain of 3D point cloud zero-shot classification. In this paper, we propose a novel\\textbf {M} utex\\textbf {P} arts\\textbf {C} ollaborative\\textbf {N} etwork (\\textbf {MPCN}). Specifically, we design to exploit the features discrepancy to discover mutex parts of objects and to spread feature activation explicitly. To further enhance the learning feature of these parts, we designed to propagate the collaborative information among the activated regions by using graphs. To validate the effectiveness of the proposed method, extensive experiments are conducted on ModelNet40, McGill, and ScanObjectNN datasets with different network architectures. Experimental evaluations demonstrate the superiority of our approach and it can outperform the state-of-the-arts by a significant margin.", "total_citations": 1, "citation_graph": {"2023": 1}}, {"title": "Neural Logic Vision Language Explainer", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:9Nmd_mFXekcC", "authors": ["Xiaofeng Yang", "Fayao Liu", "Guosheng Lin"], "publication_date": "2023/8/30", "journal": "IEEE Transactions on Multimedia", "description": "If we compare how humans reason and how deep models reason, humans reason in a symbolic manner with a formal language called logic, while most deep models reason in black-box. A natural question to ask is \u201cDo the trained deep models reason similar as humans?\u201d or \u201cCan we explain the reasoning of deep models in the language of logic?\u201d . In this work, we present NeurLogX to explain the reasoning process of deep vision language models in the language of logic. Given a trained vision language model, our method starts by generating reasoning facts through augmenting the input data. We then develop a differentiable inductive logic programming framework to learn interpretable logic rules from the facts. We show our results on various popular vision language models. Interestingly, we observe that almost all of the tested models can reason logically."}, {"title": "ViTA: Video Transformer Adaptor for Robust Video Depth Estimation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:VLnqNzywnoUC", "authors": ["Ke Xian", "Juewen Peng", "Zhiguo Cao", "Jianming Zhang", "Guosheng Lin"], "publication_date": "2023/8/28", "journal": "IEEE Transactions on Multimedia", "description": "Depth information plays a pivotal role in numerous computer vision applications, including autonomous driving, 3D reconstruction, and 3D content generation. When deploying depth estimation models in practical applications, it is essential to ensure that the models have strong generalization capabilities. However, existing depth estimation methods primarily concentrate on robust single-image depth estimation, leading to the occurrence of flickering artifacts when applied to video inputs. On the other hand, video depth estimation methods either consume excessive computational resources or lack robustness. To address the above issues, we propose ViTA, a video transformer adaptor, to estimate temporally consistent video depth in the wild. In particular, we leverage a pre-trained image transformer ( i.e. DPT) and introduce additional temporal embeddings in the transformer blocks. Such designs enable our ViTA to \u2026"}, {"title": "Make-It-4D: Synthesizing a Consistent Long-Term Dynamic Scene Video from a Single Image", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:XD-gHx7UXLsC", "authors": ["Liao Shen", "Xingyi Li", "Huiqiang Sun", "Juewen Peng", "Ke Xian", "Zhiguo Cao", "Guosheng Lin"], "publication_date": "2023/8/20", "journal": "arXiv preprint arXiv:2308.10257", "description": "We study the problem of synthesizing a long-term dynamic video from only a single image. This is challenging since it requires consistent visual content movements given large camera motions. Existing methods either hallucinate inconsistent perpetual views or struggle with long camera trajectories. To address these issues, it is essential to estimate the underlying 4D (including 3D geometry and scene motion) and fill in the occluded regions. To this end, we present Make-It-4D, a novel method that can generate a consistent long-term dynamic video from a single image. On the one hand, we utilize layered depth images (LDIs) to represent a scene, and they are then unprojected to form a feature point cloud. To animate the visual content, the feature point cloud is displaced based on the scene flow derived from motion estimation and the corresponding camera pose. Such 4D representation enables our method to maintain the global consistency of the generated dynamic video. On the other hand, we fill in the occluded regions by using a pretrained diffusion model to inpaint and outpaint the input image. This enables our method to work under large camera motions. Benefiting from our design, our method can be training-free which saves a significant amount of training time. Experimental results demonstrate the effectiveness of our approach, which showcases compelling rendering results."}, {"title": "Self-Calibrated Cross Attention Network for Few-Shot Segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:t7zJ5fGR-2UC", "authors": ["Qianxiong Xu", "Wenting Zhao", "Guosheng Lin", "Cheng Long"], "publication_date": "2023/8/18", "journal": "arXiv preprint arXiv:2308.09294", "description": "The key to the success of few-shot segmentation (FSS) lies in how to effectively utilize support samples. Most solutions compress support foreground (FG) features into prototypes, but lose some spatial details. Instead, others use cross attention to fuse query features with uncompressed support FG. Query FG is safely fused with support FG, however, query background (BG) cannot find matched BG features in support FG, yet it inevitably integrates dissimilar features. Besides, as both query FG and BG are combined with support FG, they get entangled, thereby leading to ineffective segmentation. To cope with these issues, we design a self-calibrated cross attention (SCCA) block. For efficient patch-based attention, query and support features are firstly split into patches. Then, we design a patch alignment module to align each query patch with its most similar support patch for better cross attention. Specifically, SCCA takes a query patch as Q, and groups the patches from the same query image and the aligned patches from the support image as K&V. In this way, the query BG features are fused with matched BG features (from query patches), and thus the aforementioned issues will be mitigated. Moreover, when calculating SCCA, we design a scaled-cosine mechanism to better utilize the support features for similarity calculation. Extensive experiments conducted on PASCAL-5^ i and COCO-20^ i demonstrate the superiority of our model, eg, the mIoU score under 5-shot setting on COCO-20^ i is 5.6%+ better than previous state-of-the-arts. The code is available at https://github. com/Sam1224/SCCAN."}, {"title": "Unlimited Knowledge Distillation for Action Recognition in the Dark", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:z_wVstp3MssC", "authors": ["Ruibing Jin", "Guosheng Lin", "Min Wu", "Jie Lin", "Zhengguo Li", "Xiaoli Li", "Zhenghua Chen"], "publication_date": "2023/8/18", "journal": "arXiv preprint arXiv:2308.09327", "description": "Dark videos often lose essential information, which causes the knowledge learned by networks is not enough to accurately recognize actions. Existing knowledge assembling methods require massive GPU memory to distill the knowledge from multiple teacher models into a student model. In action recognition, this drawback becomes serious due to much computation required by video process. Constrained by limited computation source, these approaches are infeasible. To address this issue, we propose an unlimited knowledge distillation (UKD) in this paper. Compared with existing knowledge assembling methods, our UKD can effectively assemble different knowledge without introducing high GPU memory consumption. Thus, the number of teaching models for distillation is unlimited. With our UKD, the network's learned knowledge can be remarkably enriched. Our experiments show that the single stream network distilled with our UKD even surpasses a two-stream network. Extensive experiments are conducted on the ARID dataset."}, {"title": "Weakly Supervised 3D Instance Segmentation without Instance-level Annotations", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:35r97b3x0nAC", "authors": ["Shichao Dong", "Guosheng Lin"], "publication_date": "2023/8/3", "journal": "arXiv preprint arXiv:2308.01721", "description": "3D semantic scene understanding tasks have achieved great success with the emergence of deep learning, but often require a huge amount of manually annotated training data. To alleviate the annotation cost, we propose the first weakly-supervised 3D instance segmentation method that only requires categorical semantic labels as supervision, and we do not need instance-level labels. The required semantic annotations can be either dense or extreme sparse (e.g. 0.02% of total points). Even without having any instance-related ground-truth, we design an approach to break point clouds into raw fragments and find the most confident samples for learning instance centroids. Furthermore, we construct a recomposed dataset using pseudo instances, which is used to learn our defined multilevel shape-aware objectness signal. An asymmetrical object inference algorithm is followed to process core points and boundary points with different strategies, and generate high-quality pseudo instance labels to guide iterative training. Experiments demonstrate that our method can achieve comparable results with recent fully supervised methods. By generating pseudo instance labels from categorical semantic labels, our designed approach can also assist existing methods for learning 3D instance segmentation at reduced annotation cost."}, {"title": "Semantic Consistent Embedding for Domain Adaptive Zero-Shot Learning", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:2KloaMYe4IUC", "authors": ["Jianyang Zhang", "Guowu Yang", "Ping Hu", "Guosheng Lin", "Fengmao Lv"], "publication_date": "2023/7/13", "journal": "IEEE Transactions on Image Processing", "description": "Unsupervised domain adaptation has limitations when encountering label discrepancy between the source and target domains. While open-set domain adaptation approaches can address situations when the target domain has additional categories, these methods can only detect them but not further classify them. In this paper, we focus on a more challenging setting dubbed Domain Adaptive Zero-Shot Learning (DAZSL), which uses semantic embeddings of class tags as the bridge between seen and unseen classes to learn the classifier for recognizing all categories in the target domain when only the supervision of seen categories in the source domain is available. The main challenge of DAZSL is to perform knowledge transfer across categories and domain styles simultaneously. To this end, we propose a novel end-to-end learning mechanism dubbed Three-way Semantic Consistent Embedding (TSCE) to \u2026"}, {"title": "Temporal Feature Matching and Propagation for Semantic Segmentation on 3D Point Cloud Sequences", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:NJ774b8OgUMC", "authors": ["Hanyu Shi", "Ruibo Li", "Fayao Liu", "Guosheng Lin"], "publication_date": "2023/5/9", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "description": "In real-world LiDAR-based applications, data is generated in the form of 3D point cloud sequences or 4D point clouds. However, the topic of semantic segmentation on 4D point clouds is under-investigated and existing methods are still not able to achieve satisfactory performance to meet the requirement for real-world applications. The temporal information across different point clouds plays an important role in dynamic scene understanding, which is not well explored in existing work. In this paper, we focus on exploring effective temporal information across two consecutive point clouds for semantic segmentation on point cloud sequences. To this end, we design three novel modules to enhance the features of target frames by extracting different temporal information in the local regions and global regions. Experimental results on SemanticKITTI and SemanticPOSS demonstrate that our method achieves superior \u2026"}, {"title": "MoDA: Modeling Deformable 3D Objects from Casual Videos", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:uLbwQdceFCQC", "authors": ["Chaoyue Song", "Tianyi Chen", "Yiwen Chen", "Jiacheng Wei", "Chuan Sheng Foo", "Fayao Liu", "Guosheng Lin"], "publication_date": "2023/4/17", "journal": "arXiv preprint arXiv:2304.08279", "description": "In this paper, we focus on the challenges of modeling deformable 3D objects from casual videos. With the popularity of neural radiance fields (NeRF), many works extend it to dynamic scenes with a canonical NeRF and a deformation model that achieves 3D point transformation between the observation space and the canonical space. Recent works rely on linear blend skinning (LBS) to achieve the canonical-observation transformation. However, the linearly weighted combination of rigid transformation matrices is not guaranteed to be rigid. As a matter of fact, unexpected scale and shear factors often appear. In practice, using LBS as the deformation model can always lead to skin-collapsing artifacts for bending or twisting motions. To solve this problem, we propose neural dual quaternion blend skinning (NeuDBS) to achieve 3D point deformation, which can perform rigid transformation without skin-collapsing artifacts. Besides, we introduce a texture filtering approach for texture rendering that effectively minimizes the impact of noisy colors outside target deformable objects. Extensive experiments on real and synthetic datasets show that our approach can reconstruct 3D models for humans and animals with better qualitative and quantitative performance than state-of-the-art methods."}, {"title": "StarNet: Style-Aware 3D Point Cloud Generation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:JQOojiI6XY0C", "authors": ["Yunfan Zhang", "Hao Wang", "Guosheng Lin", "Vun Chan Hua Nicholas", "Zhiqi Shen", "Chunyan Miao"], "publication_date": "2023/3/28", "journal": "arXiv preprint arXiv:2303.15805", "description": "This paper investigates an open research task of reconstructing and generating 3D point clouds. Most existing works of 3D generative models directly take the Gaussian prior as input for the decoder to generate 3D point clouds, which fail to learn disentangled latent codes, leading noisy interpolated results. Most of the GAN-based models fail to discriminate the local geometries, resulting in the point clouds generated not evenly distributed at the object surface, hence degrading the point cloud generation quality. Moreover, prevailing methods adopt computation-intensive frameworks, such as flow-based models and Markov chains, which take plenty of time and resources in the training phase. To resolve these limitations, this paper proposes a unified style-aware network architecture combining both point-wise distance loss and adversarial loss, StarNet which is able to reconstruct and generate high-fidelity and even 3D point clouds using a mapping network that can effectively disentangle the Gaussian prior from input's high-level attributes in the mapped latent space to generate realistic interpolated objects. Experimental results demonstrate that our framework achieves comparable state-of-the-art performance on various metrics in the point cloud reconstruction and generation tasks, but is more lightweight in model size, requires much fewer parameters and less time for model training."}, {"title": "Reliability-Adaptive Consistency Regularization for Weakly-Supervised Point Cloud Segmentation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:1yQoGdGgb4wC", "authors": ["Zhonghua Wu", "Yicheng Wu", "Guosheng Lin", "Jianfei Cai"], "publication_date": "2023/3/9", "journal": "arXiv preprint arXiv:2303.05164", "description": "Weakly-supervised point cloud segmentation with extremely limited labels is highly desirable to alleviate the expensive costs of collecting densely annotated 3D points. This paper explores to apply the consistency regularization that is commonly used in weakly-supervised learning, for its point cloud counterpart with multiple data-specific augmentations, which has not been well studied. We observe that the straightforward way of applying consistency constraints to weakly-supervised point cloud segmentation has two major limitations: noisy pseudo labels due to the conventional confidence-based selection and insufficient consistency constraints due to discarding unreliable pseudo labels. Therefore, we propose a novel Reliability-Adaptive Consistency Network (RAC-Net) to use both prediction confidence and model uncertainty to measure the reliability of pseudo labels and apply consistency training on all unlabeled points while with different consistency constraints for different points based on the reliability of corresponding pseudo labels. Experimental results on the S3DIS and ScanNet-v2 benchmark datasets show that our model achieves superior performance in weakly-supervised point cloud segmentation. The code will be released."}, {"title": "Generalizable Person Re-Identification via Viewpoint Alignment and Fusion", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:VL0QpB8kHFEC", "authors": ["Bingliang Jiao", "Lingqiao Liu", "Liying Gao", "Guosheng Lin", "Ruiqi Wu", "Shizhou Zhang", "Peng Wang", "Yanning Zhang"], "publication_date": "2022/12/5", "journal": "arXiv preprint arXiv:2212.02398", "description": "In the current person Re-identification (ReID) methods, most domain generalization works focus on dealing with style differences between domains while largely ignoring unpredictable camera view change, which we identify as another major factor leading to a poor generalization of ReID methods. To tackle the viewpoint change, this work proposes to use a 3D dense pose estimation model and a texture mapping module to map the pedestrian images to canonical view images. Due to the imperfection of the texture mapping module, the canonical view images may lose the discriminative detail clues from the original images, and thus directly using them for ReID will inevitably result in poor performance. To handle this issue, we propose to fuse the original image and canonical view image via a transformer-based module. The key insight of this design is that the cross-attention mechanism in the transformer could be an ideal solution to align the discriminative texture clues from the original image with the canonical view image, which could compensate for the low-quality texture information of the canonical view image. Through extensive experiments, we show that our method can lead to superior performance over the existing approaches in various evaluation settings."}, {"title": "Point Discriminative Learning for Data-efficient 3D Point Cloud Analysis", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:N5tVd3kTz84C", "authors": ["Fayao Liu", "Guosheng Lin", "Chuan-Sheng Foo", "Chaitanya K Joshi", "Jie Lin"], "publication_date": "2022/9/12", "conference": "2022 International Conference on 3D Vision (3DV)", "description": "3D point cloud analysis has drawn a lot of research attention due to its wide applications. However, collecting massive labelled 3D point cloud data is both time-consuming and labor-intensive. This calls for data-efficient learning methods. In this work we propose PointDisc, a point discriminative learning method to leverage self-supervisions for data-efficient 3D point cloud classification and segmentation. PointDisc imposes a novel point discrimination loss on the middle and global level features produced by the backbone network. This point discrimination loss enforces learned features to be consistent with points belonging to the corresponding local shape region and inconsistent with randomly sampled noisy points. We conduct extensive experiments on 3D object classification, 3D semantic and part segmentation, showing the benefits of PointDisc for data-efficient learning. Detailed analysis demonstrate that \u2026"}, {"title": "Decomposing generation networks with structure prediction for recipe generation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:4fKUyHm3Qg0C", "authors": ["Hao Wang", "Guosheng Lin", "Steven CH Hoi", "Chunyan Miao"], "publication_date": "2022/6/1", "journal": "Pattern Recognition", "description": "Recipe generation from food images and ingredients is a challenging task, which requires the interpretation of the information from another modality. Different from the image captioning task, where the captions usually have one sentence, cooking instructions contain multiple sentences and have obvious structures. To help the model capture the recipe structure and avoid missing some cooking details, we propose a novel framework: Decomposing Generation Networks (DGN) with structure prediction, to get more structured and complete recipe generation outputs. Specifically, we split each cooking instruction into several phases, and assign different sub-generators to each phase. Our approach includes two novel ideas: (i) learning the recipe structures with the global structure prediction component and (ii) producing recipe phases in the sub-generator output component based on the predicted structure. Extensive \u2026"}, {"title": "Motion Context Network for Weakly Supervised Object Detection in Videos", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:M05iB0D1s5AC", "authors": ["Ruibing Jin", "Guosheng Lin", "Changyun Wen", "Jianliang Wang"], "publication_date": "2020/10/9", "journal": "IEEE Signal Processing Letters", "description": "In weakly supervised object detection, most existing approaches are proposed for images. Without box-level annotations, these methods cannot accurately locate objects. Considering an object may show different motion from its surrounding objects or background, we leverage motion information to improve the detection accuracy. However, the motion pattern of an object is complex. Different parts of an object may have different motion patterns, which poses challenges in exploring motion information for object localization. Directly using motion information may degrade the localization performance. To overcome these issues, we propose a Motion Context Network (MC-Net) in this letter. Our method generates motion context features by exploiting neighborhood motion correlation information on moving regions. These motion context features are then incorporated with image information to improve the detection \u2026"}, {"title": "Approximate constraint generation for efficient structured boosting", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=ZudEhvcAAAAJ&cstart=100&pagesize=100&citation_for_view=ZudEhvcAAAAJ:UeHWp8X0CEIC", "authors": ["Guosheng Lin", "Chunhua Shen", "Anton van den Hengel"], "publication_date": "2013/9/15", "conference": "2013 IEEE International Conference on Image Processing", "description": "We propose efficient training methods (SBoost) for totally-corrective boosting based structured learning. The optimization of boosting method for structured learning is more challenging than the structured support vector machine. Basically, we propose smooth and convex formulation for boosting based structured learning, and develop approximate constraint generation together with column generation to solve the optimization with large number of constraints and variables. Because of the convexity and smoothness, the optimization in each generation iteration can be solved efficiently. We demonstrate some structured learning applications in computer vision using SBoost, including invariance learning for digit recognition, object detection and hierarchical image classification."}]}