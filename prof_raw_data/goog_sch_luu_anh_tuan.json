{"goog_sch_url": "https://scholar.google.com/citations?hl=en&user=d6ixOGYAAAAJ", "name": "Luu Anh Tuan", "interests": ["AI", "NLP", "Pretrained Language Model", "Robustness & TrustworthyInformation Extraction & Semantics"], "co_authors_url": [{"name": "Yi Tay", "url": "https://scholar.google.com/citations?user=VBclY_cAAAAJ&hl=en", "aff": "Research Scientist, Google Brain"}, {"name": "Siu Cheung Hui", "url": "https://scholar.google.com/citations?user=d4ZYx6gAAAAJ&hl=en", "aff": "Associate Professor, School of Computer Engineering, Nanyang Technological University, Singapore"}, {"name": "Vijay Prakash Dwivedi", "url": "https://scholar.google.com/citations?user=8MS7iC0AAAAJ&hl=en", "aff": "PhD Student at Nanyang Technological University, Singapore"}, {"name": "See-Kiong Ng", "url": "https://scholar.google.com/citations?user=_wsommYAAAAJ&hl=en", "aff": "Institute of Data Science and School of Computing, National University of Singapore"}, {"name": "Yoshua Bengio", "url": "https://scholar.google.com/citations?user=kukA0LcAAAAJ&hl=en", "aff": "Professor of computer science, University of Montreal, Mila, IVADO, CIFAR"}, {"name": "Xavier Bresson", "url": "https://scholar.google.com/citations?user=9pSK04MAAAAJ&hl=en", "aff": "Assoc Prof of Computer Science, National University of Singapore"}, {"name": "Xinshuai Dong", "url": "https://scholar.google.com/citations?user=A7JyL1sAAAAJ&hl=en", "aff": "Carnegie Mellon University"}, {"name": "Jung-jae Kim", "url": "https://scholar.google.com/citations?user=iMKgkrQAAAAJ&hl=en", "aff": "Institute for Infocomm Research"}, {"name": "Shuohang Wang", "url": "https://scholar.google.com/citations?user=mN-IO6wAAAAJ&hl=en", "aff": "Senior Researcher, Microsoft"}, {"name": "Jin Song Dong", "url": "https://scholar.google.com/citations?user=tuLa1AsAAAAJ&hl=en", "aff": "Professor of Computer Science, National University of Singapore"}, {"name": "Yang Liu", "url": "https://scholar.google.com/citations?user=_Pvgwd0AAAAJ&hl=en", "aff": "Nanyang Technological University"}, {"name": "Darsh J Shah", "url": "https://scholar.google.com/citations?user=XscFiX8AAAAJ&hl=en", "aff": "Massachusetts Institute of Technology"}, {"name": "Sun Jun", "url": "https://scholar.google.com/citations?user=DVsEyn0AAAAJ&hl=en", "aff": "Professor of SCIS, SMU"}], "citation_table": {"columns": ["All", "Since 2018"], "Citations": [4418, 4253], "h-index": [32, 31], "i10-index": [52, 50]}, "citation_graph": {"2013": 13, "2014": 15, "2015": 27, "2016": 40, "2017": 49, "2018": 252, "2019": 470, "2020": 654, "2021": 828, "2022": 982, "2023": 1058}, "articles": [{"title": "Benchmarking graph neural networks", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:J-pR_7NvFogC", "authors": ["Vijay Prakash Dwivedi", "Chaitanya K Joshi", "Luu Anh Tuan", "Thomas Laurent", "Yoshua Bengio", "Xavier Bresson"], "publication_date": "2023", "journal": "Journal of Machine Learning Research", "description": "In the last few years, graph neural networks (GNNs) have become the standard toolkit for analyzing and learning from data on graphs. This emerging field has witnessed an extensive growth of promising techniques that have been applied with success to computer science, mathematics, biology, physics and chemistry. But for any successful field to become mainstream and reliable, benchmarks must be developed to quantify progress. This led us in March 2020 to release a benchmark framework that i) comprises of a diverse collection of mathematical and real-world graphs, ii) enables fair model comparison with the same parameter budget to identify key architectures, iii) has an open-source, easy-to-use and reproducible code infrastructure, and iv) is flexible for researchers to experiment with new theoretical ideas. As of December 2022, the GitHub repository 1 has reached 2,000 stars and 380 forks, which demonstrates the utility of the proposed open-source framework through the wide usage by the GNN community. In this paper, we present an updated version of our benchmark with a concise presentation of the aforementioned framework characteristics, an additional medium-sized molecular dataset AQSOL, similar to the popular ZINC, but with a real-world measured chemical target, and discuss how this framework can be leveraged to explore new GNN designs and insights. As a proof of value of our benchmark, we study the case of graph positional encoding (PE) in GNNs, which was introduced with this benchmark and has since spurred interest of exploring more powerful PE for Transformers and GNNs in a robust experimental setting.", "total_citations": 720, "citation_graph": {"2020": 79, "2021": 172, "2022": 252, "2023": 214}}, {"title": "Latent Relational Metric Learning via Memory-based Attention for Collaborative Ranking", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:M3NEmzRMIkIC", "authors": ["Yi Tay", "Luu Anh Tuan", "Siu Cheung Hui"], "publication_date": "2018", "conference": "WWW 2018", "description": "This paper proposes a new neural architecture for collaborative ranking with implicit feedback. Our model, LRML (Latent Relational Metric Learning) is a novel metric learning approach for recommendation. More specifically, instead of simple push-pull mechanisms between user and item pairs, we propose to learn latent relations that describe each user item interaction. This helps to alleviate the potential geometric inflexibility of existing metric learning approaches. This enables not only better performance but also a greater extent of modeling capability, allowing our model to scale to a larger number of interactions. In order to do so, we employ a augmented memory module and learn to attend over these memory blocks to construct latent relations. The memory-based attention module is controlled by the user-item interaction, making the learned relation vector specific to each user-item pair. Hence, this can be \u2026", "total_citations": 308, "citation_graph": {"2018": 23, "2019": 48, "2020": 70, "2021": 67, "2022": 57, "2023": 42}}, {"title": "Multi-Pointer Co-Attention Networks for Recommendation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:blknAaTinKkC", "authors": ["Yi Tay", "Luu Anh Tuan", "Siu Cheung Hui"], "publication_date": "2018", "conference": "KDD 2018", "description": "Many recent state-of-the-art recommender systems such as D-ATT, TransNet and DeepCoNN exploit reviews for representation learning. This paper proposes a new neural architecture for recommendation with reviews. Our model operates on a multi-hierarchical paradigm and is based on the intuition that not all reviews are created equal, i.e., only a selected few are important. The importance, however, should be dynamically inferred depending on the current target. To this end, we propose a review-by-review pointer-based learning scheme that extracts important reviews from user and item reviews and subsequently matches them in a word-by-word fashion. This enables not only the most informative reviews to be utilized for prediction but also a deeper word-level interaction. Our pointer-based method operates with a gumbel-softmax based pointer mechanism that enables the incorporation of discrete vectors \u2026", "total_citations": 305, "citation_graph": {"2018": 17, "2019": 50, "2020": 58, "2021": 69, "2022": 68, "2023": 41}}, {"title": "Learning to Attend via Word-Aspect Associative Fusion for Aspect-based Sentiment Analysis", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:k_IJM867U9cC", "authors": ["Yi Tay", "Anh Tuan Luu", "Siu Cheung Hui"], "publication_date": "2017/12/14", "conference": "AAAI 2018", "description": "Aspect-based sentiment analysis (ABSA) tries to predict the polarity of a given document with respect to a given aspect entity. While neural network architectures have been successful in predicting the overall polarity of sentences, aspect-specific sentiment analysis still remains as an open problem. In this paper, we propose a novel method for integrating aspect information into the neural model. More specifically, we incorporate aspect information into the neural model by modeling word-aspect relationships. Our novel model, Aspect Fusion LSTM (AF-LSTM) learns to attend based on associative relationships between sentence words and aspect which allows our model to adaptively focus on the correct words given an aspect term. This ameliorates the flaws of other state-of-the-art models that utilize naive concatenations to model word-aspect similarity. Instead, our model adopts circular convolution and circular correlation to model the similarity between aspect and words and elegantly incorporates this within a differentiable neural attention framework. Finally, our model is end-to-end differentiable and highly related to convolution-correlation (holographic like) memories. Our proposed neural model achieves state-of-the-art performance on benchmark datasets, outperforming ATAE-LSTM by 4%-5% on average across multiple datasets.", "total_citations": 180, "citation_graph": {"2018": 10, "2019": 33, "2020": 42, "2021": 29, "2022": 35, "2023": 31}}, {"title": "Non-Parametric Estimation of Multiple Embeddings for Link Prediction on Dynamic Knowledge Graphs", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:TQgYirikUcIC", "authors": ["Yi Tay", "Luu Anh Tuan", "Siu Cheung Hui"], "publication_date": "2017", "conference": "Proceedings of the AAAI Conference on Artificial Intelligence", "description": "Knowledge graphs play a significant role in many intelligent systems such as semantic search and recommendation systems. Recent works in this area of knowledge graph embeddings such as TransE, TransH and TransR have shown extremely competitive and promising results in relational learning. In this paper, we propose a novel extension of the translational embedding model to solve three main problems of the current models. Firstly, translational models are highly sensitive to hyperparameters such as margin and learning rate. Secondly, the translation principle only allows one spot in vector space for each golden triplet. Thus, congestion of entities and relations in vector space may reduce precision. Lastly, the current models are not able to handle dynamic data especially the introduction of new unseen entities/relations or removal of triplets. In this paper, we propose Parallel Universe TransE (puTransE), an adaptable and robust adaptation of the translational model. Our approach non-parametrically estimates the energy score of a triplet from multiple embedding spaces of structurally and semantically aware triplet selection. Our proposed approach is simple, robust and parallelizable. Our experimental results show that our proposed approach outperforms TransE and many other embedding methods for link prediction on knowledge graphs on both public benchmark dataset and a real world dynamic dataset.", "total_citations": 178, "citation_graph": {"2017": 9, "2018": 13, "2019": 22, "2020": 34, "2021": 35, "2022": 31, "2023": 32}}, {"title": "Compare, compress and propagate: Enhancing neural architectures with alignment factorization for natural language inference", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:maZDTaKrznsC", "authors": ["Yi Tay", "Luu Anh Tuan", "Siu Cheung Hui"], "publication_date": "2018", "journal": "EMNLP 2018", "description": "This paper presents a new deep learning architecture for Natural Language Inference (NLI). Firstly, we introduce a new architecture where alignment pairs are compared, compressed and then propagated to upper layers for enhanced representation learning. Secondly, we adopt factorization layers for efficient and expressive compression of alignment vectors into scalar features, which are then used to augment the base word representations. The design of our approach is aimed to be conceptually simple, compact and yet powerful. We conduct experiments on three popular benchmarks, SNLI, MultiNLI and SciTail, achieving competitive performance on all. A lightweight parameterization of our model also enjoys a times reduction in parameter size compared to the existing state-of-the-art models, e.g., ESIM and DIIN, while maintaining competitive performance. Additionally, visual analysis shows that our propagated features are highly interpretable.", "total_citations": 156, "citation_graph": {"2017": 1, "2018": 24, "2019": 43, "2020": 32, "2021": 30, "2022": 17, "2023": 9}}, {"title": "Reasoning with Sarcasm by Reading In-between", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:GnPB-g6toBAC", "authors": ["Yi Tay", "Luu Anh Tuan", "Siu Cheung Hui", "Jian Su"], "publication_date": "2018", "conference": "ACL 2018", "description": "Sarcasm is a sophisticated speech act which commonly manifests on social communities such as Twitter and Reddit. The prevalence of sarcasm on the social web is highly disruptive to opinion mining systems due to not only its tendency of polarity flipping but also usage of figurative language. Sarcasm commonly manifests with a contrastive theme either between positive-negative sentiments or between literal-figurative scenarios. In this paper, we revisit the notion of modeling contrast in order to reason with sarcasm. More specifically, we propose an attention-based neural model that looks in-between instead of across, enabling it to explicitly model contrast and incongruity. We conduct extensive experiments on six benchmark datasets from Twitter, Reddit and the Internet Argument Corpus. Our proposed model not only achieves state-of-the-art performance on all datasets but also enjoys improved interpretability.", "total_citations": 141, "citation_graph": {"2018": 5, "2019": 13, "2020": 33, "2021": 28, "2022": 37, "2023": 25}}, {"title": "Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:hMod-77fHWUC", "authors": ["Yi Tay", "Luu Anh Tuan", "Siu Cheung Hui"], "publication_date": "2018/2/2", "conference": "Proceedings of the International Conference on Web Search and Data Mining (WSDM)", "description": "The dominant neural architectures in question answer retrieval are based on recurrent or convolutional encoders configured with complex word matching layers. Given that recent architectural innovations are mostly new word interaction layers or attention-based matching mechanisms, it seems to be a well-established fact that these components are mandatory for good performance. Unfortunately, the memory and computation cost incurred by these complex mechanisms are undesirable for practical applications. As such, this paper tackles the question of whether it is possible to achieve competitive performance with simple neural architectures. We propose a simple but novel deep learning architecture for fast and efficient question-answer ranking and retrieval. More specifically, our proposed model, HyperQA, is a parameter efficient neural network that outperforms other parameter intensive models such as \u2026", "total_citations": 132, "citation_graph": {"2018": 14, "2019": 20, "2020": 31, "2021": 31, "2022": 19, "2023": 16}}, {"title": "Graph Neural Networks with Learnable Structural and Positional Representations", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:uWQEDVKXjbEC", "authors": ["Vijay Prakash Dwivedi", "Anh Tuan Luu", "Thomas Laurent", "Yoshua Bengio", "Xavier Bresson"], "publication_date": "2022", "journal": "ICLR 2022", "description": "Graph neural networks (GNNs) have become the standard learning architectures for graphs. GNNs have been applied to numerous domains ranging from quantum chemistry, recommender systems to knowledge graphs and natural language processing. A major issue with arbitrary graphs is the absence of canonical positional information of nodes, which decreases the representation power of GNNs to distinguish e.g. isomorphic nodes and other graph symmetries. An approach to tackle this issue is to introduce Positional Encoding (PE) of nodes, and inject it into the input layer, like in Transformers. Possible graph PE are Laplacian eigenvectors. In this work, we propose to decouple structural and positional representations to make easy for the network to learn these two essential properties. We introduce a novel generic architecture which we call LSPE (Learnable Structural and Positional Encodings). We investigate several sparse and fully-connected (Transformer-like) GNNs, and observe a performance increase for molecular datasets, from 1.79% up to 64.14% when considering learnable PE for both GNN classes.", "total_citations": 130, "citation_graph": {"2020": 2, "2021": 0, "2022": 49, "2023": 79}}, {"title": "Recipe for a General, Powerful, Scalable Graph Transformer", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:5ugPr518TE4C", "authors": ["Ladislav Ramp\u00e1\u0161ek", "Mikhail Galkin", "Vijay Prakash Dwivedi", "Anh Tuan Luu", "Guy Wolf", "Dominique Beaini"], "publication_date": "2022/5/25", "journal": "NeurIPS 2022", "description": "We propose a recipe on how to build a general, powerful, scalable (GPS) graph Transformer with linear complexity and state-of-the-art results on a diverse set of benchmarks. Graph Transformers (GTs) have gained popularity in the field of graph representation learning with a variety of recent publications but they lack a common foundation about what constitutes a good positional or structural encoding, and what differentiates them. In this paper, we summarize the different types of encodings with a clearer definition and categorize them as being , or . The prior GTs are constrained to small graphs with a few hundred nodes, here we propose the first architecture with a complexity linear in the number of nodes and edges by decoupling the local real-edge aggregation from the fully-connected Transformer. We argue that this decoupling does not negatively affect the expressivity, with our architecture being a universal function approximator on graphs. Our GPS recipe consists of choosing 3 main ingredients:(i) positional/structural encoding,(ii) local message-passing mechanism, and (iii) global attention mechanism. We provide a modular framework that supports multiple types of encodings and that provides efficiency and scalability both in small and large graphs. We test our architecture on 16 benchmarks and show highly competitive results in all of them, show-casing the empirical benefits gained by the modularity and the combination of different strategies.", "total_citations": 129, "citation_graph": {"2022": 13, "2023": 116}}, {"title": "Learning to Rank Question Answer Pairs with Holographic Dual LSTM Architecture", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:RHpTSmoSYBkC", "authors": ["Yi Tay", "Minh C Phan", "Luu Anh Tuan", "Siu Cheung Hui"], "publication_date": "2017/7/20", "conference": "SIGIR 2017", "description": "We describe a new deep learning architecture for learning to rank question answer pairs. Our approach extends the long short-term memory (LSTM) network with holographic composition to model the relationship between question and answer representations. As opposed to the neural tensor layer that has been adopted recently, the holographic composition provides the benefits of scalable and rich representational learning approach without incurring huge parameter costs. Overall, we present Holographic Dual LSTM (HD-LSTM), a unified architecture for both deep sentence modeling and semantic matching. Essentially, our model is trained end-to-end whereby the parameters of the LSTM are optimized in a way that best explains the correlation between question and answer representations. In addition, our proposed deep learning architecture requires no extensive feature engineering. Via extensive \u2026", "total_citations": 129, "citation_graph": {"2017": 5, "2018": 21, "2019": 27, "2020": 29, "2021": 27, "2022": 16, "2023": 4}}, {"title": "NURBS-based postbuckling analysis of functionally graded carbon nanotube-reinforced composite shells", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:V3AGJWp-ZtQC", "authors": ["Tan N Nguyen", "Chien H Thai", "Anh-Tuan Luu", "H Nguyen-Xuan", "Jaehong Lee"], "publication_date": "2019/4/15", "journal": "Computer Methods in Applied Mechanics and Engineering", "description": "An investigation into the postbuckling and geometrically nonlinear behaviors of functionally graded carbon nanotube-reinforced composite (FG-CNTRC) shells is carried out in this study. The discrete nonlinear equation system is established based on non-uniform rational B-Spline (NURBS) basis functions and the first-order shear deformation shell theory (FSDT). The nonlinearity of shells is formed in the Total Lagrangian approach considering the von Karman assumption. The incremental solutions are obtained by using a modified Riks method. In the present formulation, the rule of mixture is used to estimate the effective material properties of FG-CNTRC shells. Effects of CNTs distribution, volume fraction and CNTs orientation on the postbuckling behavior of FG-CNTRC shells are particularly investigated. Exact geometries of shells are modeled by using NURBS interpolation. Several verifications are given to show \u2026", "total_citations": 122, "citation_graph": {"2019": 9, "2020": 29, "2021": 32, "2022": 28, "2023": 22}}, {"title": "SkipFlow: Incorporating Neural Coherence Features for End-to-End Automatic Text Scoring", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:TFP_iSt0sucC", "authors": ["Yi Tay", "Minh C Phan", "Luu Anh Tuan", "Siu Cheung Hui"], "publication_date": "2017/11/14", "conference": "AAAI 2018", "description": "Deep learning has demonstrated tremendous potential for Automatic Text Scoring (ATS) tasks. In this paper, we describe a new neural architecture that enhances vanilla neural network models with auxiliary neural coherence features. Our new method proposes a new SkipFlow mechanism that models relationships between snapshots of the hidden representations of a long short-term memory (LSTM) network as it reads. Subsequently, the semantic relationships between multiple snapshots are used as auxiliary features for prediction. This has two main benefits. Firstly, essays are typically long sequences and therefore the memorization capability of the LSTM network may be insufficient. Implicit access to multiple snapshots can alleviate this problem by acting as a protection against vanishing gradients. The parameters of the SkipFlow mechanism also acts as an auxiliary memory. Secondly, modeling relationships between multiple positions allows our model to learn features that represent and approximate textual coherence. In our model, we call this neural coherence features. Overall, we present a unified deep learning architecture that generates neural coherence features as it reads in an end-to-end fashion. Our approach demonstrates state-of-the-art performance on the benchmark ASAP dataset, outperforming not only feature engineering baselines but also other deep learning models.", "total_citations": 120, "citation_graph": {"2018": 8, "2019": 11, "2020": 20, "2021": 34, "2022": 26, "2023": 21}}, {"title": "Dyadic Memory Networks for Aspect-based Sentiment Analysis", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:iH-uZ7U-co4C", "authors": ["Yi Tay", "Luu Anh Tuan", "Siu Cheung Hui"], "publication_date": "2017", "conference": "CIKM 2017", "description": "This paper proposes Dyadic Memory Networks (DyMemNN), a novel extension of end-to-end memory networks (memNN) for aspect-based sentiment analysis (ABSA). Originally designed for question answering tasks, memNN operates via a memory selection operation in which relevant memory pieces are adaptively selected based on the input query. In the problem of ABSA, this is analogous to aspects and documents in which the relationship between each word in the document is compared with the aspect vector. In the standard memory networks, simple dot products or feed forward neural networks are used to model the relationship between aspect and words which lacks representation learning capability. As such, our dyadic memory networks ameliorates this weakness by enabling rich dyadic interactions between aspect and word embeddings by integrating either parameterized neural tensor compositions \u2026", "total_citations": 96, "citation_graph": {"2018": 16, "2019": 19, "2020": 22, "2021": 15, "2022": 15, "2023": 9}}, {"title": "Learning Term Embeddings for Taxonomic Relation Identification Using Dynamic Weighting Neural Network", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:-f6ydRqryjwC", "authors": ["Luu Anh Tuan", "Yi Tay", "Siu Cheung Hui", "See Kiong Ng"], "publication_date": "2016", "conference": "Conference on Empirical Methods on Natural Language Processing (EMNLP)", "description": "Taxonomic relation identification aims to recognize the \u2018is-a\u2019relation between two terms. Previous works on identifying taxonomic relations are mostly based on statistical and linguistic approaches, but the accuracy of these approaches is far from satisfactory. In this paper, we propose a novel supervised learning approach for identifying taxonomic relations using term embeddings. For this purpose, we first design a dynamic weighting neural network to learn term embeddings based on not only the hypernym and hyponym terms, but also the contextual information between them. We then apply such embeddings as features to identify taxonomic relations using a supervised method. The experimental results show that our proposed approach significantly outperforms other state-of-the-art methods by 9% to 13% in terms of accuracy for both general and specific domain datasets.", "total_citations": 93, "citation_graph": {"2016": 1, "2017": 6, "2018": 16, "2019": 16, "2020": 18, "2021": 17, "2022": 10, "2023": 9}}, {"title": "Towards Robustness Against Natural Language Word Substitutions", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:f2IySw72cVMC", "authors": ["Xinshuai Dong", "Anh Tuan Luu", "Rongrong Ji", "Hong Liu"], "publication_date": "2021", "journal": "ICLR 2021", "description": "Robustness against word substitutions has a well-defined and widely acceptable form, i.e., using semantically similar words as substitutions, and thus it is considered as a fundamental stepping-stone towards broader robustness in natural language processing. Previous defense methods capture word substitutions in vector space by using either -ball or hyper-rectangle, which results in perturbation sets that are not inclusive enough or unnecessarily large, and thus impedes mimicry of worst cases for robust training. In this paper, we introduce a novel \\textit{Adversarial Sparse Convex Combination} (ASCC) method. We model the word substitution attack space as a convex hull and leverages a regularization term to enforce perturbation towards an actual substitution, thus aligning our modeling better with the discrete textual space. Based on the ASCC method, we further propose ASCC-defense, which leverages ASCC to generate worst-case perturbations and incorporates adversarial training towards robustness. Experiments show that ASCC-defense outperforms the current state-of-the-arts in terms of robustness on two prevailing NLP tasks, \\emph{i.e.}, sentiment analysis and natural language inference, concerning several attacks across multiple model architectures. Besides, we also envision a new class of defense towards robustness in NLP, where our robustly trained word vectors can be plugged into a normally trained model and enforce its robustness without applying any other defense techniques.", "total_citations": 85, "citation_graph": {"2020": 1, "2021": 11, "2022": 37, "2023": 36}}, {"title": "Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:HoB7MX3m0LUC", "authors": ["Yi Tay", "Shuohang Wang", "Luu Anh Tuan", "Jie Fu", "Minh C Phan", "Xingdi Yuan", "Jinfeng Rao", "Siu Cheung Hui", "Aston Zhang"], "publication_date": "2019/5/26", "conference": "ACL 2019", "description": "This paper tackles the problem of reading comprehension over long narratives where documents easily span over thousands of tokens. We propose a curriculum learning (CL) based Pointer-Generator framework for reading/sampling over large documents, enabling diverse training of the neural model based on the notion of alternating contextual difficulty. This can be interpreted as a form of domain randomization and/or generative pretraining during training. To this end, the usage of the Pointer-Generator softens the requirement of having the answer within the context, enabling us to construct diverse training samples for learning. Additionally, we propose a new Introspective Alignment Layer (IAL), which reasons over decomposed alignments using block-based self-attention. We evaluate our proposed method on the NarrativeQA reading comprehension benchmark, achieving state-of-the-art performance, improving existing baselines by relative improvement on BLEU-4 and relative improvement on Rouge-L. Extensive ablations confirm the effectiveness of our proposed IAL and CL components.", "total_citations": 84, "citation_graph": {"2019": 5, "2020": 3, "2021": 19, "2022": 29, "2023": 28}}, {"title": "Multi-cast attention networks", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:J_g5lzvAfSwC", "authors": ["Yi Tay", "Luu Anh Tuan", "Siu Cheung Hui"], "publication_date": "2018/7/19", "book": "Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining", "description": "Attention is typically used to select informative sub-phrases that are used for prediction. This paper investigates the novel use of attention as a form of feature augmentation, i.e, casted attention. We propose Multi-Cast Attention Networks (MCAN), a new attention mechanism and general model architecture for a potpourri of ranking tasks in the conversational modeling and question answering domains. Our approach performs a series of soft attention operations, each time casting a scalar feature upon the inner word embeddings. The key idea is to provide a real-valued hint (feature) to a subsequent encoder layer and is targeted at improving the representation learning process. There are several advantages to this design, e.g., it allows an arbitrary number of attention mechanisms to be casted, allowing for multiple attention types (e.g., co-attention, intra-attention) and attention variants (e.g., alignment-pooling, max \u2026", "total_citations": 78, "citation_graph": {"2018": 5, "2019": 15, "2020": 14, "2021": 20, "2022": 19, "2023": 5}}, {"title": "Lightweight and Efficient Neural Natural Language Processing with Quaternion Networks", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:ZHo1McVdvXMC", "authors": ["Yi Tay", "Aston Zhang", "Luu Anh Tuan", "Jinfeng Rao", "Shuai Zhang", "Shuohang Wang", "Jie Fu", "Siu Cheung Hui"], "publication_date": "2019/6/11", "conference": "ACL 2019", "description": "Many state-of-the-art neural models for NLP are heavily parameterized and thus memory inefficient. This paper proposes a series of lightweight and memory efficient neural architectures for a potpourri of natural language processing (NLP) tasks. To this end, our models exploit computation using Quaternion algebra and hypercomplex spaces, enabling not only expressive inter-component interactions but also significantly () reduced parameter size due to lesser degrees of freedom in the Hamilton product. We propose Quaternion variants of models, giving rise to new architectures such as the Quaternion attention Model and Quaternion Transformer. Extensive experiments on a battery of NLP tasks demonstrates the utility of proposed Quaternion-inspired models, enabling up to reduction in parameter size without significant loss in performance.", "total_citations": 67, "citation_graph": {"2019": 4, "2020": 12, "2021": 18, "2022": 16, "2023": 16}}, {"title": "Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with Parameters", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:pyW8ca7W8N0C", "authors": ["Aston Zhang", "Yi Tay", "Shuai Zhang", "Alvin Chan", "Anh Tuan Luu", "Siu Cheung Hui", "Jie Fu"], "publication_date": "2021/2/17", "journal": "ICLR 2021", "description": "Recent works have demonstrated reasonable success of representation learning in hypercomplex space. Specifically, \"fully-connected layers with Quaternions\" (4D hypercomplex numbers), which replace real-valued matrix multiplications in fully-connected layers with Hamilton products of Quaternions, both enjoy parameter savings with only 1/4 learnable parameters and achieve comparable performance in various applications. However, one key caveat is that hypercomplex space only exists at very few predefined dimensions (4D, 8D, and 16D). This restricts the flexibility of models that leverage hypercomplex multiplications. To this end, we propose parameterizing hypercomplex multiplications, allowing models to learn multiplication rules from data regardless of whether such rules are predefined. As a result, our method not only subsumes the Hamilton product, but also learns to operate on any arbitrary nD hypercomplex space, providing more architectural flexibility using arbitrarily learnable parameters compared with the fully-connected layer counterpart. Experiments of applications to the LSTM and Transformer models on natural language inference, machine translation, text style transfer, and subject verb agreement demonstrate architectural flexibility and effectiveness of the proposed approach.", "total_citations": 66, "citation_graph": {"2021": 11, "2022": 24, "2023": 31}}, {"title": "Densely connected attention propagation for reading comprehension", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:2P1L_qKh6hAC", "authors": ["Yi Tay", "Luu Anh Tuan", "Siu Cheung Hui", "Jian Su"], "publication_date": "2018/11/10", "journal": "Proceedings of NeurIPS 2018", "description": "We propose DecaProp (Densely Connected Attention Propagation), a new densely connected neural architecture for reading comprehension (RC). There are two distinct characteristics of our model. Firstly, our model densely connects all pairwise layers of the network, modeling relationships between passage and query across all hierarchical levels. Secondly, the dense connectors in our network are learned via attention instead of standard residual skip-connectors. To this end, we propose novel Bidirectional Attention Connectors (BAC) for efficiently forging connections throughout the network. We conduct extensive experiments on four challenging RC benchmarks. Our proposed approach achieves state-of-the-art results on all four, outperforming existing baselines by up to 2.6% to 14.2% in absolute F1 score.", "total_citations": 61, "citation_graph": {"2018": 1, "2019": 17, "2020": 16, "2021": 13, "2022": 10, "2023": 3}}, {"title": "Taxonomy Construction Using Syntactic Contextual Evidence", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:IWHjjKOFINEC", "authors": ["Luu Anh Tuan", "Jung-jae Kim", "See-Kiong Ng"], "publication_date": "2014", "conference": "Conference on Empirical Methods on Natural Language Processing (EMNLP)", "description": "Taxonomies are the backbone of many structured, semantic knowledge resources. Recent works for extracting taxonomic relations from text focused on collecting lexical-syntactic patterns to extract the taxonomic relations by matching the patterns to text. These approaches, however, often show low coverage due to the lack of contextual analysis across sentences. To address this issue, we propose a novel approach that collectively utilizes contextual information of terms in syntactic structures such that if the set of contexts of a term includes most of contexts of another term, a subsumption relation between the two terms is inferred. We apply this method to the task of taxonomy construction from scratch, where we introduce another novel graph-based algorithm for taxonomic structure induction. Our experiment results show that the proposed method is well complementary with previous methods of linguistic pattern matching and significantly improves recall and thus F-measure.", "total_citations": 60, "citation_graph": {"2015": 6, "2016": 14, "2017": 5, "2018": 8, "2019": 7, "2020": 6, "2021": 4, "2022": 4, "2023": 6}}, {"title": "Modeling and verification of safety critical systems: A case study on pacemaker", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:d1gkVwhDpl0C", "authors": ["Luu Anh Tuan", "Man Chun Zheng", "Quan Thanh Tho"], "publication_date": "2010/6/9", "conference": "2010 Fourth International Conference on Secure Software Integration and Reliability Improvement", "description": "The pacemaker challenge proposed by Software Quality Research Laboratory is looking for formal methods to produce precise and reliable systems. Safety critical systems like pacemaker need to guarantee important properties (like deadlock-free, safety, etc.), which concern human lives. Formal methods have been applied in designing safety critical systems with verified desirable properties. In this paper, we propose a formal model of pacemaker, modeling its behaviors and its communication with the external environment, using a real-time formalism. Critical properties, such as deadlock freeness and heart rate limits are then verified using the model checker PAT(Process Analysis Toolkit). This work yields a verified formal model of pacemaker systems, which can serve as specification for real pacemaker implementations.", "total_citations": 59, "citation_graph": {"2009": 1, "2010": 0, "2011": 3, "2012": 5, "2013": 8, "2014": 6, "2015": 4, "2016": 3, "2017": 2, "2018": 8, "2019": 6, "2020": 2, "2021": 4, "2022": 2, "2023": 4}}, {"title": "Capturing Greater Context for Question Generation", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:u_35RYKgDlwC", "authors": ["Luu Anh Tuan", "Darsh J Shah", "Regina Barzilay"], "publication_date": "2020", "conference": "AAAI 2020", "description": "Automatic question generation can benefit many applications ranging from dialogue systems to reading comprehension. While questions are often asked with respect to long documents, there are many challenges with modeling such long documents. Many existing techniques generate questions by effectively looking at one sentence at a time, leading to questions that are easy and not reflective of the human process of question generation. Our goal is to incorporate interactions across multiple sentences to generate realistic questions for long documents. In order to link a broad document context to the target answer, we represent the relevant context via a multi-stage attention mechanism, which forms the foundation of a sequence to sequence model. We outperform state-of-the-art methods on question generation on three question-answering datasets-SQuAD, MS MARCO and NewsQA. 1", "total_citations": 58, "citation_graph": {"2020": 8, "2021": 16, "2022": 25, "2023": 9}}, {"title": "Overview of the gene ontology task at BioCreative IV", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:ZeXyd9-uunAC", "authors": ["Yuqing Mao", "Kimberly Van Auken", "Donghui Li", "Cecilia N. Arighi", "Peter McQuilton", "G. Thomas Hayman", "Susan Tweedie", "Mary L. Schaeffer", "Stanley J. F. Laulederkind", "Shur-Jen Wang", "Julien Gobeill", "Patrick Ruch", "Anh Tuan Luu", "Jung-jae Kim", "Jung-Hsien Chiang", "Yu-De Chen", "Chia-Jung Yang", "Hongfang Liu", "Dongqing Zhu", "Yanpeng Li", "Hong Yu", "Ehsan Emadzadeh", "Graciela Gonzalez", "Jian-Ming Chen", "Hong-Jie Dai", "Zhiyong Lu"], "publication_date": "2014", "journal": "Database Oxford Journals", "description": "Gene Ontology (GO) annotation is a common task among model organism databases (MODs) for capturing gene function data from journal articles. It is a time-consuming and labor-intensive task, and is thus often considered as one of the bottlenecks in literature curation. There is a growing need for semiautomated or fully automated GO curation techniques that will help database curators to rapidly and accurately identify gene function information in full-length articles. Despite multiple attempts in the past, few studies have proven to be useful with regard to assisting real-world GO curation. The shortage of sentence-level training data and opportunities for interaction between text-mining developers and GO curators has limited the advances in algorithm development and corresponding use in practical circumstances. To this end, we organized a text-mining challenge task for literature-based GO annotation in \u2026", "total_citations": 53, "citation_graph": {"2014": 2, "2015": 13, "2016": 13, "2017": 6, "2018": 7, "2019": 2, "2020": 5, "2021": 4, "2022": 0, "2023": 1}}, {"title": "Long Range Graph Benchmark", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:bnK-pcrLprsC", "authors": ["Vijay Prakash Dwivedi", "Ladislav Ramp\u00e1\u0161ek", "Mikhail Galkin", "Ali Parviz", "Guy Wolf", "Anh Tuan Luu", "Dominique Beaini"], "publication_date": "2022/6/16", "journal": "NeurIPS 2022", "description": "Graph Neural Networks (GNNs) that are based on the message passing (MP) paradigm generally exchange information between 1-hop neighbors to build node representations at each layer. In principle, such networks are not able to capture long-range interactions (LRI) that may be desired or necessary for learning a given task on graphs. Recently, there has been an increasing interest in development of Transformer-based methods for graphs that can consider full node connectivity beyond the original sparse structure, thus enabling the modeling of LRI. However, MP-GNNs that simply rely on 1-hop message passing often fare better in several existing graph benchmarks when combined with positional feature representations, among other innovations, hence limiting the perceived utility and ranking of Transformer-like architectures. Here, we present the Long Range Graph Benchmark (LRGB) with 5 graph learning datasets: , , , and that arguably require LRI reasoning to achieve strong performance in a given task. We benchmark both baseline GNNs and Graph Transformer networks to verify that the models which capture long-range dependencies perform significantly better on these tasks. Therefore, these datasets are suitable for benchmarking and exploration of MP GNNs and Graph Transformer architectures that are intended to capture LRI.", "total_citations": 52, "citation_graph": {"2022": 5, "2023": 47}}, {"title": "Hermitian Co-Attention Networks for Text Matching in Asymmetrical Domains.", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:yD5IFk8b50cC", "authors": ["Yi Tay", "Anh Tuan Luu", "Siu Cheung Hui"], "publication_date": "2018/7/13", "journal": "IJCAI", "description": "Co-Attentions are highly effective attention mechanisms for text matching applications. Co-Attention enables the learning of pairwise attentions, ie, learning to attend based on computing word-level affinity scores between two documents. However, text matching problems can exist in either symmetrical or asymmetrical domains. For example, paraphrase identification is a symmetrical task while question-answer matching and entailment classification are considered asymmetrical domains. In this paper, we argue that Co-Attention models in asymmetrical domains require different treatment as opposed to symmetrical domains, ie, a concept of word-level directionality should be incorporated while learning word-level similarity scores. Hence, the standard inner product in real space commonly adopted in co-attention is not suitable. This paper leverages attractive properties of the complex vector space and proposes a co-attention mechanism based on the complex-valued inner product (Hermitian products). Unlike the real dot product, the dot product in complex space is asymmetric because the first item is conjugated. Aside from modeling and encoding directionality, our proposed approach also enhances the representation learning process. Extensive experiments on five text matching benchmark datasets demonstrate the effectiveness of our approach.", "total_citations": 48, "citation_graph": {"2018": 3, "2019": 7, "2020": 11, "2021": 14, "2022": 10, "2023": 3}}, {"title": "Cross Temporal Recurrent Networks for Ranking Question Answer Pairs", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:isC4tDSrTZIC", "authors": ["Yi Tay", "Luu Anh Tuan", "Siu Cheung Hui"], "publication_date": "2018", "conference": "AAAI 2018", "description": "Temporal gates play a significant role in modern recurrent-based neural encoders, enabling fine-grained control over recursive compositional operations over time. In recurrent models such as the long short-term memory (LSTM), temporal gates control the amount of information retained or discarded over time, not only playing an important role in influencing the learned representations but also serving as a protection against vanishing gradients. This paper explores the idea of learning temporal gates for sequence pairs (question and answer), jointly influencing the learned representations in a pairwise manner. In our approach, temporal gates are learned via 1D convolutional layers and then subsequently cross applied across question and answer for joint learning. Empirically, we show that this conceptually simple sharing of temporal gates can lead to competitive performance across multiple benchmarks. Intuitively, what our network achieves can be interpreted as learning representations of question and answer pairs that are aware of what each other is remembering or forgetting, ie, pairwise temporal gating. Via extensive experiments, we show that our proposed model achieves state-of-the-art performance on two community-based QA datasets and competitive performance on one factoid-based QA dataset.", "total_citations": 45, "citation_graph": {"2018": 12, "2019": 12, "2020": 10, "2021": 3, "2022": 5, "2023": 3}}, {"title": "Multi-range reasoning for machine comprehension", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:YFjsv_pBGBYC", "authors": ["Yi Tay", "Luu Anh Tuan", "Siu Cheung Hui"], "publication_date": "2018/3/24", "journal": "arXiv preprint arXiv:1803.09074", "description": "We propose MRU (Multi-Range Reasoning Units), a new fast compositional encoder for machine comprehension (MC). Our proposed MRU encoders are characterized by multi-ranged gating, executing a series of parameterized contract-and-expand layers for learning gating vectors that benefit from long and short-term dependencies. The aims of our approach are as follows: (1) learning representations that are concurrently aware of long and short-term context, (2) modeling relationships between intra-document blocks and (3) fast and efficient sequence encoding. We show that our proposed encoder demonstrates promising results both as a standalone encoder and as well as a complementary building block. We conduct extensive experiments on three challenging MC datasets, namely RACE, SearchQA and NarrativeQA, achieving highly competitive performance on all. On the RACE benchmark, our model outperforms DFN (Dynamic Fusion Networks) by 1.5%-6% without using any recurrent or convolution layers. Similarly, we achieve competitive performance relative to AMANDA on the SearchQA benchmark and BiDAF on the NarrativeQA benchmark without using any LSTM/GRU layers. Finally, incorporating MRU encoders with standard BiLSTM architectures further improves performance, achieving state-of-the-art results.", "total_citations": 40, "citation_graph": {"2018": 10, "2019": 13, "2020": 6, "2021": 6, "2022": 5}}, {"title": "Multi-task Neural Network for Non-discrete Attribute Prediction in Knowledge Graphs", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&pagesize=100&citation_for_view=d6ixOGYAAAAJ:r0BpntZqJG4C", "authors": ["Yi Tay", "Luu Anh Tuan", "Minh C Phan", "Siu Cheung Hui"], "publication_date": "2017/8/16", "conference": "Accepted at CIKM 2017", "description": "Many popular knowledge graphs such as Freebase, YAGO or DBPedia maintain a list of non-discrete attributes for each entity. Intuitively, these attributes such as height, price or population count are able to richly characterize entities in knowledge graphs. This additional source of information may help to alleviate the inherent sparsity and incompleteness problem that are prevalent in knowledge graphs. Unfortunately, many state-of-the-art relational learning models ignore this information due to the challenging nature of dealing with non-discrete data types in the inherently binary-natured knowledge graphs. In this paper, we propose a novel multi-task neural network approach for both encoding and prediction of non-discrete attribute information in a relational setting. Specifically, we train a neural network for triplet prediction along with a separate network for attribute value regression. Via multi-task learning, we are \u2026", "total_citations": 40, "citation_graph": {"2018": 1, "2019": 4, "2020": 8, "2021": 8, "2022": 11, "2023": 8}}]}