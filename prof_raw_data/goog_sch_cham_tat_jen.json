{"goog_sch_url": "https://scholar.google.com/citations?hl=en&user=Lx3X7W0AAAAJ", "name": "Cham Tat Jen", "interests": ["Computer Vision"], "co_authors_url": [], "citation_table": {"columns": ["All", "Since 2018"], "Citations": [5608, 2519], "h-index": [40, 23], "i10-index": [76, 42]}, "citation_graph": {"1998": 23, "1999": 34, "2000": 60, "2001": 56, "2002": 91, "2003": 165, "2004": 150, "2005": 190, "2006": 172, "2007": 150, "2008": 143, "2009": 156, "2010": 192, "2011": 166, "2012": 156, "2013": 204, "2014": 184, "2015": 223, "2016": 208, "2017": 253, "2018": 201, "2019": 242, "2020": 350, "2021": 501, "2022": 612, "2023": 608}, "articles": [{"title": "A multiple hypothesis approach to figure tracking", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:u5HHmVD_uO8C", "authors": ["Tat-Jen Cham", "James M Rehg"], "publication_date": "1999", "conference": "Computer Vision and Pattern Recognition, 1999. IEEE Computer Society Conference on.", "description": "This paper describes a probabilistic multiple-hypothesis framework for tracking highly articulated objects. In this framework, the probability density of the tracker state is represented as a set of modes with piecewise Gaussians characterizing the neighborhood around these modes. The temporal evolution of the probability density is achieved through sampling from the prior distribution, followed by local optimization of the sample positions to obtain updated modes. This method of generating hypotheses from state-space search does not require the use of discrete features unlike classical multiple-hypothesis tracking. The parametric form of the model is suited for high dimensional state-spaces which cannot be efficiently modeled using non-parametric approaches. Results are shown for tracking Fred Astaire in a movie dance sequence.", "total_citations": 520, "citation_graph": {"1999": 5, "2000": 25, "2001": 26, "2002": 33, "2003": 54, "2004": 39, "2005": 54, "2006": 42, "2007": 33, "2008": 28, "2009": 29, "2010": 26, "2011": 17, "2012": 19, "2013": 16, "2014": 14, "2015": 11, "2016": 8, "2017": 9, "2018": 3, "2019": 9, "2020": 5, "2021": 2, "2022": 3, "2023": 1}}, {"title": "Pluralistic image completion", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:eJXPG6dFmWUC", "authors": ["Chuanxia Zheng", "Tat-Jen Cham", "Jianfei Cai"], "publication_date": "2019", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "description": "Most image completion methods produce only one result for each masked input, although there may be many reasonable possibilities. In this paper, we present an approach for pluralistic image completion-the task of generating multiple and diverse plausible solutions for image completion. A major challenge faced by learning-based approaches is that usually only one ground truth training instance per label. As such, sampling from conditional VAEs still leads to minimal diversity. To overcome this, we propose a novel and probabilistically principled framework with two parallel paths. One is a reconstructive path that utilizes the only one given ground truth to get prior distribution of missing parts and rebuild the original image from this distribution. The other is a generative path for which the conditional prior is coupled to the distribution obtained in the reconstructive path. Both are supported by GANs. We also introduce a new short+ long term attention layer that exploits distant relations among decoder and encoder features, improving appearance consistency. When tested on datasets with buildings (Paris), faces (CelebA-HQ), and natural images (ImageNet), our method not only generated higherquality completion results, but also with multiple and diverse plausible outputs.", "total_citations": 460, "citation_graph": {"2019": 12, "2020": 67, "2021": 110, "2022": 152, "2023": 116}}, {"title": "Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:kRWSkSYxWN8C", "authors": ["Yujun Cai", "Liuhao Ge", "Jun Liu", "Jianfei Cai", "Tat-Jen Cham", "Junsong Yuan", "Nadia Magnenat Thalmann"], "publication_date": "2019", "conference": "Proceedings of the IEEE/CVF international conference on computer vision", "description": "Despite great progress in 3D pose estimation from single-view images or videos, it remains a challenging task due to the substantial depth ambiguity and severe self-occlusions. Motivated by the effectiveness of incorporating spatial dependencies and temporal consistencies to alleviate these issues, we propose a novel graph-based method to tackle the problem of 3D human body and 3D hand pose estimation from a short sequence of 2D joint detections. Particularly, domain knowledge about the human hand (body) configurations is explicitly incorporated into the graph convolutional operations to meet the specific demand of the 3D pose estimation. Furthermore, we introduce a local-to-global network architecture, which is capable of learning multi-scale features for the graph-based representations. We evaluate the proposed method on challenging benchmark datasets for both 3D hand pose estimation and 3D body pose estimation. Experimental results show that our method achieves state-of-the-art performance on both tasks.", "total_citations": 371, "citation_graph": {"2019": 3, "2020": 49, "2021": 88, "2022": 107, "2023": 123}}, {"title": "A dynamic Bayesian network approach to figure tracking using learned dynamic models", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:u-x6o8ySG0sC", "authors": ["Vladimir Pavlovic", "James M Rehg", "Tat-Jen Cham", "Kevin P Murphy"], "publication_date": "1999", "conference": "Computer Vision, 1999. The Proceedings of the Seventh IEEE International Conference on", "description": "The human figure exhibits complex and rich dynamic behavior that is both nonlinear and time-varying. However most work on tracking and synthesizing figure motion has employed either simple, generic dynamic models or highly specific hand-tailored ones. Recently, a broad class of learning and inference algorithms for time-series models have been successfully cast in the framework of dynamic Bayesian networks (DBNs). This paper describes a novel DBN-based switching linear dynamic system (SLDS) model and presents its application to figure motion analysis. A key feature of our approach is an approximate Viterbi inference technique for overcoming the intractability of exact inference in mixed-state DBNs. We present experimental results for learning figure dynamics from video data and show promising initial results for tracking, interpolation, synthesis, and classification using learned models.", "total_citations": 370, "citation_graph": {"1999": 2, "2000": 10, "2001": 9, "2002": 12, "2003": 37, "2004": 20, "2005": 40, "2006": 39, "2007": 16, "2008": 22, "2009": 21, "2010": 20, "2011": 16, "2012": 7, "2013": 12, "2014": 15, "2015": 13, "2016": 15, "2017": 10, "2018": 7, "2019": 7, "2020": 5, "2021": 4, "2022": 3, "2023": 2}}, {"title": "Large-margin multi-modal deep learning for RGB-D object recognition", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:P5F9QuxV20EC", "authors": ["Anran Wang", "Jiwen Lu", "Jianfei Cai", "Tat-Jen Cham", "Gang Wang"], "publication_date": "2015/9/11", "journal": "IEEE Transactions on Multimedia", "description": "Most existing feature learning-based methods for RGB-D object recognition either combine RGB and depth data in an undifferentiated manner from the outset, or learn features from color and depth separately, which do not adequately exploit different characteristics of the two modalities or utilize the shared relationship between the modalities. In this paper, we propose a general CNN-based multi-modal learning framework for RGB-D object recognition. We first construct deep CNN layers for color and depth separately, which are then connected with a carefully designed multi-modal layer. This layer is designed to not only discover the most discriminative features for each modality, but is also able to harness the complementary relationship between the two modalities. The results of the multi-modal layer are back-propagated to update parameters of the CNN layers, and the multi-modal feature learning and the back \u2026", "total_citations": 185, "citation_graph": {"2015": 5, "2016": 11, "2017": 29, "2018": 37, "2019": 28, "2020": 27, "2021": 18, "2022": 17, "2023": 9}}, {"title": "Fast training and selection of haar features using statistics in boosting-based face detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:IjCSPb-OGe4C", "authors": ["Minh-Tri Pham", "Tat-Jen Cham"], "publication_date": "2007/10/14", "conference": "Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on", "description": "Training a cascade-based face detector using boosting and Haar features is computationally expensive, often requiring weeks on single CPU machines. The bottleneck is at training and selecting Haar features for a single weak classifier, currently in minutes. Traditional techniques for training a weak classifier usually run in 0(NT log N), with N examples (approximately 10,000), and T features (approximately 40,000). We present a method to train a weak classifier in time 0(Nd 2 + T), where d is the number of pixels of the probed image sub-window (usually from 350 to 500), by using only the statistics of the weighted input data. Experimental results revealed a significantly reduced training time of a weak classifier to the order of seconds. In particular, this method suffers very minimal immerse in training time with very large increases in members of Haar features, enjoying a significant gain in accuracy, even with reduced \u2026", "total_citations": 183, "citation_graph": {"2006": 1, "2007": 0, "2008": 10, "2009": 17, "2010": 16, "2011": 10, "2012": 9, "2013": 16, "2014": 16, "2015": 11, "2016": 9, "2017": 9, "2018": 10, "2019": 11, "2020": 13, "2021": 10, "2022": 7, "2023": 6}}, {"title": "T2net: Synthetic-to-realistic translation for solving single-image depth estimation tasks", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:B3FOqHPlNUQC", "authors": ["Chuanxia Zheng", "Tat-Jen Cham", "Jianfei Cai"], "publication_date": "2018", "conference": "Proceedings of the European conference on computer vision (ECCV)", "description": "Current methods for single-image depth estimation use training datasets with real image-depth pairs or stereo pairs, which are not easy to acquire. We propose a framework, trained on synthetic image-depth pairs and unpaired real images, that comprises an image translation network for enhancing realism of input images, followed by a depth prediction network. A key idea is having the first network act as a wide-spectrum input translator, taking in either synthetic or real images, and ideally producing minimally modified realistic images. This is done via a reconstruction loss when the training input is real, and a GAN loss when synthetic, removing the need for heuristic self-regularization. The second network is trained on a task loss for synthetic image-depth pairs, with an extra GAN loss to unify real and synthetic feature distributions. Importantly, the framework can be trained end-to-end, leading to good results, even surpassing early deep-learning methods that use real paired data.", "total_citations": 179, "citation_graph": {"2018": 3, "2019": 24, "2020": 39, "2021": 36, "2022": 40, "2023": 37}}, {"title": "Reconstruction of 3D figure motion from 2D correspondences", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:d1gkVwhDpl0C", "authors": ["David E DiFranco", "Tat-Jen Cham", "James M Rehg"], "publication_date": "2001", "conference": "Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on", "description": "We present a method for computing the 3D motion of articulated models from 2D correspondences. An iterative batch algorithm is proposed which estimates the maximum a posteriori trajectory based on 2D measurements subject to a number of constraints. These include (i) kinematic constraints based on a 3D kinematic model, (ii) joint angle limits, (iii) dynamic smoothing, and (iv) 3D key frames which can be specified by the user. The framework handles any variation in the number of constraints as well as partial or missing data. This method is shown to obtain favorable reconstruction results on a number of complex human motion sequences.", "total_citations": 143, "citation_graph": {"2000": 1, "2001": 3, "2002": 12, "2003": 14, "2004": 18, "2005": 15, "2006": 13, "2007": 11, "2008": 6, "2009": 10, "2010": 9, "2011": 3, "2012": 2, "2013": 2, "2014": 5, "2015": 6, "2016": 0, "2017": 3, "2018": 1, "2019": 0, "2020": 1, "2021": 2, "2022": 2}}, {"title": "Wireless multi-user multi-projector presentation system", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:kNdYIx-mwKoC", "publication_date": "2006/2/28", "description": "Media slides are often employed in conference sessions, meetings, lectures, and other interactive forums. The proliferation of laptops and handheld computers allows a speaker to present directly from the laptop by connecting to the projector at the conference site. Physically connecting and disconnecting each presenter's laptop to the projection apparatus, however, can be a clumsy and disruptive process, particularly since the presenters may be seated at various locations around the room. A wireless interface between a presentation server and a laptop in a multi-user multi-projector presentation system allows a media sequence from each media source to be displayed on a common display via the presentation server and the wireless interface. Presenters need not run or swap cables or other physical connections to switch media sources to the common display. The interface requires no software modification to \u2026", "total_citations": 132, "citation_graph": {"2005": 5, "2006": 1, "2007": 3, "2008": 4, "2009": 5, "2010": 7, "2011": 7, "2012": 13, "2013": 9, "2014": 5, "2015": 8, "2016": 19, "2017": 8, "2018": 7, "2019": 4, "2020": 7, "2021": 11, "2022": 3, "2023": 3}}, {"title": "Method for efficiently tracking object models in video sequences via dynamic ordering of features", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:KlAtU1dfN6UC", "publication_date": "2004/9/21", "description": "An object model has a plurality of features and is described by a model state. An unregistered feature of the object model, and an available frame from a sequence of images are selected to minimize a cost function of a subsequent search for a match of the selected model feature to the image in the selected frame. Upon a match, the feature is registered in that frame. The model state is then updated for each available frame. The steps of selecting, searching and updating are repeated. A video storage module may contain only one frame corresponding to a single time instance, in which case the framework used is based on integrated sequential feature selection. Alternatively, the video store may contain the entire video sequence, in which case feature selection is performed across all video frames for maximum tracking efficiency. Finally, the video store may contain a small number of previous frames plus the current \u2026", "total_citations": 123, "citation_graph": {"2003": 2, "2004": 0, "2005": 1, "2006": 2, "2007": 2, "2008": 3, "2009": 1, "2010": 5, "2011": 4, "2012": 4, "2013": 12, "2014": 13, "2015": 10, "2016": 10, "2017": 13, "2018": 9, "2019": 12, "2020": 8, "2021": 5, "2022": 4, "2023": 3}}, {"title": "Learning progressive joint propagation for human motion prediction", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:5awf1xo2G04C", "authors": ["Yujun Cai", "Lin Huang", "Yiwei Wang", "Tat-Jen Cham", "Jianfei Cai", "Junsong Yuan", "Jun Liu", "Xu Yang", "Yiheng Zhu", "Xiaohui Shen", "Ding Liu", "Jing Liu", "Nadia Magnenat Thalmann"], "publication_date": "2020", "conference": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part VII 16", "description": "Despite the great progress in human motion prediction, it remains a challenging task due to the complicated structural dynamics of human behaviors. In this paper, we address this problem in three aspects. First, to capture the long-range spatial correlations and temporal dependencies, we apply a transformer-based architecture with the global attention mechanism. Specifically, we feed the network with the sequential joints encoded with the temporal information for spatial and temporal explorations. Second, to further exploit the inherent kinematic chains for better 3D structures, we apply a progressive-decoding strategy, which performs in a central-to-peripheral extension according to the structural connectivity. Last, in order to incorporate a general motion space for high-quality prediction, we build a memory-based dictionary, which aims to preserve the global motion patterns in training data to guide the \u2026", "total_citations": 110, "citation_graph": {"2020": 4, "2021": 23, "2022": 35, "2023": 48}}, {"title": "Fast polygonal integration and its application in extending haar-like features to improve object detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:qUcmZB5y_30C", "authors": ["Minh-Tri Pham", "Yang Gao", "Viet-Dung D Hoang", "Tat-Jen Cham"], "publication_date": "2010/6/13", "conference": "2010 IEEE computer society conference on computer vision and pattern recognition", "description": "The integral image is typically used for fast integrating a function over a rectangular region in an image. We propose a method that extends the integral image to do fast integration over the interior of any polygon that is not necessarily rectilinear. The integration time of the method is fast, independent of the image resolution, and only linear to the polygon's number of vertices. We apply the method to Viola and Jones' object detection framework, in which we propose to improve classical Haar-like features with polygonal Haar-like features. We show that the extended feature set improves object detection's performance. The experiments are conducted in three domains: frontal face detection, fixed-pose hand detection, and rock detection for Mars' surface terrain assessment.", "total_citations": 110, "citation_graph": {"2010": 2, "2011": 4, "2012": 4, "2013": 12, "2014": 12, "2015": 2, "2016": 6, "2017": 10, "2018": 6, "2019": 10, "2020": 14, "2021": 10, "2022": 7, "2023": 5}}, {"title": "Mmss: Multi-modal sharable and specific feature learning for rgb-d object recognition", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:1sJd4Hv_s6UC", "authors": ["Anran Wang", "Jianfei Cai", "Jiwen Lu", "Tat-Jen Cham"], "publication_date": "2015", "conference": "Proceedings of the IEEE international conference on computer vision", "description": "Most of the feature-learning methods for RGB-D object recognition either learn features from color and depth modalities separately, or simply treat RGB-D as undifferentiated four-channel data, which cannot adequately exploit the relationship between different modalities. Motivated by the intuition that different modalities should contain not only some modal-specific patterns but also some shared common patterns, we propose a multi-modal feature learning framework for RGB-D object recognition. We first construct deep CNN layers for color and depth separately, and then connect them with our carefully designed multi-modal layers, which fuse color and depth information by enforcing a common part to be shared by features of different modalities. In this way, we obtain features reflecting shared properties as well as modal-specific properties in different modalities. The information of the multi-modal learning frameworks is back-propagated to the early CNN layers. Experimental results show that our proposed multi-modal feature learning method outperforms state-of-the-art approaches on two widely used RGB-D object benchmark datasets.", "total_citations": 109, "citation_graph": {"2015": 3, "2016": 10, "2017": 18, "2018": 10, "2019": 18, "2020": 14, "2021": 18, "2022": 10, "2023": 6}}, {"title": "Dynamic shadow elimination for multi-projector displays", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:9yKSN-GCB0IC", "authors": ["Rahul Sukthankar", "Tat-Jen Cham", "Gita Sukthankar"], "publication_date": "2001/12/8", "conference": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001", "description": "A major problem with interactive displays based on front-projection is that users cast undesirable shadows on the display surface. This situation is only partially addressed by mounting a single projector at an extreme angle and pre-warping the projected image to undo keystoning distortions. This paper demonstrates that shadows can be muted by redundantly illuminating the display surface using multiple projectors, all mounted at different locations. However, this technique alone does not eliminate shadows: multiple projectors create multiple dark regions on the surface (penumbral occlusions). We solve the problem by using cameras to automatically identify occlusions as they occur and dynamically adjust each projector's output so that additional light is projected onto each partially-occluded patch. The system is self-calibrating: relevant homographies relating projectors, cameras and the display surface are \u2026", "total_citations": 107, "citation_graph": {"2001": 1, "2002": 5, "2003": 9, "2004": 5, "2005": 18, "2006": 9, "2007": 6, "2008": 6, "2009": 4, "2010": 6, "2011": 3, "2012": 1, "2013": 0, "2014": 2, "2015": 1, "2016": 5, "2017": 6, "2018": 1, "2019": 5, "2020": 0, "2021": 5, "2022": 2, "2023": 2}}, {"title": "Estimating camera pose from a single urban ground-view omnidirectional image and a 2D building outline map", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:9ZlFYXVOiuMC", "authors": ["Tat-Jen Cham", "Arridhana Ciptadi", "Wei-Chian Tan", "Minh-Tri Pham", "Liang-Tien Chia"], "publication_date": "2010/6/13", "conference": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "description": "A framework is presented for estimating the pose of a camera based on images extracted from a single omnidirectional image of an urban scene, given a 2D map with building outlines with no 3D geometric information nor appearance data. The framework attempts to identify vertical corner edges of buildings in the query image, which we term VCLH, as well as the neighboring plane normals, through vanishing point analysis. A bottom-up process further groups VCLH into elemental planes and subsequently into 3D structural fragments modulo a similarity transformation. A geometric hashing lookup allows us to rapidly establish multiple candidate correspondences between the structural fragments and the 2D map building contours. A voting-based camera pose estimation method is then employed to recover the correspondences admitting a camera pose solution with high consensus. In a dataset that is even \u2026", "total_citations": 99, "citation_graph": {"2011": 8, "2012": 10, "2013": 10, "2014": 8, "2015": 16, "2016": 6, "2017": 13, "2018": 9, "2019": 8, "2020": 2, "2021": 3, "2022": 4, "2023": 1}}, {"title": "Image pre-conditioning for out-of-focus projector blur", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:UebtZRa9Y70C", "authors": ["Michael S Brown", "Peng Song", "Tat-Jen Cham"], "publication_date": "2006/6/17", "conference": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)", "description": "We present a technique to reduce image blur caused by out-of-focus regions in projected imagery. Unlike traditional restoration algorithms that operate on a blurred image to recover the original, the nature of our problem requires that the correction be applied to the original image before blurring. To accomplish this, a camera is used to estimate a series of spatially varying point-spread-functions (PSF) across the projector\u2019s image. These discrete PSFs are then used to guide a pre-processing algorithm based on Wiener filtering to condition the image before projection. Results show that using this technique can help ameliorate the visual effects from out-of-focus projector blur.", "total_citations": 86, "citation_graph": {"2006": 3, "2007": 5, "2008": 5, "2009": 3, "2010": 4, "2011": 5, "2012": 4, "2013": 9, "2014": 8, "2015": 6, "2016": 8, "2017": 3, "2018": 5, "2019": 4, "2020": 5, "2021": 4, "2022": 2, "2023": 3}}, {"title": "Online learning asymmetric boosted classifiers for object detection", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:Y0pCki6q_DkC", "authors": ["Minh-Tri Pham", "Tat-Jen Cham"], "publication_date": "2007/6/17", "conference": "2007 IEEE Conference on Computer Vision and Pattern Recognition", "description": "We present an integrated framework for learning asymmetric boosted classifiers and online learning to address the problem of online learning asymmetric boosted classifiers, which is applicable to object detection problems. In particular, our method seeks to balance the skewness of the labels presented to the weak classifiers, allowing them to be trained more equally. In online learning, we introduce an extra constraint when propagating the weights of the data points from one weak classifier to another, allowing the algorithm to converge faster. In compared with the Online Boosting algorithm recently applied to object detection problems, we observed about 0-10% increase in accuracy, and about 5-30% gain in learning speed.", "total_citations": 82, "citation_graph": {"2007": 2, "2008": 6, "2009": 12, "2010": 10, "2011": 7, "2012": 8, "2013": 8, "2014": 4, "2015": 6, "2016": 7, "2017": 2, "2018": 1, "2019": 1, "2020": 3, "2021": 2, "2022": 2}}, {"title": "Modality and component aware feature fusion for rgb-d scene classification", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:UxriW0iASnsC", "authors": ["Anran Wang", "Jianfei Cai", "Jiwen Lu", "Tat-Jen Cham"], "publication_date": "2016", "conference": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "description": "While convolutional neural networks (CNN) have been excellent for object recognition, the greater spatial variability in scene images typically meant that the standard full-image CNN features are suboptimal for scene classification. In this paper, we investigate a framework allowing greater spatial flexibility, in which the Fisher vector (FV) encoded distribution of local CNN features, obtained from a multitude of region proposals per image, is considered instead. The CNN features are computed from an augmented pixel-wise representation comprising multiple modalities of RGB, HHA and surface normals, as extracted from RGB-D data. More significantly, we make two postulates:(1) component sparsity---that only a small variety of region proposals and their corresponding FV GMM components contribute to scene discriminability, and (2) modal non-sparsity---within these discriminative components, all modalities have important contribution. In our framework, these are implemented through regularization terms applying group lasso to GMM components and exclusive group lasso across modalities. By learning and combining regressors for both proposal-based FV features and global CNN features, we were able to achieve state-of-the-art scene classification performance on the SUNRGBD Dataset and NYU Depth Dataset V2.", "total_citations": 81, "citation_graph": {"2016": 1, "2017": 10, "2018": 13, "2019": 14, "2020": 11, "2021": 17, "2022": 5, "2023": 9}}, {"title": "Shadow elimination and occluder light suppression for multi-projector displays", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:2osOgNQ5qMEC", "authors": ["Tat-Jen Cham", "James M Rehg", "Rahul Sukthankar", "Gita Sukthankar"], "publication_date": "2003/6/18", "conference": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings.", "description": "Two related problems of front projection displays, which occur when users obscure a projector, are: (i) undesirable shadows cast on the display by the users, and (ii) projected light falling on and distracting the users. This paper provides a computational framework for solving these two problems based on multiple overlapping projectors and cameras. The overlapping projectors are automatically aligned to display the same dekeystoned image. The system detects when and where shadows are cast by occluders and is able to determine the pixels, which are occluded in different projectors. Through a feedback control loop, the contributions of unoccluded pixels from other projectors are boosted in the shadowed regions, thereby eliminating the shadows. In addition, pixels, which are being occluded, are blanked, thereby preventing the projected light from falling on a user when they occlude the display. This can be \u2026", "total_citations": 79, "citation_graph": {"2002": 3, "2003": 3, "2004": 8, "2005": 17, "2006": 9, "2007": 9, "2008": 5, "2009": 1, "2010": 3, "2011": 3, "2012": 1, "2013": 0, "2014": 3, "2015": 1, "2016": 0, "2017": 4, "2018": 0, "2019": 1, "2020": 0, "2021": 1}}, {"title": "The spatially-correlative loss for various image translation tasks", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:N5tVd3kTz84C", "authors": ["Chuanxia Zheng", "Tat-Jen Cham", "Jianfei Cai"], "publication_date": "2021", "conference": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition", "description": "We propose a novel spatially-correlative loss that is simple, efficient, and yet effective for preserving scene structure consistency while supporting large appearance changes during unpaired image-to-image (I2I) translation. Previous methods attempt this by using pixel-level cycle-consistency or feature-level matching losses, but the domain-specific nature of these losses hinder translation across large domain gaps. To address this, we exploit the spatial patterns of self-similarity as a means of defining scene structure. Our spatially-correlative loss is geared towards only capturing spatial relationships within an image rather than domain appearance. We also introduce a new self-supervised learning method to explicitly learn spatially-correlative maps for each specific translation task. We show distinct improvement over baseline models in all three modes of unpaired I2I translation: single-modal, multi-modal, and even single-image translation. This new loss can easily be integrated into existing network architectures and thus allows wide applicability.", "total_citations": 78, "citation_graph": {"2021": 4, "2022": 29, "2023": 45}}, {"title": "Automated B-spline curve representation incorporating MDL and error-minimizing control point insertion strategies", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:qjMakFHDy7sC", "authors": ["T-J Cham", "Roberto Cipolla"], "publication_date": "1999/1", "journal": "Pattern Analysis and Machine Intelligence, IEEE Transactions on", "description": "The main issues of developing an automatic and reliable scheme for spline-fitting are discussed and addressed in this paper, which are not fully covered in previous papers or algorithms. The proposed method incorporates B-spline active contours, the minimum description length (MDL) principle, and a novel control point insertion strategy based on maximizing the potential for energy-reduction maximization (PERM). A comparison of test results shows that it outperforms one of the better existing methods.", "total_citations": 73, "citation_graph": {"1999": 1, "2000": 7, "2001": 5, "2002": 2, "2003": 5, "2004": 7, "2005": 5, "2006": 2, "2007": 7, "2008": 4, "2009": 7, "2010": 3, "2011": 1, "2012": 3, "2013": 1, "2014": 2, "2015": 2, "2016": 2, "2017": 2, "2018": 0, "2019": 0, "2020": 1, "2021": 1}}, {"title": "Face and human gait recognition using image-to-class distance", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:4TOpqqG69KYC", "authors": ["Yi Huang", "Dong Xu", "Tat-Jen Cham"], "publication_date": "2010/3", "journal": "Circuits and Systems for Video Technology, IEEE Transactions on", "description": "We propose a new distance measure for face recognition and human gait recognition. Each probe image (a face image or an average human silhouette image) is represented as a set of local features uniformly sampled over a grid with fixed spacing, and each gallery image is represented as a set of local features sampled at each pixel. We formulate an integer programming problem to compute the distance (referred to as the image-to-class distance) from one probe image to all the gallery images belonging to a certain class, in which any feature of the probe image can be matched to only one feature from one of the gallery images. Considering computational efficiency as well as the fact that face images or average human silhouette images are roughly aligned in the preprocessing step, we also enforce a spatial neighborhood constraint by only allowing neighboring features that are within a given spatial distance to \u2026", "total_citations": 69, "citation_graph": {"2009": 1, "2010": 3, "2011": 6, "2012": 7, "2013": 12, "2014": 8, "2015": 5, "2016": 7, "2017": 8, "2018": 2, "2019": 4, "2020": 0, "2021": 2, "2022": 1, "2023": 1}}, {"title": "Multi-modal unsupervised feature learning for RGB-D scene labeling", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:cFHS6HbyZ2cC", "authors": ["Anran Wang", "Jiwen Lu", "Gang Wang", "Jianfei Cai", "Tat-Jen Cham"], "publication_date": "2014", "conference": "Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13", "description": "Most of the existing approaches for RGB-D indoor scene labeling employ hand-crafted features for each modality independently and combine them in a heuristic manner. There has been some attempt on directly learning features from raw RGB-D data, but the performance is not satisfactory. In this paper, we adapt the unsupervised feature learning technique for RGB-D labeling as a multi-modality learning problem. Our learning framework performs feature learning and feature encoding simultaneously which significantly boosts the performance. By stacking basic learning structure, higher-level features are derived and combined with lower-level features for better representing RGB-D data. Experimental results on the benchmark NYU depth dataset show that our method achieves competitive performance, compared with state-of-the-art.", "total_citations": 68, "citation_graph": {"2014": 2, "2015": 17, "2016": 12, "2017": 15, "2018": 3, "2019": 5, "2020": 3, "2021": 3, "2022": 2, "2023": 1}}, {"title": "Symmetry detection through local skewed symmetries", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:UeHWp8X0CEIC", "authors": ["Tat-Jen Cham", "Roberto Cipolla"], "publication_date": "1995/6/1", "journal": "Image and Vision Computing", "description": "We explore how global symmetry can be detected prior to segmentation and under noise and occlusion. The definition of local symmetries is extended to affine geometries by considering the tangents and curvatures of local structures, and a quantitative measure of local symmetry known as symmetricity is introduced, which is based on Mahalanobis distances from the tangent-curvature states of local structures to the local skewed symmetry state-subspace. These symmetricity values, together with the associated local axes of symmetry, are spatially related in the local skewed symmetry field (LSSF). In the implementation, a fast, local symmetry detection algorithm allows initial hypotheses for the symmetry axis to be generated through the use of a modified Hough transform. This is then improved upon by maximizing a global symmetry measure based on accumulated local support in the LSSF-a straight active contour \u2026", "total_citations": 68, "citation_graph": {"1994": 2, "1995": 0, "1996": 5, "1997": 2, "1998": 7, "1999": 7, "2000": 3, "2001": 3, "2002": 2, "2003": 3, "2004": 6, "2005": 2, "2006": 3, "2007": 2, "2008": 0, "2009": 4, "2010": 1, "2011": 2, "2012": 0, "2013": 5, "2014": 0, "2015": 1, "2016": 4, "2017": 3}}, {"title": "Calibrating scalable multi-projector displays using camera homography trees", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:zYLM7Y9cAGgC", "authors": ["Han Chen", "Rahul Sukthankar", "Grant Wallace", "Tat-Jen Cham"], "publication_date": "2001", "journal": "Computer Vision and Pattern Recognition", "description": "We present a practical vision-based calibration system for large format multi-projector displays. A spanning tree of homographies, automatically constructed from several camera images, accurately registers arbitrarily-mounted projectors to a global reference frame. Experiments on the 18\u2019\u00d7 8\u2019Princeton Display Wall (a 24 projector array with 6000\u00d7 3000 resolution) demonstrate that our algorithm achieves sub-pixel accuracy even on large display surfaces. A direct comparison with the previous best algorithm shows that our technique is significantly more accurate, requires far fewer camera images, and runs faster by an order of magnitude.", "total_citations": 64, "citation_graph": {"2001": 1, "2002": 7, "2003": 7, "2004": 7, "2005": 4, "2006": 8, "2007": 1, "2008": 5, "2009": 4, "2010": 2, "2011": 3, "2012": 2, "2013": 3, "2014": 4, "2015": 3, "2016": 1, "2017": 0, "2018": 0, "2019": 1, "2020": 0, "2021": 1}}, {"title": "Method for object registration via selection of models with dynamically ordered features", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:Zph67rFs4hoC", "publication_date": "2003/7/22", "description": "A plurality of object models, where each object model comprises a plurality of features and is described by a model state, are registered in at least one image a subset of the object models is selected. Different object models have different sets of features, which may or may not overlap. A feature of each selected object model is registered in one of the images, and the model state for each selected object model is updated accordingly. The model states of some or all of the object models are then updated according to a set of constraints. These steps are repeated until one or more object models are registered. At the beginning of each registration cycle, a cost function of a subsequent search is determined for each unregistered feature of each object model. An unregistered feature of each object model is then selected such that the cost function is minimized. Object models to which the selected features belong are then \u2026", "total_citations": 61, "citation_graph": {"2004": 1, "2005": 2, "2006": 1, "2007": 1, "2008": 3, "2009": 3, "2010": 0, "2011": 4, "2012": 2, "2013": 3, "2014": 7, "2015": 9, "2016": 1, "2017": 5, "2018": 4, "2019": 3, "2020": 3, "2021": 4, "2022": 4, "2023": 1}}, {"title": "Agilegan: stylizing portraits by inversion-consistent transfer learning", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:hkOj_22Ku90C", "authors": ["Guoxian Song", "Linjie Luo", "Jing Liu", "Wan-Chun Ma", "Chunpong Lai", "Chuanxia Zheng", "Tat-Jen Cham"], "publication_date": "2021/7/19", "journal": "ACM Transactions on Graphics (TOG)", "description": "Portraiture as an art form has evolved from realistic depiction into a plethora of creative styles. While substantial progress has been made in automated stylization, generating high quality stylistic portraits is still a challenge, and even the recent popular Toonify suffers from several artifacts when used on real input images. Such StyleGAN-based methods have focused on finding the best latent inversion mapping for reconstructing input images; however, our key insight is that this does not lead to good generalization to different portrait styles. Hence we propose AgileGAN, a framework that can generate high quality stylistic portraits via inversion-consistent transfer learning. We introduce a novel hierarchical variational autoencoder to ensure the inverse mapped distribution conforms to the original latent Gaussian distribution, while augmenting the original space to a multi-resolution latent space so as to better encode \u2026", "total_citations": 59, "citation_graph": {"2021": 4, "2022": 29, "2023": 25}}, {"title": "A statistical framework for long-range feature matching in uncalibrated image mosaicing", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:Tyk-4Ss8FVUC", "authors": ["Tat-Jen Cham", "Roberto Cipolla"], "publication_date": "1998/6/25", "conference": "Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 98CB36231)", "description": "The problem considered is that of estimating the projective transformation between two images in situations where the image motion is large and feature matching is not aided by a proximity heuristic. The overall algorithm designed is based on a multiresolution, multihypothesis scheme, and similarities between tracking and matching through multiple resolution levels are exploited. Two major tools are developed in this paper: (i) a Bayesian framework for incorporating similarity measures of feature correspondences in regression to specify the different levels of confidence in the correspondences; and (ii) a Bayesian version of RANSAC, which is able to utilise prior estimates and matching probabilities. The algorithm is tested on a number of real images with large image motion and promising results were obtained.", "total_citations": 59, "citation_graph": {"1998": 2, "1999": 7, "2000": 3, "2001": 1, "2002": 2, "2003": 5, "2004": 4, "2005": 1, "2006": 2, "2007": 4, "2008": 0, "2009": 1, "2010": 7, "2011": 5, "2012": 2, "2013": 3, "2014": 1, "2015": 2, "2016": 2, "2017": 0, "2018": 0, "2019": 2, "2020": 0, "2021": 0, "2022": 2}}, {"title": "Detection with multi-exit asymmetric boosting", "url": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Lx3X7W0AAAAJ&pagesize=100&citation_for_view=Lx3X7W0AAAAJ:0EnyYjriUFMC", "authors": ["Minh-Tri Pham", "Viet-Dung D Hoang", "Tat-Jen Cham"], "publication_date": "2008/6/23", "conference": "2008 IEEE Conference on Computer Vision and Pattern Recognition", "description": "We introduce a generalized representation for a boosted classifier with multiple exit nodes, and propose a method to training which combines the idea of propagating scores across boosted classifiers [14, 17] and the use of asymmetric goals [13]. A means for determining the ideal constant asymmetric goal is provided, which is theoretically justified under a conservative bound on the ROC operating point target and empirically near-optimal under the exact bound. Moreover, our method automatically minimizes the number of weak classifiers, avoiding the need to retrain a boosted classifier multiple times for empirical best performance as in conventional methods. Experimental results shows significant reduction in training time and number of weak classifiers, as well as better accuracy, compared to conventional cascades and multi-exit boosted classifiers.", "total_citations": 58, "citation_graph": {"2009": 3, "2010": 9, "2011": 7, "2012": 6, "2013": 8, "2014": 8, "2015": 5, "2016": 6, "2017": 2, "2018": 1, "2019": 0, "2020": 1, "2021": 1}}]}